---
title: 随机变量的统计特征
date: 2017-08-16 11:08:09
tags: [人工智能,概率,统计]
categories: 数学
---
随机变量的统计特征主要包括期望，方差，协方差以及相关系数。
<!--more-->
# 期望
- 离散型随机变量：
$$E(X) = \sum_{k=1}^{ +\infty}p_kx_k$$
- 连续型随机变量：
$$E(X) = \int_{-\infty}^{ +\infty} {xf(x)dx}$$

## 期望有以下性质(C为常数,其他均为随机变量):

1. $E(C)=C$
1. $E(CX)=CE(X)$
1. $E(X+Y)=E(X)+E(Y)$
1. $E(XY)=E(X)E(Y)$ （$X,Y$ 相互独立）

## 随机变量 $X$ 的函数的期望
前面讨论随机变量的分布函数时，同时讨论了随机变量的函数的分布函数，这里同样对于**随机变量 $X$ 的函数的期望**进行讨论，其定义及求法如下所示。

设 $Y$ 是随机变量 $X$ 的函数：$Y=g(X)$ ( $g$ 是连续函数)

1. 如果 $X$ 是离散型随机变量，它的分布律为
$$P(X=x_k) = p_k, k = 1,2,…$$
若 $\sum_{k=1}^{\infty}g(x_k)p_k$ 绝对收敛，则有
$$E(Y) = E[g(X)] = \sum_{k=1}^{\infty}g(x_k)p_k$$
1. 如果 $X$ 是连续型随机变量，它的概率密度函数为 $f(x)$, 若 $\int_{-\infty}^{\infty}g(x)f(x)dx$ 绝对收敛，则有
$$E(Y) = E[g(X)] = \int_{-\infty}^{\infty}g(x)f(x)dx$$

这个定理的重要意义在于求 $E(Y)$ 的时候，不用再求 $Y$ 的分布律或概率密度函数，直接利用 $X$ 的分布律或概率密度函数即可。

# 方差
方差的原始定义为

$$D(X) = E[(X-E(X))^2] = E(X^2) - E(X)^2$$
## 方差有以下性质：

1. $D(C)=0$
1. $D(CX)=C^2D(X)$
1. $D(X+Y)=D(X)+D(Y)+2E([X−E(X)][Y−E(Y)])$
如果 $X，Y$ 是相互独立的，那么 $E([X−E(X)][Y−E(Y)])=0$ , 当这一项不为0的时候，称作变量 $X,Y$ 的协方差。

# 常见分布的期望和方差
前面我们提到了若干种典型的离散分布和连续分布，下面是这几种分布的期望和方差，记住这些常用的期望和方差能够在使用的时候省去推导过程。

分布类型 | 概率密度函数 | 期望 | 方差
:-:|:-:|:-:|:-:
伯努利分布~$B(1,p)$ | $p = p^x(1-p)^{1-x}$ | $p$ | $p(1−p)$
二项分布~$B(n,p)$ | $p_i = C_n^i p^i(1-p)^{n-i}(i=1,2)$ | $np$ | $np(1−p)$
泊松分布~$P(λ)$ | $p_i = \frac{\lambda^ki e^{-\lambda}}{i!}(i = 1,2,...)$ | $λ$ | $λ$
均匀分布~$U(a,b)$ | $f(x) = \frac{1}{b-a}$ | $\frac{a+b}{2}$ | $\frac{(b-a)^2}{12}$
正态分布~$N(μ,σ^2)$ | $f(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ | $μ$ | $σ^2$
指数分布~$E(λ)$ | $f(x) = \begin{cases}  \lambda e^{-x\lambda} &{x>0} \\ 0&{其他}\end{cases}$ | $\frac{1}{\lambda}$ | $\frac{1}{\lambda^2}$

# 切比雪夫不等式
切比雪夫不等式的定义如下：

设随机变量 $X$ 具有数学期望 $E(X)=μ$, 方差 $D(X)=σ^2$, 则对于任意正数 $ϵ$, 下面的不等式成立
$$P(|X-\mu|\ge \epsilon) \le \frac{\sigma^2}{\epsilon^2}$$
从定义可知，切比雪夫不等式也可写成如下的形式：

$$P(|X-\mu| \le \epsilon) \ge 1 - \frac{\sigma^2}{\epsilon^2}$$
切比雪夫不等式的一个重要意义在于当随机变量 $X$ 的分布未知，只知道 $E(X)$ 和 $D(X)$ 的情况下，对于事件 $(|X−μ|≤ϵ)$ 概率的下限的估计。

# 协方差
协方差表达了两个随机变量的相关性，正的协方差表达了正相关性，负的协方差表达了负相关性。协方差为0 表示两者不相关，对于同样的两个随机变量来说，计算出的协方差的绝对值越大，相关性越强。

协方差的定义入下:

$$Cov(X,Y)=E[X−E(X)][Y−E(Y)]$$

## 协方差有以下性质：

1. $Cov(X,Y)=Cov(Y,X)$
1. $Cov(X,Y)=E(XY)−E(X)E(Y)$
1. $Cov(aX,bY)=abCov(X,Y)$（a，b是常数）
1. $Cov(X_1+X_2, Y) = Cov(X_1, Y) + Cov(X_2,Y)$

## 相关系数
假如我们现在有身高和体重这两个未知变量，对于一系列的样本我们算出的的协方差为30，那这究竟是多大的一个量呢？如果我们又发现，身高与鞋号的协方差为5，是否说明，相对于鞋号，身高与体重的的相关性更强呢？

为了能进行这样的横向对比，我们计算相关系数(correlation coefficient)， 相关系数相当于是“归一化”的协方差。

$$\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)D(Y)}}$$
相关系数是用协方差除以两个随机变量的标准差。**相关系数的大小在-1和1之间变化，等于0表示不相关**。再也不会出现因为计量单位变化，而数值变化较大的情况，而相关系数的大小的含义与协方差是一样的。

需要注意的是上面提到的**相关**均指**线性相关**，$X,Y$ 不相关是指 $X,Y$ 之间不存在线性关系，但是他们还可能存在除线性关系以外的关系。因此，有以下结论: **$X,Y$ 相互独立则 $X,Y$ 一定不相关；反之 $X,Y$ 不相关，两者不一定相互独立。**



# 矩和协方差矩阵
## 矩
下面介绍概率论中几种矩的定义

设 $X,Y$ 为随机变量,则

1. $E(X^k), k=1,2,3….$ 称为 $X$ 的 $k$ 阶原点矩，简称 $k$ 阶矩
1. $E((X-E[X])^k), k=1,2,3….$ 称为 $X$ 的 $k$ 阶中心距
1. $E(X^kY^l),k,l=1,2,…$ 称为 $X$ 和 $Y$ 的 $k+l$ 阶混合矩
1. $E((X-E[X])^k(Y-E[Y])^l)),k,l=1,2,…$称为 $X$ 和 $Y$ 的 $k+l$ 阶混合中心矩

由以上定义我们可以知道，随机变量的期望是其**一阶原点矩，方差是其二阶中心距，协方差是其二阶混合中心矩。**

## 协方差矩阵
除此之外，另外一个常用的概念是协方差矩阵， 其定义如下：

对于 $n$ 维随机变量 ($(X_1,X_2,X_3…,X_n)$) 构成的矩阵

$$C= \begin{bmatrix} c_{11} & c_{12} & \cdots & c_{1n} \\ c_{21} & c_{22} & \cdots & c_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ c_{n1} & c_{n2} & \cdots & c_{nn} \\ \end{bmatrix}$$
其中各个元素为
$$c_{ij} = Cov(X_i,X_j) = E((X_i - E[X_i])(X_j - E[X_j]))，i,j=1,2,3..n$$
则称矩阵 $C$ 为协方差矩阵，由于 $c_{ij} = c_{ji}$ ， 因此上面的矩阵为一个对称矩阵。

协方差矩阵其实是将二维随机变量的协方差一般化后拓展到了 $n$ 维随机变量上的一种表示形式，但是除了作为一种表示形式以外，协方差矩阵还存在着某些性质使得其在多个领域均有应用，如主成成分分析。
