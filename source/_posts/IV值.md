---
title: IV值
date: 2019-08-20 15:22:10
tags: [人工智能,特征工程]
categories: [机器学习]
permalink: 机器学习/IV值.md
---

$IV$的全称是Information Value，中文意思是信息价值，或者信息量。在特征选择的过程中，可以定量的描述模型的预测能力。每一个特征对应一个$IV$值。

| IV值     | IV<0.02        | 0.02$\leqslant$IV<0.1 | 0.1$\leqslant$IV<0.3 | IV$\geqslant$0.3 |
| -------- | -------------- | --------------------- | -------------------- | ---------------- |
| 预测能力 | 不具有预测能力 | 预测能力很弱          | 中等程度预测能力     | 预测能力很强     |

# IV值的直观理解

对$IV$值的直观理解：我们假设在一个分类问题中，`label`为：$Y_1$，$Y_2$；特征为$C_1，C_2，C_3，……，C_n$。

| 特征                     | label        |
| ------------------------ | ------------ |
| $C_1，C_2，C_3，……，C_n$ | $Y_1$，$Y_2$ |

对于一个样本$A$，要判断A属于$Y_1$还是$Y_2$，我们是需要一定的信息的，假设这个信息总量是$I$，而这些所需要的信息，就蕴含在所有的特征中，那么，对于其中的一个特征$C_i$来说，其蕴含的信息越多，那么它对于判断$A$属于$Y_1$还是$Y_2$的贡献就越大，$C_i$的信息价值就越大，$C_i$的$IV$就越大。

<!--more-->

# 计算方法

1. 计算每一个特征的$WOE_i$
   要对一个特征进行$WOE$编码，需要首先把这个变量进行分组处理（也叫离散化、分箱等等）。分组后，对于第$i$组，$WOE$的计算公式如下：
   $$
   WOE_i = ln(\frac{py_i}{pn_i})=ln(\frac{y_i/y_T}{n_i/n_T})
   $$
   **WOE实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。WOE越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。**

2. 计算每一个特征的$VI$值
   $$\begin{aligned}
   IV_i &= (py_i-pn_i)\times WOE_i =(py_i-pn_i)\times ln(\frac{py_i}{pn_i})= (\frac{y_i}{y_T}-\frac{n_i}{n_T}) \times ln(\frac{y_i/y_T}{n_i/n_T})  \\
   IV &= \sum_{i}^n IV_i
   \end{aligned}$$
   - $y_T$：样本中好的用户总数
   - $n_T$：样本中坏的用户总数
   - $y_i$：第$i$组中好的用户数
   - $n_i$：第$i$组中坏的用户数

# 计算实例

假设我们需要构建一个预测模型，预测公司客户对我们的这项营销活动响应的可能性有多大。假设我们已经从公司客户列表中随机抽取了100000个客户进行了营销活动测试，收集了这些客户的响应结果，作为我们的建模数据集，其中响应的客户有10000个。

| 样本总量       | 响应样本量   | 未响应样本量 |
| -------------- | ------------ | ------------ |
| 100000    十万 | 10000    1万 | 90000    9万 |

我们以其中的一个特征“最近一次购买金额”为例计算$WOE$和$IV$值

| 最近一次购买金额 | 响应  | 未响应 | 合计   | 响应比例 | WOE      | IV       |
| ---------------- | ----- | ------ | ------ | -------- | -------- | -------- |
| <100元           | 2500  | 47500  | 50000  | 5%       | -0.74721 | 0.20756  |
| [100,200)        | 3000  | 27000  | 30000  | 10%      | 0        | 0        |
| [200,500)        | 3000  | 12000  | 15000  | 20%      | 0.81093  | 0.135155 |
| $\geqslant$500   | 1500  | 3500   | 5000   | 30%      | 1.349927 | 0.149992 |
| 合计             | 10000 | 90000  | 100000 | 10%      | 0        | 0.492706 |

- <100元：$IV_1=(\frac {2500}{10000}-\frac{47500}{90000})\times WOE_1=(\frac {2500}{10000}-\frac{47500}{90000})\times ln(\frac{2500/10000}{47500/90000})=0.20756$
- [100,200)  ：$IV_2=(\frac {3000}{10000}-\frac{27000}{90000})\times WOE_2=(\frac {3000}{10000}-\frac{27000}{90000})\times ln(\frac{3000/10000}{27000/90000})=0$
- [200,500)  ：$IV_3=(\frac {3000}{10000}-\frac{12000}{90000})\times WOE_2=(\frac {3000}{10000}-\frac{12000}{90000})\times ln(\frac{3000/10000}{12000/90000})=0.135155$
- $\geqslant$500：$IV_4=(\frac {1500}{10000}-\frac{3500}{90000})\times WOE_1=(\frac {1500}{10000}-\frac{3500}{90000})\times ln(\frac{1500/10000}{3500/90000})=0.149992$

$$
IV=\sum_i^n IV_i= IV_1+IV_2+IV_3+IV_4 = 0.492706
$$

## WOE的理解 

**WOE实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。WOE越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。**

1. 当前分组中，响应的比例越大，WOE值越大；
2. 当前分组WOE的正负，由当前分组响应和未响应的比例，与样本整体响应和未响应的比例的大小关系决定，当前分组的比例小于样本整体比例时，WOE为负，当前分组的比例大于整体比例时，WOE为正，当前分组的比例和整体比例相等时，WOE为0。
3. WOE的取值范围是全体实数。

## 为什么用IV而不是直接用WOE

从上面的内容来看，特征各分组的$WOE$和$IV$都隐含着这个分组对目标变量的预测能力这样的意义。那我们为什么不直接用$WOE$相加或者绝对值相加作为衡量一个变量整体预测能力的指标呢？

1. 当我们衡量一个变量的预测能力时，我们所使用的指标值不应该是负数，否则，说一个变量的预测能力的指标是$-2.3$，听起来很别扭。从这个角度讲，乘以$(py_i-pn_i)$这个系数，保证了变量每个分组的结果都是非负数，你可以验证一下，当一个分组的$WOE$是正数时，$(py_i-pn_i)$也是正数，当一个分组的$WOE$是负数时，$(py_i-pn_i)$也是负数，而当一个分组的$WOE=0$时，$(py_i-pn_i)$也是0。

   当然，上面的原因不是最主要的，因为其实我们上面提到的$WOE=\sum_i^n |WOE_i|$这个指标也可以完全避免负数的出现。

2. 乘以$(py_i-pn_i)$后，体现出了变量当前分组中个体的数量占整体个体数量的比例，对变量预测能力的影响。怎么理解这句话呢？我们还是举个例子。

   假设我们上面所说的营销响应模型中，还有一个特征A（是否为公司VIP客户），其取值只有两个`0,1`，数据如下：

   | A(是否为公司VIP) | 响应  | 未响应 | 合计   | 响应比例 | WOE       | IV           |
   | ---------------- | ----- | ------ | ------ | -------- | --------- | ------------ |
   | 1                | 90    | 10     | 100    | 90%      | 4.3944492 | 0.0390618    |
   | 0                | 9910  | 89990  | 99900  | 10%      | -0.00893  | $7.937^{-5}$ |
   | 合计             | 10000 | 90000  | 100000 | 10%      | 4.4033788 | 0.0391411    |

   我们从上表可以看出，当特征A（是否为公司VIP）取值1时，其响应比例达到了90%，非常的高，但是我们能否说特征的预测能力非常强呢？不能。为什么呢？原因就在于，A取1时，响应比例虽然很高，但这个分组的客户数太少了，占的比例太低了。虽然，如果一个客户在A这个特征上取1，那他有90%的响应可能性，但是一个客户特征A取1的可能性本身就非常的低。所以，对于样本整体来说，特征的预测能力并没有那么强。

   我们分别看一下变量各分组和整体的$WOE$，$IV$。从这个表我们可以看到，变量取1时，响应比达到90%，对应的$WOE$很高，但对应的$IV$却很低，原因就在于$IV$在$WO$E的前面乘以了一个系数$(py_i-pn_i)$，而这个系数很好的考虑了这个分组中样本占整体样本的比例，比例越低，这个分组对变量整体预测能力的贡献越低。相反，如果直接用$WOE$的绝对值加和，会得到一个很高的指标，这是不合理的。

# IV的极端情况以及处理方式

IV依赖WOE，并且IV是一个很好的衡量自变量对目标变量影响程度的指标。但是，使用过程中应该注意一个问题：特征的任何分组中，不应该出现响应数=0或非响应数=0的情况。

原因很简单，当变量一个分组中，响应数=0时，此时对应的$IV_i$为$-∞$。$WOE_i=ln(\frac{0/n_i}{y_T/n_T})=-∞$

而当变量一个分组中，没有响应的数量 = 0时，此时的$IV_i$为$+∞$。$WOE_i=ln(\frac{y_i/0}{y_T/n_T})=+∞$

那么，遇到响应比例为0或者100%的情况，我们应该怎么做呢？建议如下：

1. 如果可能，直接把这个分组做成一个规则，作为模型的前置条件或补充条件；

2. 重新对变量进行离散化或分组，使每个分组的响应比例都不为0且不为100%，尤其是当一个分组个体数很小时（比如小于100个），强烈建议这样做，因为本身把一个分组个体数弄得很小就不是太合理。

3. 如果上面两种方法都无法使用，建议人工把该分组的响应数和非响应的数量进行一定的调整。如果响应数原本为0，可以人工调整响应数为1，如果非响应数原本为0，可以人工调整非响应数为1.
   













## 随机森林特征重要性Identifying churn drivers with Random Forests 

决策树类算法的特点之一就是有良好的模型解释性。我们可以分析出得到相应结果的数据原因，也可以得到哪些特征比较重要。下面来回顾一下得到这些的主要方法：

1，**平均不纯度减少（MDI）**：表示每个特征对误差的平均减少程度。《统计学习要素》的作者非常简洁的解释了这种方法：“在每一棵树的每一个分裂中，分裂准则的改进是对分裂变量的重要度量，并分别在森林中的所有树上为每个变量累积。”让我们详细说明一下这段话的意思。如我们所知，决策树根据一些规则，将结点分裂为两个子结点。每次分裂都是针对一个可以使误差最小化的特征。误差的计算可以使均方误差，基尼纯度，信息增益，或者其他一些根据需要设置的指标。我们总结了所有树上，这个特定变量得到的所有分割使误差减少的情况。在sk-learn包中，每次分裂带来的提升效果，是由到达节点的样本数加权得到的，然后对特征的重要性进行归一化处理。值得注意的是，这种方法往往高估了具有许多类别的特性的重要性。这里描述了一种纠正MDI偏置的替代方法。

2，**平均精确率减少（MDA）**：打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。这种巧妙的方法利用袋外数据来计算重要性。OOB数据是训练集的一部分，但不用于训练这种特殊的树。用OOB数据计算出基本误差，然后对每个特征，随机打乱顺序。实际上，这就像用相同的分布使用随机数据替换变量一样，并忽视树对该特性的已有知识。对于不重要的特征来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的特征来说，打乱顺序就会降低模型的精确率。

3，**Boruta**：重复删除比最佳特征差的特征。主要思想就是检查比随机噪声重要的特征。首先我们要建立影子变量将所有特征混合。这就像在“减少平均精度”中描述的变量打乱一样，但这个方法是同时对所有变量进行操作。我们将影子特征加入到原有特征中，然后用随机森林进行训练。使用上述介绍的MDA或者MDI方法，我们可以看到哪个原始变量比影子变量重要。如果不相关的特征较少，则重要性度量更精确。因此，上述过程重复到预定义的次数，或者直到达到最小特征计数为止。这个算法从最不相关的特征开始删除，因此我们可以用删除顺序作为特征重要性排序。Boruta是一个“相关”的特征选择算法。这与通过确定最佳预测精度得到的最小数据集方法有细微的区别。正如该方法的作者所说的那样：“这个算法尝试找到所有对预测结果有用的特征，而不是找到一个使误差最小的特征集合。”







1：对于随机森林中的每一颗决策树,使用相应的OOB(袋外数据)数据来计算它的袋外数据误差,记为errOOB1.

2:  随机地对袋外数据OOB所有样本的特征X加入噪声干扰(就可以随机的改变样本在特征X处的值),再次计算它的袋外数据误差,记为errOOB2.

3：假设随机森林中有Ntree棵树,那么对于特征X的重要性=∑(errOOB2-errOOB1)/Ntree,之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高。

我们这里只介绍用基尼指数来评价的方法，想了解另一种方法的可以参考文献2。 
我们将变量重要性评分（variable importance measures）用VIMVIM来表示，将Gini指数用GIGI来表示，假设有mm个特征X1，X2，X3，...，XcX1，X2，X3，...，Xc，现在要计算出每个特征XjXj的Gini指数评分VIM(Gini)jVIMj(Gini)，亦即第jj个特征在RF所有决策树中节点分裂不纯度的平均改变量。 
Gini指数的计算公式为 
GIm=∑|K|k=1∑k′≠kpmkpmk′=1−∑|K|k=1p2mkGIm=∑k=1|K|∑k′≠kpmkpmk′=1−∑k=1|K|pmk2
其中，KK表示有KK个类别，pmkpmk表示节点mm中类别kk所占的比例。 
直观地说，就是随便从节点mm中随机抽取两个样本，其类别标记不一致的概率。 
特征XjXj在节点mm的重要性，即节点mm分枝前后的GiniGini指数变化量为 
VIM(Gini)jm=GIm−GIl−GIrVIMjm(Gini)=GIm−GIl−GIr
其中，GIlGIl和GIrGIr分别表示分枝后两个新节点的GiniGini指数。 
如果，特征XjXj在决策树ii中出现的节点在集合MM中，那么XjXj在第ii颗树的重要性为 
VIM(Gini)ij=∑m∈MVIM(Gini)jmVIMij(Gini)=∑m∈MVIMjm(Gini)   一个决策树中多次出现特征
假设RFRF中共有nn颗树，那么 
VIM(Gini)j=∑ni=1VIM(Gini)ijVIMj(Gini)=∑i=1nVIMij(Gini)
最后，把所有求得的重要性评分做一个归一化处理即可。 
VIMj=VIMj∑ci=1VIMi
 ———————————————— 
版权声明：本文为CSDN博主「zjuPeco」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/zjuPeco/article/details/77371645

xgboost使用得分作为特征的重要性