<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[矩阵求导]]></title>
    <url>%2F2018%2F09%2F23%2F%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[C++STL]]></title>
    <url>%2F2018%2F08%2F30%2FC-STL%2F</url>
    <content type="text"><![CDATA[组件 描述 容器（Containers） 容器是用来管理某一类对象的集合。C++ 提供了各种不同类型的容器，比如 deque、list、vector、map 等。 算法（Algorithms） 算法作用于容器。它们提供了执行各种操作的方式，包括对容器内容执行初始化、排序、搜索和转换等操作。 迭代器（iterators） 迭代器用于遍历对象集合的元素。这些集合可能是容器，也可能是容器的子集。 容器 容器 特征 内存结构 可随机存取 元素搜寻速度 头文件 vector 在序列尾部进行插入和删除，访问和修改元素的时间复杂度为O(1)，但插入和删除的时间复杂度与到末尾的距离成正比。 单端数组 可以 慢 &lt;vector&gt; list 对任意元素的访问与两端的距离成正比，但对某个位置的插入和删除花费为常数时间，即O(1) 双向链表 否 非常慢 &lt;list&gt; deque 与vector基本相同，唯一不同的是，在序列头部插入和删除的时间复杂度也是O(1) 双端数组 可以 慢 &lt;deque&gt; set 由节点组成的红黑树，具有快速查找的功能 二叉树 否 快 &lt;set&gt; multiset 可以支持重复元素，同样具有快速查找能力 二叉树 否 快 &lt;set&gt; map 由{键，值}对组成的集合，同样具有快速查找能力 二叉树 对key而言可以 对key而言快 &lt;map&gt; multimap 一个键可以对应于多个值，同样具有快速查找能力 二叉树 否 对key而言快 &lt;map&gt; 算法算法是用来操作容器中数据的模板函数，它抽象了对数据结构的操作行为。要使用STL中定义的算法，应首先引入&lt;algorithm&gt;头文件。例如STL中的sort()函数可以对容器中的数据进行排序，可以使用find()函数来搜索容器中的某个元素。这里的算法可以与C#中泛型方法进行对比来理解。 迭代器STL实现要点是将容器和算法分开，使两者彼此独立。迭代器使两个联系起来，迭代器提供访问容器中的方法。迭代器实质上是一种智能指针，它重载了-&gt;和*操作符。事实上，C++指针也是一种迭代器。在C#中同样有迭代器的概念，具体参考MSDN.aspx)，不同的是，在C++ 中迭代器分为五类，这五类分别为： 输入迭代器（Input Iterator）——提供对数据的只读访问； 输出迭代器（Output Iterator）——提供对数据的只写访问； 前推迭代器（Forward Iterator）——提供对数据的读写操作，并能向前推进的迭代器； 双向迭代器（Bidirectional Iterator）——提供对数据的读写操作，并能向前和向后操作； 随机访问迭代器（Random Access Iterator）——提供对数据的读写操作，并能在数据中随机移动。 函数对象函数对象，又称为仿函数，STL中的函数对象就是重载了运算符()的模板类的对象，因为该类对象的调用方式类似与函数的调用方式，所以称为函数对象. 适配器适配器是用来修改其他组件接口，与设计模式中的适配器模的达到的效果是一样的。STL中定义了3种形式的适配器：容器适配器、迭代器适配和函数适配器 容器适配器——包括栈（stack）、队列（queue）和优先队列（priority_queue），容器适配器是对基本容器类型进行进一步的封装，从而转换为新的接口类型。 迭代器适配器——对STL中基本迭代器的功能进行扩展，该类适配器包括反向迭代器、插入迭代器和流迭代器。 函数适配器——通过转换或修改来扩展其他函数对象的功能。该类适配器有否定器、绑定器和函数指针适配器。函数对象适配器的作用就是使函数转化为函数对象，或将多参数的函数对象转换为少参数的函数对象，如STL中bind2nd()就是绑定器。 空间配置器当容器中保存的是用户自定义类型数据时，有的数据类型结构简单，占用的空间很小，而有的数据类型结构复杂，占用的内存空间较大；并且有的应用程序需要频繁地进行数据的插入删除操作，这样就需要对内存空间进行频繁地申请和释放工作，然而对内存的频繁操作，会产生严重的性能问题，为了解决这个问题，STL中提供了两个空间配置器，一个是简单空间配置器，仅仅对C运行库中malloc和free进行了简单的封装操作，另一个是“基于内存池的控件配置器”，即容器在每次申请内存的时候，内存池会基于一定的策略，向操作系统申请交大的内存空间，从而避免每次都向OS申请内存。STL中的空间配置器就是负责内存的分配和释放的工作]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>STL</tag>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++多态]]></title>
    <url>%2F2018%2F08%2F30%2FC-%E5%A4%9A%E6%80%81%2F</url>
    <content type="text"><![CDATA[虚函数虚函数 是在基类中使用关键字 virtual 声明的函数。在派生类中重新定义基类中定义的虚函数时，会告诉编译器不要静态链接到该函数。 我们想要的是在程序中任意点可以根据所调用的对象类型来选择调用的函数，这种操作被称为动态链接，或后期绑定。 多态父类调用子类的函数需要用到虚函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;iostream&gt;using namespace std;class Shape &#123;protected: int width, height;public: Shape(int a = 0, int b = 0) &#123; width = a; height = b; &#125; virtual int area() &#123; cout &lt;&lt; "Parent class area :" &lt;&lt; endl; return 0; &#125;&#125;;class Rectangle : public Shape&#123;public: Rectangle(int a = 0, int b = 0) :Shape(a, b) &#123; &#125; int area() &#123; cout &lt;&lt; "Rectangle class area :" &lt;&lt; endl; return (width * height); &#125;&#125;;class Triangle : public Shape&#123;public: Triangle(int a = 0, int b = 0) :Shape(a, b) &#123; &#125; int area() &#123; cout &lt;&lt; "Triangle class area :" &lt;&lt; endl; return (width * height / 2); &#125;&#125;;// 程序的主函数int main()&#123; Shape *shape; Rectangle rec(10, 7); Triangle tri(10, 5); // 存储矩形的地址 shape = &amp;rec; // 调用矩形的求面积函数 area shape-&gt;area(); // 存储三角形的地址 shape = &amp;tri; // 调用三角形的求面积函数 area shape-&gt;area(); return 0;&#125;]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>多态</tag>
        <tag>虚函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双循环链表]]></title>
    <url>%2F2018%2F08%2F28%2F%E5%8F%8C%E5%BE%AA%E7%8E%AF%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[静态链表]]></title>
    <url>%2F2018%2F08%2F28%2F%E9%9D%99%E6%80%81%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[LDA线性判别分析]]></title>
    <url>%2F2018%2F08%2F13%2FLDA%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[欲使同类样本的投影点尽可能接近，可以让同类样本的投影点的协方差尽可能小；而欲使异类样本的投影点尽可能远离，可以让异类样本的类中心之间的距离尽可能大 LDA思想LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。可能还是有点抽象，我们先看看最简单的情况。假设我们有两类数据 分别为红色和蓝色，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。 同类的数据点尽可能的接近（within class） 不同类的数据点尽可能的分开（between class） 上图中国提供了两种投影方式，哪一种能更好的满足我们的标准呢？从直观上可以看出，右图要比左图的投影效果好，因为右图的黑色数据和蓝色数据各个较为集中，且类别之间的距离明显。左图则在边界处数据混杂。以上就是LDA的主要思想了，当然在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面。 算法流程输入：数据集$D=\{(x_1,y_1), (x_2,y_2), …,((x_m,y_m))\}$,其中任意样本$x_i$为$n$维向量，$y_i \in \{C_1,C_2,…,C_k\}$，降维到的维度$d$。输出：降维后的样本集$D′$ 计算类内散度矩阵$S_w$$X_j(j=1,2…k)$为第$j$类样本的集合;$\mu_j(j=1,2…k)$为第$j$类样本的均值向量 S_w = \sum\limits_{j=1}^{k}S_{wj} = \sum\limits_{j=1}^{k}\sum\limits_{x \in X_j}(x-\mu_j)(x-\mu_j)^T 计算类间散度矩阵$S_b$$N_j(j=1,2…k)$为第$j$类样本的个数;$μ$为所有样本均值向量。 S_b = \sum\limits_{j=1}^{k}N_j(\mu_j-\mu)(\mu_j-\mu)^T 计算矩阵$S^{−1}wS_b$ 计算$S^{−1}wS_b$的最大的$d$个特征值和对应的$d$个特征向量($w_1,w_2,…w_d)$,得到投影矩阵 对样本集中的每一个样本特征$x_i$,转化为新的样本$z_i=W^Tx_i$ 得到输出样本集$D’=\{(z_1,y_1), (z_2,y_2), …,((z_m,y_m))\}$ 推导欲使同类样本的投影点尽可能接近，可以让同类样本的投影点的协方差尽可能小；而欲使异类样本的投影点尽可能远离，可以让异类样本的类中心之间的距离尽可能大 二分类假设我们的数据集$D=\{(x_1,y_1), (x_2,y_2), …,((x_m,y_m))\}$,其中任意样本$x_i$为$n$维向量，$y_i \in \{0,1\}$。我们定义$N_j(j=0,1)$为第$j$类样本的个数，$X_j(j=0,1)$为第$j$类样本的集合，而$\mu_j(j=0,1)$为第j类样本的均值向量，定义$\Sigma_j(j=0,1)$为第$j$类样本的协方差矩阵（严格说是缺少分母部分的协方差矩阵）。 $\mu_j $的表达式为： \mu _j = \frac{1}{N_j}\sum\limits_{x \in X_j}x\;\;(j=0,1)$Σ_j$的表达式为： \Sigma_j = \sum\limits_{x \in X_j}(x-\mu_j)(x-\mu_j)^T\;\;(j=0,1)由于是两类数据，因此我们只需要将数据投影到一条直线上即可。假设我们的投影直线是向量$w$,则对任意一个样本本$x_i$,它在直线$w$的投影为$w^Tx_i$,对于我们的两个类别的中心点$μ_0,μ_1$,在直线$w$的投影为$w^Tμ_0$和$w^Tμ_1$。由于LDA需要让不同类别的数据的类别中心之间的距离尽可能的大，也就是我们要最大化$||w^Tμ_0−w^Tμ_1||_2^2$,同时我们希望同一种类别数据的投影点尽可能的接近，也就是要同类样本投影点的协方差$w^TΣ_0w$和$w^TΣ_1w$尽可能的小，即最小化$w^TΣ_0w+w^TΣ_1w$。投影之后的协方差$WX(WX)^T$则$WXX^TW^T=WΣW^T$综上所述，我们的优化目标为： \underbrace{arg\;max}_w\;\;J(w) = \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w+w^T\Sigma_1w} = \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}我们一般定义类内散度矩阵 $S_w$ 为： $S_w = \Sigma_0 + \Sigma_1 = \sum \limits_{x \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum \limits_{x \in X_1}(x-\mu_1)(x-\mu_1)^T$ 同时定义类间散度矩阵$S_b$为：$S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T$ 这样我们的优化目标重写为： \underbrace{arg\;max}_w\;\;J(w) = \frac{w^TS_bw}{w^TS_ww}因为分子分母都是关于$w$的二次型，若$w$是一个解，则对任意常数$α$，$αw$也是一个解。因此解与w的长度无关，只与其方向有关。不妨令$w^TS_ww=1$，则最优化目标等价于 \begin{eqnarray} &\min_w& -w^TS_bw\\ &s.t.& w^TS_ww=1 \end{eqnarray}引入拉格朗日乘子，上式等价于 \begin{eqnarray} &\min& c(w)=-w^TS_bw+\lambda(w^TS_ww-1) \end{eqnarray}求导得 \frac{dc}{dw}=-2S_bw+2\lambda S_ww令其等于0，得 S_bw=\lambda S_ww如果$S_w$可逆，等式两边同乘$S_w^{-1}$有 S_w^{-1}S_bw=\lambda w可喜的发现$w$就是矩阵 $S_w^{-1}S_b$ 的特征向量，因此求解问题转化成求矩阵特征值问题上了，首先求出 $S_w^{-1}S_b$ 的特征值，然后取前 $K$ 个特征向量按列组成 w 矩阵即可 多分类假设我们的数据集$D=\{(x_1,y_1), (x_2,y_2), …,((x_m,y_m))\}$其中任意样本$x_i$为$n$维向量，$y_i \in \{C_1,C_2,…,C_k\}$。我们定义$N_j(j=1,2…k)$为第$j$类样本的个数，$X_j(j=1,2…k)$为第$j$类样本的集合，而$\mu_j(j=1,2…k)$为第$j$类样本的均值向量，定义$\Sigma_j(j=1,2…k)$为第$j$类样本的协方差矩阵。在二类LDA里面定义的公式可以很容易的类推到多类LDA。 由于我们是多类向低维投影，则此时投影到的低维空间就不是一条直线，而是一个超平面了。假设我们投影到的低维空间的维度为$d$，对应的基向量为$(w_1,w_2,…w_d)$，基向量组成的矩阵为$W$, 它是一个$n×d$的矩阵。 此时我们的优化目标应该可以变成为:\frac{W^TS_bW}{W^TS_wW} 其中类间散度$S_b = \sum\limits_{j=1}^{k}N_j(\mu_j-\mu)(\mu_j-\mu)^T$,$μ$为所有样本均值向量。 类内散度$S_w = \sum\limits_{j=1}^{k}S_{wj} = \sum\limits_{j=1}^{k}\sum\limits_{x \in X_j}(x-\mu_j)(x-\mu_j)^T$ 由于现在分子分母都是矩阵，要将矩阵变成实数，可以取矩阵的行列式或者矩阵的迹。其中，矩阵的行列式等于矩阵特征值之积，矩阵的迹等于矩阵特征值之和。所以优化目标可以转化为： 常见的一个LDA多类优化目标函数定义为： \max_W \frac{tr(W^TS_bW)}{tr(W^TS_wW)}\\ or\\ \max_W \frac{|W^TS_bW|}{|W^TS_wW|}可以通过如下广义特征值求解： S_bW=\lambda S_wW $W$的解则是$S_w^{-1}S_b$的$N-1$个最大广义特征值所对应的特征向量按列组成的矩阵。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>特征工程</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xgboost]]></title>
    <url>%2F2018%2F08%2F13%2Fxgboost%2F</url>
    <content type="text"><![CDATA[简介下图就是CART树和一堆CART树的示例，用来判断一个人是否会喜欢计算机游戏 算法 模型我们希望训练出 $K$ 颗树，将它们集成起来从而预测我们的$y$。我们可以用以下公式表示： \hat{y}=\sum_{k=1}^Kf_k(x_i)\\ f(x)=w_{q(x)}在这里，我们用一个函数 $f_k(x)$ 来表示一颗决策树，那个函数 $f$ 可以理解为将样本x映射到树的某个叶子结点中，树中的每个叶子结点都会对应着一个权重$w$。如图，这就是提升树的一个例子，这里一共有两颗树，意味着我们有两个函数 $f_1,f_2，K=2$ ，然后将样本分别放到我们的两颗树中，就可以计算出两个值，把它加起来就是我们要预测的y 目标函数$K$ 表示有 $K$ 棵树，$f_k$ 相当于第 $k$ 棵。因此我们的目标函数可以写成 Obj=\sum_il(\hat{y_i},y)+\sum_k\Omega(f_k)\\ where\ \Omega(f)=\gamma T+\frac{1}{2}\lambda||w||^2其中 $l$ 是可导且凸的损失函数，用来衡量 $\hat{y}$ 与 $y$ 的相近程度，第二项 $Ω$ 则是正则项，它包含了两个部分，第一个是 $γT$，这里的 $T$ 表示叶子结点的数量，$γ$ 是超参，也就是说如果 $γ$ 越大，那么我们的叶子结点数量就会越小。另外一部分则是L2正则项，通过对叶子结点的权重进行惩罚，使得不会存在权重过大的叶子结点防止过拟合。$w$ 就表示叶子结点的权重。 梯度提升假设第 $t$ 轮的预测值为$y^{(t)}$ ，第 $t$ 颗回归树为 $f_t(x)$。则模型迭代如下: \begin{align}\hat y_i^{(0)} &= 0 \\ y_i^{(0)} & = f_1(x_i)= \hat y_i^{(0)}+f_1(x_i) \\ y_i^{(2)}&=f_1(x_i)+f_2(x_i)= \hat y_i^{(1)}+f_2(x_i) \\ &\cdots \\ y_i^{(t)}&=\sum_{k=1}^tf_k(x_i)= \hat y_i^{(t-1)}+f_t(x_i) \end{align}但是对于上面这么一个目标函数，我们是很难进行优化的，于是我们将它变换一下，我们通过每一步增加一个基分类器 $f_t(x)$ ，贪婪地去优化这个目标函数，使得每次增加 $f_t(x)$，都使得loss变小。如此一来，我们就得到了一个可以用于评价当前分类器 $f_t(x)$ 性能的一个评价函数： \begin{align*} Obj^{(t)}&=\sum_{i=1}^nl(y_i,\hat{y_i}^{(t)})+ \sum_{i=1}^t\Omega(f_t) \\ &=\sum_{i=1}^nl(y_i,\hat{y_i}^{(t-1)}+f_t(x_i))+\Omega(f_t) + constant \end{align*}选取一个 $f_t(x)$来使得我们的目标函数尽量最大地降低。$constant$就是前 $t-1$ 棵树的复杂度 泰勒展开 因为 $l(y_i,\hat y_i^{(t-1)})$ 是常数字所以最优化可以化简为下式子 \begin{align*} Obj^{(t)} &=\sum_{i=1}^n [ g_if_t(x_i)+\frac 1 2 h_if_t^2(x_i)] + \Omega (f_t) \\ &=\sum_{i=1}^n [ g_iw_q(x_i)+\frac 1 2 h_iw_q^2(x_i)] + \gamma T + \lambda \frac{1}{2}\sum _{j=1}^Tw_j^2\\ &= \sum_{j=1}^T [( \sum_{i \in I_j} g_i)w_j+\frac 1 2(\sum_{i \in I_j} h_i + \lambda)w_j^2] + \gamma T \\ &= \sum_{j=1}^T [G_j w_j + \frac 1 2 (H_j + \lambda) w_j^2] + \gamma T \end{align*}$j$为叶子结点的序号，$T$ 为叶子结点的总数 ；$i$ 为样本的序号，$n$ 为样本的总数；$w_q(x_i)$是求取$x_i$权值的对应函数；$\sum_{i \in I_j} g_i$ 为同一叶子结点的 $g_i$ 的和；$\sum_{i \in I_j} g_i w_j$ 为同一结点的 $g_i w_j$ ;$\sum_{j=1}^T \sum_{i \in I_j} = \sum_{i=1}^n$。其中 g_i=\frac{\partial l(y_i,\hat{y_i}^{(t-1)})}{\partial \hat{y_i}^{(t-1)}} \quad h_i=\frac{\partial ^2l(y_i,\hat{y_i}^{(t-1)})}{\partial ^2\hat{y_i}^{(t-1)}} \\ G_j = \sum_{i \in I_j} g_i \quad H_j = \sum_{i \in I_j} h_i求取基模型$f_t(x)$-叶子结点权值$w$ $f_t(x_i)$ 是什么？它其实就是 $f_t$ 的某个叶子结点的值 $w$ 。之前我们提到过，叶子结点的值是可以作为模型的参数 Obj^{(t)} = \sum_{j=1}^T [G_j w_j + \frac 1 2 (H_j + \lambda) w_j^2] + \gamma T令$\frac{\partial Obj^{(t)}}{\partial w}=0$ 得到 w_j^* = - \frac {G_j} {H_j + \lambda}带入上式得到 Obj^{(t)} = - \frac 1 2 \sum_{j=1}^T \frac {G_j^2} {H_j + \lambda} + \gamma T划分点\begin{align} Obj_{split} &= - \frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda}] + \gamma T_{split} \\ Obj_{noSplit} &= - \frac 1 2 \frac {(G_L + G_R)^2}{H_L + H_R + \lambda} + \gamma T_{noSplit} \\ Gain &= Obj_{noSplit} - Obj_{split} \\ &= \frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda} - \frac {(G_L + G_R)^2}{H_L + H_R + \lambda}] - \gamma(T_{split} - T_{nosplit}) \\ &=\frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda} - \frac {(G_L + G_R)^2}{H_L + H_R + \lambda}] - \gamma \end{align}因为是二分类，二叉树所以$T_{split} - T_{nosplit} = 1$，$Gain$越大越好 总结目标函数与叶子结点权值w_j^* = - \frac {G_j} {H_j + \lambda}Obj= - \frac 1 2 \sum_{j=1}^T \frac {G_j^2} {H_j + \lambda} + \gamma T其中 g_i=\frac{\partial l(y_i,\hat{y_i}^{(t-1)})}{\partial \hat{y_i}^{(t-1)}} \quad h_i=\frac{\partial ^2l(y_i,\hat{y_i}^{(t-1)})}{\partial ^2\hat{y_i}^{(t-1)}} \\ G_j = \sum_{i \in I_j} g_i \quad H_j = \sum_{i \in I_j} h_i打分函数示例Obj代表了当我们指定一个树的结构的时候，我们在目标上面最多减少多少。我们可以把它叫做结构分数(structure score) 分裂结点论文中给出了两种分裂结点的方法，贪心算法遍历所有分割点进行划分挑选增益最大的切分点。近似算法:对于数据量大的情况下进行近似算法 贪心法：直观的方法是枚举所有的树结构，并根据上面数structure score来打分，找出最优的那棵树加入模型中，再不断重复。但暴力枚举根本不可行，所以类似于一般决策树的构建，XGBoost也是采用贪心算法，每一次尝试去对已有的叶子加入一个分割。对于一个具体的分割方案，增益计算如下： Gain=\frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda} - \frac {(G_L + G_R)^2}{H_L + H_R + \lambda}] - \gamma对于每次扩展，我们还是要枚举所有可能的分割方案，如何高效地枚举所有的分割呢？我假设我们要枚举所有 $x &lt; a$ 这样的条件，对于某个特定的分割 $a$ 我们要计算 $a$ 左边和右边的导数和。对于所有的 $a$，首先根据需要划分的那列特征值排序，然后从左到右的扫描就可以枚举出所有分割的梯度和$G_L$和$G_R$，再用上面的公式计算每个分割方案的分数就可以了。观察这个目标函数，大家会发现第二个值得注意的事情就是引入分割不一定会使得情况变好，因为我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。大家可以发现，当我们正式地推导目标的时候，像计算分数和剪枝这样的策略都会自然地出现，而不再是一种因为heuristic（启发式）而进行的操作了。 算法说明上面是针对一个特征，如果有m个特征，需要对所有参数都采取一样的操作，然后找到最好的那个特征所对应的划分。 近似算法XGBoost使用exact greedy算法来寻找分割点建树，但是当数据量非常大难以被全部加载进内存时或者分布式环境下时，exact greedy算法将不再合适。因此作者提出近似算法来寻找分割点。近似算法的大致流程见下面的算法。该算法会首先根据特征分布的百分位数 (percentiles of feature distribution)，提出候选划分点 (candidate splitting points)。接着，该算法将连续型特征映射到由这些候选点划分的分桶(buckets) 中，聚合统计信息，基于该聚合统计找到在 proposal 间的最优解。 Global：学习每棵树前，提出候选切分点； Local：每次分裂前，重新提出候选切分点； 第一个for循环做的工作：对特征 $K$ 根据该特征分布的分位数找到切割点的候选集合 $S_k = \{s_{k1}, s_{k2}, … ,s_{kl} \}$；这样做的目的是提取出部分的切分点不用遍历所有的切分点。其中获取某个特征K的候选切割点的方式叫proposal。主要有两种proposal方式：global proposal和local proposal。 第二个for循环的工作：将每个特征的取值映射到由这些该特征对应的候选点集划分的分桶(buckets)区间 $\{s_{k,v}≥x_{jk}&gt;s_{k,v−1}\}$ 中，对每个桶（区间）内的样本统计值 $G,H$ 进行累加统计，最后在这些累计的统计量上寻找最佳分裂点。这样做的主要目的是获取每个特征的候选分割点的 $G,H$ 量。 近似算法举例 xgboost与GBDT的区别 Xgboost是GB算法的高效实现，xgboost中的基学习器除了可以是CART（gbtree）也可以是线性分类器（gblinear）。 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子结点个数、每个叶子结点上输出的score的L2模的平方和 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 支持并行化处理。xgboost的并行是在特征粒度上的，在训练之前，预先对特征进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行结点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。 可以处理稀疏、缺失数据(结点分裂算法能自动利用特征的稀疏性),可以学习出它的分裂方向，加快稀疏计算速度。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>模型</tag>
        <tag>树模型</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT]]></title>
    <url>%2F2017%2F10%2F20%2FGBDT%2F</url>
    <content type="text"><![CDATA[简介GBDT也是集成学习Boosting家族的成员,由梯度提升方法与回归树结合而成。分类|损失函数回归|$(y-\hat y)^2$分类|$p_K log_2 \; p_K$ 回归树回归树生成算法 提升树提升树可以表示为以下形式：这里我们约定 $T(x;Θ_m)$ 表示第 $m$ 棵决策树；$Θ_m$表示决策树的参数；$M$ 为树的个数。强分类器 $f_M(x)$ 可以由多个弱分类器 $T(x;Θ_m)$ 线性相加而成 f_M (x)=\sum_{m=1}^MT(x;Θ_m )提升树的前向分步算法。第$m$步的模型可以写成 f_m (x)=f_{m-1} (x)+ T(x;Θ_m )然后得到损失函数 L(f_m (x),y)=L(f_{m-1} (x)+ T(x;Θ_m ),y)迭代的目的是构建 $T(x;Θ_m)$，使得本轮损失 $L(f_m(x),y)$ 最小。思想其实并不复杂，但是问题也很明显，对于不同的任务会有不同的损失函数，当损失函数是平方损失和指数损失函数时，每一步的优化还是简单的。但是对于一般损失函数而言，每一步的优化并不容易 提升树算法 梯度提升树采用泰勒展开式将上式中的残差展开， 泰勒公式一元函数在点$x_k$处的泰勒展开式为： f(x) = f(x_k)+(x-x_k)f'(x_k)+\frac{1}{2!}(x-x_k)^2f''(x_k)+o^n拟合残差的近似梯度提升思想正是为了解决上面的问题。它的主要思想是先求$h_m$，再求$β_m$。观察式子 \sum _{i=1}^N = L(y_i,f_{m-1}(x_i)+\beta h_m(x_i))我们要最小化的式子由N部分相加而成，如果能够最小化每一部分，自然也就最小化了整个式子。考察其中任一部分，并将其进行泰勒一阶展开 L(y_i,f_{m-1}(x_i)+\beta h_m(x_i))=L(y_i,f_{m-1}(x_i))+\beta h_m(x_i)\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)} \\ L(y_i,f_{m-1}(x_i)+\beta h_m(x_i))-L(y_i,f_{m-1}(x_i))=\beta h_m(x_i)\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}由于需要 L(y_i,f_{m-1}(x_i)+\beta h_m(x_i))-L(y_i,f_{m-1}(x_i))]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>模型</tag>
        <tag>树模型</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM支持向量机]]></title>
    <url>%2F2017%2F10%2F18%2FSVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[简介支持向量机（support vector machines，SVM）是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。构建一个超平面将数据点分开,使所有数据距离超平面的距离最大。其中红色与蓝色的数据点成为支持向量 知识体系结构 学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机； 当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机； 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。 算法模型输入：是线性可分的 $m$ 个样本${(x_1,y_1), (x_2,y_2), …, (x_m,y_m),}$,其中 $x$ 为 $n$ 维特征向量。$y$为二元输出，值为1，或者-1.输出：是分离超平面的参数和$w^{\ast }和b^{\ast }$和分类决策函数。 \begin{align*} &min\;\; \frac{1}{2}||w||_2^2\\ s.t\;\;\;\; & y_i(w\cdot x_i + b) \geq 1 \;\;(i =1,2,...n) \end{align*}求得最优解,$w^{\ast },b^{\ast }$ 推导w^*\cdot x+b^*=0分类决策函数为 f(x)=sign(w^*\cdot x+b^*)其他点$(x_0,y_0)$平面$w\cdot x_i + b=0$的距离 \frac{|w\cdot x_i + b|}{||w||_2^2}因为$y=\pm 1$所以 \frac{y(w\cdot x_i + b)}{||w||_2^2}假设$y(w\cdot x_i + b)\geqslant 1$则 \begin{align*} &min\;\; \frac{1}{2}||w||_2^2\\ s.t\;\;\;\; & y_i(w\cdot x_i + b) \geq 1 \;\;(i =1,2,...n) \end{align*}线性可分支持向量机与硬间隔最大化概念 线性可分支持量机的定义给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为w\cdot x + b = 0以及相应的分类决策函数f(x)=sign(w\cdot x + b )线性可分如下图所示： 支持向量距离超平面最近的数据点，如下图所示的圆圈处的数据点。 硬间隔\left\{\begin{matrix} w\cdot x_{i}+b\geqslant +1 &y_i=+1\\ w\cdot x_{i}+b\leqslant -1 &y_i=-1 \end{matrix}\right.硬间隔：两个异类支持向量到超平面的距离之和如下所示\gamma =\frac{2}{\left \| w \right \|} 数学模型SVM的模型是让所有点到超平面的距离大于一定的距离，也就是所有的分类点要在各自类别的支持向量两边。用数学式子表示为： \begin{align*} &min\;\; \frac{1}{2}||w||_2^2\\ s.t\;\;\;\; & y_i(w\cdot x_i + b) \geq 1 \;\;(i =1,2,...n) \end{align*} 推导略 怎么得到支持向量根据KKT条件中的对偶互补条件$α_i∗(y_i(w\cdot x_i+b)−1)=0$，如果$α_i&gt;0$则有$y_i(w\cdot x_i+b)=1 $即点在支持向量上，否则如果$α_i=0$则有$y_i(w\cdot x_i+b)≥1$，即样本在支持向量上或者已经被正确分类。 算法输入：线性可分训练集$T={(x_1,y_1), (x_2,y_2), …, (x_m,y_m),}$,其中$x$为n维特征向量。$y$为二元输出，值为1，或者-1.输出：是分离超平面的参数和$w^{\ast }和b^{\ast }$和分类决策函数。 构造约束优化问题\begin{align*} &\underbrace{min}_{\alpha} \;\; \frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum\limits_{i=1}^{m} \alpha_i\\ s.t\;\;\;\; & \sum\limits_{i=1}^{m}\alpha_iy_i = 0\\ &\alpha_i \geq 0 \; i=1,2,...m \end{align*} 利用SMO算法求得求出上式最小时对应的$α$向量的值$α^∗$向量. 计算：w^{*} = \sum\limits_{i=1}^{m}\alpha_i^{*}y_ix_i求b则稍微麻烦一点。注意到，对于任意支持向量$(x_s, y_s)$，都有y_s(w\cdot x_s+b) = y_s(\sum\limits_{i=1}^{S}\alpha_iy_ix_ix_s+b) = 1假设我们有S个支持向量，则对应我们求出S个$b^{\ast }$,理论上这些$b^{\ast }$都可以作为最终的结果， 但是我们一般采用一种更健壮的办法，即求出所有支持向量所对应的$b_s^{\ast }$，然后将其平均值作为最后的结果。 线性支持向量机与软间隔最大化概念 线性支持向量机训练数据集不是线性可分的。通常情况是，训练数据中有一些特异点（outlier），将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。这时需要修改硬间隔最大化，使其成为软间隔最大化。 本来如果我们不考虑异常点，SVM的超平面应该是下图中的红色线所示，但是由于有一个蓝色的异常点，导致我们学习到的超平面是下图中的粗虚线所示，这样会严重影响我们的分类模型预测效果。 软间隔SVM对训练集里面的每个样本$(x_i,y_i)$引入了一个松弛变量$ξ_i≥0$,使函数间隔加上松弛变量大于等于1，也就是说：y_i(w\cdot x_i +b) \geq 1- \xi_i 数学模型对每个松弛变量$ξ_i$，支付一个代价$ξ_i$，这个就得到了我们的软间隔最大化的SVM学习条件。目标函数由原来的 \begin{align*} &min\;\; \frac{1}{2}||w||^2 +C\sum\limits_{i=1}^{m}\xi_i \\ s.t. \; \; \;\; &y_i(w\cdot x_i + b) \geq 1 - \xi_i \;\;(i =1,2,...m)\\ &\xi_i \geq 0 \;\;(i =1,2,...m) \end{align*}C&gt;0称为惩罚参数，一般由应用问题决定，C值大时对误分类的惩罚增大，C值小时对误分类的惩罚减小。最小化目标函数包含两层含义：使$\frac{1}{2}||w||^2$尽量小即间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。 支持向量 根据软间隔最大化时KKT条件中的对偶互补条件$α_i∗(y_i(w\cdot x_i+b)−1+ξ_i^∗)=0$我们有 如果$α=0$,那么$y_i(w\cdot x_i+b)−1≥0$,即样本在支持向量上或者已经被正确分类。如图中所有远离支持向量的点。 如果$0≤α≤C$,那么$ξ_i=0,y_i(w\cdot x_i+b)−1=0$,即点在支持向量上。如图中在虚线支持向量上的点。 如果$α=C$，说明这是一个可能比较异常的点，需要检查此时$\xi_i$ 如果$0≤ξ_i≤1$,那么点被正确分类，但是却在超平面和自己类别的支持向量之间。如图中的样本2和4. 如果$ξ_i=1$,那么点在分离超平面上，无法被正确分类。 如果$ξ_i&gt;1$,那么点在超平面的另一侧，也就是说，这个点不能被正常分类。如图中的样本1和3 算法输入：线性可分的$m$个样本$(x_1,y_1),(x_2,y_2),…,(x_m,y_m)$,,其中$x$为$n$维特征向量。$y$为二元输出，值为1，或者-1.输出：是分离超平面的参数和$w^{\ast }$和$b^{\ast }$和分类决策函数。 选择一个惩罚系数C&gt;0, 构造约束优化问题 \begin{align*} &\underbrace{min}_{\alpha} \;\; \frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum\limits_{i=1}^{m} \alpha_i\\ s.t\;\;\;\; & \sum\limits_{i=1}^{m}\alpha_iy_i = 0\\ &0\leqslant \alpha_i \leqslant C \; \;\;i=1,2,...m \end{align*} 用SMO算法求出上式最小时对应的$α$向量的值$α^∗$向量. 计算$w^{} = \sum\limits_{i=1}^{m}\alpha_i^{}y_ix_i$ 找出所有的S个支持向量,即满足$0&lt;α_s&lt;C$对应的样本$(x_s,y_s)$，通过 $y_s(\sum\limits_{i=1}^{S}\alpha_iy_ix_i^Tx_s+b) = 1$，计算出每个支持向量$(x_s,y_s)$对应的$b_s^{\ast }$,计算出这些$b_s^{\ast } = y_s - \sum\limits_{i=1}^{S}\alpha_iy_ix_i^Tx_s$. 所有的$b_s^∗$对应的平均值即为最终的$b^{\ast } = \frac{1}{S}\sum\limits_{i=1}^{S}b_s^{\ast }$ 这样最终的分类超平面为：$w^∗\cdot x+b^∗=0$，最终的分类决策函数为： f(x) = sign(w^{*} \cdot x + b^{*}) 合页损失函数线性支持向量机还有另外一种解释如下： \underbrace{ min}_{w, b}[1-y_i(w \cdot x + b)]_{+} + \lambda ||w||_2^2其中$L(y(w \bullet x + b)) = [1-y_i(w \bullet x + b)]_{+}$称为合页损失函数(hinge loss function)，下标+表示为： [z]_{+}= \begin{cases} z & {z >0}\\ 0& {z\leq 0} \end{cases}也就是说，如果点被正确分类，且函数间隔大于1，损失是0，否则损失是$1−y(w∙x+b)$,如下图中的绿线。我们在下图还可以看出其他各种模型损失和函数间隔的关系：对于0-1损失函数，如果正确分类，损失是0，误分类损失1， 如下图黑线，可见0-1损失函数是不可导的。对于感知机模型，感知机的损失函数是$[-y_i(w \cdot x + b)]_{+}$，这样当样本被正确分类时，损失是0，误分类时，损失是$-y_i(w \cdot x + b)$，如下图紫线。对于逻辑回归之类和最大熵模型对应的对数损失，损失函数是$log[1+exp(−y(w\cdot x+b))]$, 如下图红线所示。 非线性支持向量机与核函数概念 非线性分类问题非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题。如下图所示，无法用直线（线性模型）将正负实例正确分开，但可以用一条椭圆曲线（非线性模型）将它们正确分开。用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。 核函数设$\mathbb{X}$是输入空间（欧氏空间$\mathbb{R}^{n}$的子集或离散集合），又设$\mathbb{H}$为特征空间（希尔伯特空间），如果存在一个从$\mathbb{X}$到$\mathbb{H}$的映射\Phi (x):\mathbb{X}\rightarrow \mathbb{H}使得对所有$x,z\in \mathbb{X}$，函数$K(x,z)$满足条件K(x,z)=\Phi (x)\cdot \Phi (z)则称$K(x,z)$为核函数，$\Phi (x)$为映射函数，式中$\Phi (x)\cdot \Phi (z)$为$\Phi (x)$和$\Phi (z)$的内积。 正定核函数一个函数要想成为正定核函数，必须满足他里面任何点的集合形成的Gram矩阵是半正定的。也就是说,对于任意的$x_i \in \chi ， i=1,2,3…m$，$K(x_i,x_j)$对应的Gram矩阵$K = \bigg[ K(x_i, x_j )\bigg]$是半正定矩阵，则$K(x,z)$是正定核函数。 常用核函数 线性核函数线性核函数（Linear Kernel）其实就是我们前两篇的线性可分SVM，表达式为：K(x, z) = x \cdot z也就是说，线性可分SVM我们可以和线性不可分SVM归为一类，区别仅仅在于线性可分SVM用的是线性核函数。 多项式核函数多项式核函数（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：K(x, z) = （\gamma x \cdot z + r)^d其中，$γ,r,d$都需要自己调参定义。 高斯核函数高斯核函数（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是非线性分类SVM最主流的核函数。libsvm默认的核函数就是它。表达式为：K(x, z) = exp(-\gamma||x-z||^2)其中，$γ$大于0，需要自己调参定义。 Sigmoid核函数Sigmoid核函数（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：K(x, z) = tanh（\gamma x \cdot z + r)其中，$γ,r$都需要自己调参定义。 算法输入：$m$个样本$(x_1,y_1), (x_2,y_2), …, (x_m,y_m),$,其中x为n维特征向量。y为二元输出，值为1，或者-1.输出：分离超平面的参数$w^∗$和$b^∗$和分类决策函数。算法过程如下： 选择适当的核函数$K(x,z)$和一个惩罚系数$C&gt;0$, 构造约束优化问题\begin{align*} &\underbrace{min}_{\alpha} \;\; \frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum\limits_{i=1}^{m} \alpha_i\\ s.t\;\;\;\; & \sum\limits_{i=1}^{m}\alpha_iy_i = 0\\ &0\leqslant \alpha _i\leqslant C \; i=1,2,...m \end{align*} 用SMO算法求出上式最小时对应的$α$向量的值$α^∗$向量 得到$w^{\ast } = \sum\limits_{i=1}^{m}\alpha_i^{\ast }y_i\phi(x_i)$，此处可以不直接显式的计算$w^∗$。 找出所有的S个支持向量,即满足$0 &lt; \alpha_s &lt; C$对应的样本$(x_s,y_s)$，通过 $y_s(\sum\limits_{i=1}^{S}\alpha_iy_iK(x_i,x_s)+b) = 1$，计算出每个支持向量$(x_s,y_s)$对应的$b_s^{\ast }$,计算出这些$b_s^{\ast } = y_s - \sum\limits_{i=1}^{S}\alpha_iy_iK(x_i,x_s)$. 所有的$b_s^{\ast }$对应的平均值即为最终的$b^{*} = \frac{1}{S}\sum\limits_{i=1}^{S}b_s^{\ast }$ 这样最终的分类超平面为：$\sum\limits_{i=1}^{m}\alpha_i^{\ast }y_iK(x, x_i)+ b^{\ast } = 0$，最终的分类决策函数为： f(x) = sign(\sum\limits_{i=1}^{m}\alpha_i^{\ast }y_iK(x, x_i)+ b^{\ast })SMO算法支持向量机原理(一) 线性支持向量机支持向量机原理(二) 线性支持向量机的软间隔最大化模型支持向量机原理(三) 线性不可分支持向量机与核函数支持向量机原理(四) SMO算法原理]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>模型</tag>
        <tag>线性分类器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经元-感知器梯度下降]]></title>
    <url>%2F2017%2F09%2F04%2F%E6%84%9F%E7%9F%A5%E5%99%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[感知器-神经元 输入权值 一个感知器可以接收多个输$(x_1, x_2,…,x_n\mid x_i\in R)$(特征)，每个输入上有一个权值$w_i\in R$。此外还有一个偏置项$b$,就是上图中的$w_0$ 激活函数 感知器的激活函数可以有很多选择，比如我们可以选择下面这个阶跃函数$f$来作为激活函数：f(z)=\left\{\begin{matrix} 1&\: \, z>0\\ 0&\: \,otherwise \end{matrix}\right. 输出 感知器的输出由下面这个公式来计算y=f(\mathrm{w}\bullet\mathrm{x}+b)\qquad偏置项$b$,就是上图中的$w_0$y=f(\mathrm{w}\bullet\mathrm{x})线性单元和梯度下降当感知器为线性函数时候y=\mathrm{w}\bullet\mathrm{x}+b偏置项$b$,就是上图中的$w_0$，$y=\mathrm{w}\bullet\mathrm{x}$其优化如下所示梯度下降法在监督学习下，对于一个样本，我们知道它的特征$x$，以及标记$y$。同时，我们还可以根据模型 $y=\mathrm{w}\bullet\mathrm{x}+b$ 计算得到输出 $\bar{y}$。注意这里面我们用$y$表示训练样本里面的标记，也就是实际值；用带上划线的 $\bar{y}$ 表示模型计算的出来的预测值。我们当然希望模型计算出来的和越接近越好。则损失函数为loss = \frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2 \frac{\partial loss}{\partial w}=\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}^{(i)}\mathrm{w}_{new}=\mathrm{w}_{old}+\eta\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}^{(i)}\begin{bmatrix} w_0 \\ w_1 \\ w_2 \\ ... \\ w_m \\ \end{bmatrix}_{new}= \begin{bmatrix} w_0 \\ w_1 \\ w_2 \\ ... \\ w_m \\ \end{bmatrix}_{old}+\eta\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)}) \begin{bmatrix} 1 \\ x_1^{(i)} \\ x_2^{(i)} \\ ... \\ x_m^{(i)} \\ \end{bmatrix}]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>凸优化</tag>
        <tag>感知器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛顿法和拟牛顿法]]></title>
    <url>%2F2017%2F09%2F03%2F%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[牛顿法（Newton method）和拟牛顿法（quasi Newton method）也是求解无约束最优化问题的常用方法。方法使用函数 $f(x)$的一二阶导数。具有收敛速度快、迭代次数少、计算量大的特点。 问题描述考虑如下无约束极小化问题： \min_{x\in R^n} \; f(x)最优点记为$x^\ast$ 牛顿法 输入：目标函数$f(x)$，梯度$g(x)=\bigtriangledown f(x)$，海森矩阵$H(x)$，精度要求$\varepsilon $ 输出：$f(x)$的极小点$x^\ast$ 确定初始点$x_0$，置$k=0$。 计算 $g_k=g(x^{(k)})$ 若$\left | g_k \right |&lt;\varepsilon $，则停止计算，得近似解$x^\ast=x^{(k)}$ 计算$H_k=H(x^{(k)})$ 置$x^{(k+1)}=x^{(k)}-H_k^{-1}g_k$ 置$k=k+1$，转第二步骤 其中$g_k=\bigtriangledown f(x^{(k)})$，$H_k=\bigtriangledown^2 f(x^{(k)})=\left [\frac{\partial^2 f}{\partial x_i\partial x_j} \right ]_{n\times n}$。$H_{k}^{-1}$为$x^{(k)}$的Hessian的逆矩阵, g(f)=\bigtriangledown f=\begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \\ \end{bmatrix} \; \; \; \; \; H(f)= \bigtriangledown^2 f= \begin{bmatrix} \frac{\partial^2 f}{\partial x^2_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x^2_2} \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\\ \vdots & \ddots & \vdots \\\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} \cdots & \frac{\partial^2 f}{\partial x^2_n} \end{bmatrix}代数推导假设函数 $f(x)$ 二次可微，则在当前第$k$次迭代点 $x^{(k)}$ 对 $f(x)$ 进行二阶泰勒展开 f \left ( x \right ) \approx f \left ( x^{(k)} \right )+{f}'\left ( x^{(k)} \right )\left ( x-x^{(k)} \right )+\frac{1}{2}{f}''\left ( x^{(k)} \right )\left ( x-x^{(k)} \right )^2求函数 $f(x)$ 极值则可以转化为对 $f(x)$ 求导并令其为0， \frac{\partial f}{\partial x} \approx f'\left ( x^{(k)} \right )+{f}''\left ( x^{(k)} \right )\left ( x-x^{(k)} \right )=0得到 \begin{align*} x&=x^{(k)}-\frac{f'\left ( x^{(k)} \right )}{f''\left ( x^{(k)} \right )} \\ &= x^{(k)}-H_{k}^{-1}g_k \\ x^{(k+1)} &= x^{(k)}-H_{k}^{-1}g_k \end{align*}阻尼牛顿法原始牛顿法由于迭代公式中没有步长因子，而是固定步长，有时会使迭代超过极值点，即$f(x^{(k+1)})&gt;f(x^k)$，在牛顿方向上附加了步长因子，每次调整时会在搜索空间，在该方向找到最优步长，然后调整 \lambda _k=arg\; \underset{\lambda \in \mathbb{R}}{min}\; f(x_k+\lambda H_k^{-1}g_k)\;\;\;\;\;\;\; \tag 2 输入：目标函数$f(x)$，梯度$g(x)=\bigtriangledown f(x)$，海森矩阵$H(x)$，精度要求$\varepsilon $ 输出：$f(x)$的极小点$x^\ast$ 确定初始点$x_0$，置$k=0$。 计算 $g_k=g(x^{(k)})$ 若$\left | g_k \right |&lt;\varepsilon $，则停止计算，得近似解$x^\ast=x^{(k)}$ 计算$H_k=H(x^{(k)})$ 利用$(2)$式子得到步长$\lambda_k$置$x^{(k+1)}=x^{(k)}-\lambda_k H_k^{-1}g_k$ 置$k=k+1$，转第二步骤 拟牛顿算法拟牛顿思路 因为Hessian矩阵的计算量大而且无法保持正定所以采用拟牛顿算法。拟牛顿算法的基本思想是不用二阶偏导数而构造出可以近似Hessian矩阵(或者Hessian矩阵的逆矩阵)的正定对称矩阵，在拟牛顿的条件下优化目标函数 拟牛顿条件\begin{align*} \frac{\partial f}{\partial x} &\approx f'\left ( x^{(k)} \right )+{f}''\left ( x^{(k)} \right )\left ( x-x^{(k)} \right )\\ f'\left ( x^{(k+1)} \right ) &\approx f'\left ( x^{(k)} \right )+{f}''\left ( x^{(k)} \right )\left ( x^{(k+1)}-x^{(k)} \right ) \\ g_{k+1}-g_k &\approx H_k\left ( x^{(k+1)}-x^{(k)} \right ) \end{align*} 拟牛顿法对$H_k$或$H_k^{-1}$取近似值，可减少计算量。记$B\approx H$，$D\approx H^{-1}$，$y_k=g_{k+1}-g_k$，$s_k=x_{k+1}-x_k$。根据拟牛顿条件，可得近似公式： \color{red}{B_{k+1}=\frac{y_k}{s_k} \;\;\;\;\;\;\;\;D_{k+1}=\frac{s_k}{y_k}}DFP算法DFP算法采用的是$D$，但并不直接计算$D$，而是计算每一步DD的增量$ΔD$来间接的求出$D$。这也是很多优化算法的做法，因为一般上一步的中间结果对下一步的计算仍有价值，若直接抛弃重新计算耗时耗力耗内存，重新发明了轮子。 D_{k+1}=D_k+\Delta D_k$D_0$通常取单位矩阵I，关键导出每一步的$ΔD_k$。通过一系列艰苦而又卓绝的推导计算假设取便，最终的导出结果为： \color{red}{\begin{align*} D_{k+1}&=D_k+\Delta D_k\; \; \; \; k=0,1,2,\cdots \\ \Delta D_k &=\frac{s_ks_k^T}{s_k^Ty_k} -\frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k} \end{align*}}一般来说，在进行中间增量计算时，都要经过这一步艰苦而又卓绝的推导计算。 BFGS算法BFGS算法与DFP算法类似，只是采用的BB来近似HH。最终的公式为： \color{red}{\begin{align*} \Delta B_k &=\frac{y_ky_k^T}{s_k^Ty_k} -\frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k} \end{align*}}L-BFGSL-BFGS算法对BFGS算法进行改进，不再存储矩阵$D_k$，因为$D_k$有时候比较大，计算机的肚子盛不下。但是我们用到$D_k$的时候怎么办呢？答案是根据公式求出来。 从上面的算法推导可知，$D_k$只跟$D_0$和序列$\{s_k\}$和$\{y_k\}$有关。即我们知道了后者，即可以求得前者。进一步近似，我们只需要序列$\{s_k\}$和$\{y_k\}$的最近$m$个值即可。这样说来，我们的计算机内存中只需要存储这两个序列即可，瞬间卸掉了很多东西，正是春风得意马蹄轻。当然，这样cpu的计算量也会相应的增加，这是可以接受的，马，也是可以接受的。 最终的递推关系为 D_{k+1}=V^T_kD_kV_k+\rho_k s_ks^T_k其中 \rho_k=\frac{1}{y^T_ks_k},V_k=I-\rho_ky_ks^T_k]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>凸优化</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉格朗日]]></title>
    <url>%2F2017%2F09%2F02%2F%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%2F</url>
    <content type="text"><![CDATA[拉格朗日对偶性是解决带约束的最优化问题的方法，在实际应用中，通过拉格朗日对偶原理将原始问题转换成对偶问题，将原来不容易解决的问题转化为一个容易解决的问题，如支持向量机，最大熵模型。 原始问题假设$f(x)，c_i(x),h_j(x)$是定义在$\mathbb{R}^{n}$上的连续可微函数。我们需要求解约束最优化问题： \underset{x\in \mathbb{R}^n}{min} f(x)\begin{align} \mathbb{s.t.}\quad &c_i(x) \le 0,\quad i=1,2,\cdots,k\\ &h_j(x)=0,\quad j=1,2,\cdots,l \end{align}广义拉格朗日函数为了求解原始问题，我们首先引入广义拉格朗日函数(generalized Lagrange function)： L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k \alpha_ic_i(x) + \sum_{j=1}^l\beta_jh_j(x) \tag{4}其中，$x=(x_1,x_2,\cdots,x_n)^T \in \mathbb{R}^n$，$\alpha_i$和$\beta_j$是拉格朗日乘子，特别要求$\alpha_i\geqslant 0$ 原始问题的等价转换：极小极大问题如果把$L(x,\alpha,\beta)$看作是$\alpha、\beta$的函数，求其最大值，即\theta_p(x)=\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta) \tag 5 确定$\alpha、\beta$使$L(x,\alpha,\beta)$取得最大值，（此过程中把$x$看做常量）下面通过$x$是否满足约束条件两方面来分析这个函数 如果$x$ 满足原始问题中约束，由(2)、(3)、(4)、(5)可知 $θ(x)=f(x)$。（少的两项一个是非正的，一个是0，要取最大值的话当然得令两者都为0 如果 $x$ 不满足原始问题中的约束，那么 $θ(x)=+∞$。若某个$i$使约束$c_i(x)&gt;0$，则可令则可令$\alpha \rightarrow +∞$，若某个$j$使得$h_j(x)\neq 0,$,则可令$\beta_j h_j(x) \rightarrow +∞$，而将其余各$\alpha _i、\beta_j$均取为0。 综上： \theta_p(x)=\left\{\begin{matrix} f(x),&x 满足原始问题约束\\ +\infty,&其他 \end{matrix}\right.求解原问题的最小值与原始问题等价 \color{red}{\underset{x\in \mathbb{R}^n}{min}\; \theta_p(x)= \underset{x\in \mathbb{R}^n}{min}\; f(x)= \underset{x\in \mathbb{R}^n}{min}\;\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta) }原始问题和对偶问题的关系 弱对偶关系：弱对偶关系一定存在，若原始问题和对偶问题都有最优值，则\color{red}{\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) \leqslant \underset{x\in \mathbb{R}^n}{min}\;\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta) ={min}\; f(x)} 强对偶关系设$x^\ast$和$a^\ast$,$β^\ast$分别是原始问题$\underset{x\in \mathbb{R}^n}{min}\;\underset{\alpha,\beta:\alpha_i\ge0}{max}\; L(x,\alpha,\beta)$和对偶问题$\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta)$的可行解，并且\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) = \underset{x\in \mathbb{R}^n}{min}\;\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta) ={min}\; f(x)\color{red}{\underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha^\ast ,\beta^\ast ) = \max_{\alpha,\beta:\alpha_i\ge0}L(x^\ast ,\alpha,\beta)={min}\; f(x^\ast )}则$x^\ast$和$a^\ast$,$β^\ast$分别是原始问题和对偶问题的最优解。 弱对偶关系若原始问题和对偶问题都有最优值，则 \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) \leqslant L(x,\alpha,\beta) \leqslant \max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)\color{red}{\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) \leqslant \underset{x\in \mathbb{R}^n}{min}\;\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta) ={min}\; f(x)}推论：设$x^\ast$和$a^\ast$,$β^\ast$分别是原始问题$\underset{x\in \mathbb{R}^n}{min}\;\underset{\alpha,\beta:\alpha_i\ge0}{max}\; L(x,\alpha,\beta) $和对偶问题$\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) $的可行解，并且$\underset{x\in \mathbb{R}^n}{min}\;\underset{\alpha,\beta:\alpha_i\ge0}{max}L(x,\alpha,\beta) =\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) $，则$x^\ast$和$a^\ast$,$β^\ast$分别是原始问题和对偶问题的最优解。 强对偶关系 定理1：判断强对偶关系假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，并且不等式约束$c_i(x)$是严格可行的，即存在$x$,对所有$i$有$c_i(x)&lt;0$，则存在$x^\ast$和$a^\ast,β^\ast$分别是原始问题和对偶问题的解,并且满足 \color{red}{\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha^\ast ,\beta^\ast ) =L(x^\ast,\alpha^\ast ,\beta^\ast )= \underset{x\in \mathbb{R}^n}{min}\;\max_{\alpha,\beta:\alpha_i\ge0}L(x^\ast ,\alpha,\beta) ={min}\; f(x^\ast )} 定理2：KKT条件(求最优解)假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，并且不等式约束$c_i(x)$是严格可行的，则$x^\ast$和$a^\ast,β^\ast$分别是原始问题和对偶问题的解的充分必要条件是$x^\ast,a^\ast,β^\ast$满足下面的Karush-Kuhn-Tucker(KKT)条件：（判断极值、其余项为0） \begin{align*} \nabla_xL(x^\ast,\alpha^\ast,\beta^\ast)&=0\\ \nabla_{\alpha}L(x^\ast,\alpha^\ast,\beta^\ast) &=0 \\ \nabla_{\beta}L(x^\ast,\alpha^\ast,\beta^\ast)&=0 \end{align*}\begin{align*} \alpha_i^*c_i(\boldsymbol{x}^*)&=0,\quad i=1,2,\cdots,k\\ c_i(\boldsymbol{x}^*)&\le0,\quad i=1,2,\cdots,k \\ \alpha_i^*&\ge0,\quad i=1,2,\cdots,k \\ h_j(\boldsymbol{x}^*)&=0,\quad j=1,2,\cdots,l \end{align*}当原问题是凸优化问题时，满足KKT条件的点也是原、对偶问题的最优解。 仿射函数f(x)=A\cdot x+b仿射函数就是一个线性函数，其输入是$n$ 维向量，参数 $A$ 可以是常数，也可以是 $m×n$ 的矩阵，$b$可以是常数，也可以是 $m$ 维的列向量，输出是一个 $m $维的列向量。在几何上，仿射函数是一个线性空间到另一个线性空间的变换。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>凸优化</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[核函数]]></title>
    <url>%2F2017%2F09%2F02%2F%E6%A0%B8%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[之前分析的感知机、主成分分析（Principle component analysis, PCA）包括后面看的支撑向量机（Support vector machines, SVM），都有用到核函数。核函数是将信号映射到高维，而PCA一般用来降维。这里简单梳理一下核函数的知识： 核函数基本概念定义设$\chi $是输入空间(欧氏空间$\mathbb{R}^{n}$的子集或离散集合)，又设$\mathbb{R}^{H}$为特征空间(希尔伯特空间)，如果存在一个从$\chi $到$\mathbb{R}^{H}$的映射\phi (x):\chi \rightarrow \mathbb{H}使得对所有$x,z\in \chi $，函数$K(x,z)$满足条件K(x,z)=\phi (x) \cdot \phi (z)则称$K(x,z)$为核函数，$\phi (x)$为映射函数，式中$\phi (x) \cdot \phi (z)$为$\phi (x)$和$\phi (z)$的内积 核函数：是映射关系$\phi (x)$的内积，映射函数本身仅仅是一种映射关系，并没有增加维度的特性，不过可以利用核函数的特性，构造可以增加维度的核函数，这通常是我们希望的。 核函数的作用 聚类、分类 二维映射到三维，区分就更容易了，这是聚类、分类常用核函数的原因。为什么PCA这样一个降维算法也用核函数呢？ 降维 左图为原数据，右图为映射到三维的数据，可以看出：同样是降到1维，先通过Kernel映射到（Kernel是映射的内积，不要弄乱了）三维，再投影到1维，就容易分离开，这就是Kernel在PCA降维中的应用，本质还是对原有数据增加维度。 核函数为什么可以映射到高维为什么实现数据映射到高维 设原空间数据点$a_1=(x_1,x_2)；a_2=(x_1’,x_2’)$； 设高维空间的数据点为$A_1 =\phi (a_1)=(z_1,z_2,z_3)；A_2=\phi (a_1)=(z_1’,z_2’,z_3’)$ $\left \langle a_1,a_2 \right \rangle$为两点之间的内积$\left \langle a_1,a_2 \right \rangle = \left \langle (x_1,x_2),(x_1’,x_2’) \right \rangle =x_1x_1’+x_2x_2’$ $&lt;\phi (a_1),\phi (a_2)&gt;=&lt;\phi (x_1,x_2),\phi (x_1’,x_2’)&gt;=&lt;(z_1,z_2,z_3),(z_1’,z_2’,z_3’)&gt;=&lt;(x_1^2,\sqrt{2}x_1x_2,x_2^2),({x}_1’^2,\sqrt{2}x_1’x_2’,{x}_2’^2)&gt;=x_1^2x_1’^2+2x_1x_2x_1’x_2’+x_2^2x_2’^2=(x_1x_1’+x_2x_2’)^2=(&lt;a_1,a_2’&gt;)^2=k(a_1,a_2)$ 为什么不用映射函数$\phi (x)$，而用他们的内积形式$K(x,z)$，即Kernel函数？因为$(x,z)$一起出现的时候，$K(x,z)=\phi (x) \cdot \phi (z)$有许多固定的形式可以调用，而不必求解或者关心$\phi (x)$的具体形式，这大大简化了求解。 核函数的用法 两点之间的距离 \begin{align*} \left \| \phi (x)- \phi (x')\right \|^2 &=(\phi (x)- \phi (x'))^T(\phi (x)- \phi (x'))\\ &=\phi (x)^T\phi (x)-2\phi (x)^T\phi (x')+\phi (x')^T\phi (x')\\ &=-2]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++重载运算符和重载函数]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E9%87%8D%E8%BD%BD%E8%BF%90%E7%AE%97%E7%AC%A6%E5%92%8C%E9%87%8D%E8%BD%BD%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[C++ 允许在同一作用域中的某个函数和运算符指定多个定义，分别称为函数重载和运算符重载。 重载声明是指一个与之前已经在该作用域内声明过的函数或方法具有相同名称的声明，但是它们的参数列表和定义（实现）不相同。 当您调用一个重载函数或重载运算符时，编译器通过把您所使用的参数类型与定义中的参数类型进行比较，决定选用最合适的定义。选择最合适的重载函数或重载运算符的过程，称为重载决策。 C++ 中的函数重载在同一个作用域内，可以声明几个功能类似的同名函数，但是这些同名函数的形式参数（指参数的个数、类型或者顺序）必须不同。您不能仅通过返回类型的不同来重载函数。123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;using namespace std;class printData&#123; public: void print(int i) &#123; cout &lt;&lt; "整数为: " &lt;&lt; i &lt;&lt; endl; &#125; void print(double f) &#123; cout &lt;&lt; "浮点数为: " &lt;&lt; f &lt;&lt; endl; &#125; void print(char c[]) &#123; cout &lt;&lt; "字符串为: " &lt;&lt; c &lt;&lt; endl; &#125;&#125;;int main(void)&#123; printData pd; // 输出整数 pd.print(5); // 输出浮点数 pd.print(500.263); // 输出字符串 char c[] = "Hello C++"; pd.print(c); return 0;&#125; C++ 中的运算符重载重载的运算符是带有特殊名称的函数，函数名是由关键字 operator 和其后要重载的运算符符号构成的。与其他函数一样，重载运算符有一个返回类型和一个参数列表。 一元运算符12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;using namespace std;class Distance&#123;private: int feet; // 0 到无穷 int inches; // 0 到 12public: // 所需的构造函数 Distance()&#123; feet = 0; inches = 0; &#125; Distance(int f, int i)&#123; feet = f; inches = i; &#125; // 显示距离的方法 void displayDistance() &#123; cout &lt;&lt; "F: " &lt;&lt; feet &lt;&lt; " I:" &lt;&lt; inches &lt;&lt; endl; &#125; // 重载负运算符（ - ） Distance operator- () &#123; feet = -feet; inches = -inches; return Distance(feet, inches); &#125;&#125;;int main()&#123; Distance D1(11, 10), D2(-5, 11); D1.displayDistance(); D2.displayDistance(); -D1; // 取相反数 D1.displayDistance(); // 距离 D1 -D2; // 取相反数 D2.displayDistance(); // 距离 D2 return 0;&#125; 输出1234F: 11 I:10F: -5 I:11F: -11 I:-10F: 5 I:-11 二元运算符1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include &lt;iostream&gt;using namespace std;class Box&#123; public: double getVolume(void) &#123; return length * breadth * height; &#125; void setLength( double len ) &#123; length = len; &#125; void setBreadth( double bre ) &#123; breadth = bre; &#125; void setHeight( double hei ) &#123; height = hei; &#125; // 重载 + 运算符，用于把两个 Box 对象相加 Box operator+(const Box&amp; b) &#123; Box box; box.length = this-&gt;length + b.length; box.breadth = this-&gt;breadth + b.breadth; box.height = this-&gt;height + b.height; return box; &#125; private: double length; // 长度 double breadth; // 宽度 double height; // 高度&#125;;// 程序的主函数int main( )&#123; Box Box1; // 声明 Box1，类型为 Box Box Box2; // 声明 Box2，类型为 Box Box Box3; // 声明 Box3，类型为 Box double volume = 0.0; // 把体积存储在该变量中 // Box1 详述 Box1.setLength(6.0); Box1.setBreadth(7.0); Box1.setHeight(5.0); // Box2 详述 Box2.setLength(12.0); Box2.setBreadth(13.0); Box2.setHeight(10.0); // Box1 的体积 volume = Box1.getVolume(); cout &lt;&lt; "Volume of Box1 : " &lt;&lt; volume &lt;&lt;endl; // Box2 的体积 volume = Box2.getVolume(); cout &lt;&lt; "Volume of Box2 : " &lt;&lt; volume &lt;&lt;endl; // 把两个对象相加，得到 Box3 Box3 = Box1 + Box2; // Box3 的体积 volume = Box3.getVolume(); cout &lt;&lt; "Volume of Box3 : " &lt;&lt; volume &lt;&lt;endl; return 0;&#125; 关系运算符重载12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;iostream&gt;using namespace std;class Distance&#123; private: int feet; // 0 到无穷 int inches; // 0 到 12 public: // 所需的构造函数 Distance()&#123; feet = 0; inches = 0; &#125; Distance(int f, int i)&#123; feet = f; inches = i; &#125; // 显示距离的方法 void displayDistance() &#123; cout &lt;&lt; "F: " &lt;&lt; feet &lt;&lt; " I:" &lt;&lt; inches &lt;&lt;endl; &#125; // 重载负运算符（ - ） Distance operator- () &#123; feet = -feet; inches = -inches; return Distance(feet, inches); &#125; // 重载小于运算符（ &lt; ） bool operator &lt;(const Distance&amp; d) &#123; if(feet &lt; d.feet) &#123; return true; &#125; if(feet == d.feet &amp;&amp; inches &lt; d.inches) &#123; return true; &#125; return false; &#125;&#125;;int main()&#123; Distance D1(11, 10), D2(5, 11); if( D1 &lt; D2 ) &#123; cout &lt;&lt; "D1 is less than D2 " &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "D2 is less than D1 " &lt;&lt; endl; &#125; return 0;&#125; C++ 输入/输出运算符重载12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;using namespace std;class Distance&#123;private: int feet; // 0 到无穷 int inches; // 0 到 12public: // 所需的构造函数 Distance()&#123; feet = 0; inches = 0; &#125; Distance(int f, int i)&#123; feet = f; inches = i; &#125; friend ostream &amp;operator&lt;&lt;(ostream &amp;output, const Distance &amp;D) &#123; output &lt;&lt; "F : " &lt;&lt; D.feet &lt;&lt; " I : " &lt;&lt; D.inches; return output; &#125; friend istream &amp;operator&gt;&gt;(istream &amp;input, Distance &amp;D) &#123; input &gt;&gt; D.feet &gt;&gt; D.inches; return input; &#125;&#125;;int main()&#123; Distance D1(11, 10), D2(5, 11), D3; cout &lt;&lt; "Enter the value of object : " &lt;&lt; endl; cin &gt;&gt; D3; cout &lt;&lt; "First Distance : " &lt;&lt; D1 &lt;&lt; endl; cout &lt;&lt; "Second Distance :" &lt;&lt; D2 &lt;&lt; endl; cout &lt;&lt; "Third Distance :" &lt;&lt; D3 &lt;&lt; endl; return 0;&#125; 可重载运算符 类别 运算符 双目算术运算符 + (加)，-(减)，*(乘)，/(除)，% (取模) 关系运算符 ==(等于)，!= (不等于)，&lt; (小于)，&gt; (大于&gt;，&lt;=(小于等于)，&gt;=(大于等于) 逻辑运算符 &#124;&#124;(逻辑或)，&amp;&amp;(逻辑与)，&#124;(逻辑非) 单目运算符 + (正)，-(负)，*(指针)，&amp;(取地址) 自增自减运算符 ++(自增)，—(自减) 位运算符 &#124; (按位或)，&amp; (按位与)，~(按位取反)，^(按位异或),，&lt;&lt; (左移)，&gt;&gt;(右移) 赋值运算符 =, +=, -=, *=, /= , % = , &amp;=, &#124;=, ^=, &lt;&lt;=, &gt;&gt;= 空间申请与释放 new, delete, new[ ] , delete[] 其他运算符 ()(函数调用)，-&gt;(成员访问)，,(逗号)， 参考]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++继承]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E7%BB%A7%E6%89%BF%2F</url>
    <content type="text"><![CDATA[面向对象程序设计中最重要的一个概念是继承。继承允许我们依据另一个类来定义一个类，这使得创建和维护一个应用程序变得更容易。这样做，也达到了重用代码功能和提高执行时间的效果。 当创建一个类时，您不需要重新编写新的数据成员和成员函数，只需指定新建的类继承了一个已有的类的成员即可。这个已有的类称为基类，新建的类称为派生类。 父类&amp;子类1class derived-class: access-specifier base-class 其中，访问修饰符 access-specifier 是 public、protected 或 private 其中的一个，base-class 是之前定义过的某个类的名称。如果未使用访问修饰符 access-specifier，则默认为 private。 继承模式 描述 public 继承 基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：public, protected, private protected 继承 基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：protected, protected, private private 继承 基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：private, private, private 1234567891011121314151617181920212223242526272829303132333435363738394041using namespace std;// 基类class Shape&#123; public: void setWidth(int w) &#123; width = w; &#125; void setHeight(int h) &#123; height = h; &#125; protected: int width; int height;&#125;;// 派生类class Rectangle: public Shape&#123; public: int getArea() &#123; return (width * height); &#125;&#125;;int main(void)&#123; Rectangle Rect; Rect.setWidth(5); Rect.setHeight(7); // 输出对象的面积 cout &lt;&lt; "Total area: " &lt;&lt; Rect.getArea() &lt;&lt; endl; return 0;&#125; 输出1Total area: 35 访问控制和继承派生类可以访问基类中所有的非私有成员。因此基类成员如果不想被派生类的成员函数访问，则应在基类中声明为 private。我们可以根据访问权限总结出不同的访问类型，如下所示： 访问 public protected private 同一个类 yes yes yes 派生类 yes yes no 外部的类 yes no no 一个派生类继承了所有的基类方法，但下列情况除外： 基类的构造函数、析构函数和拷贝构造函数。 基类的重载运算符。 基类的友元函数。 多继承多继承即一个子类可以有多个父类，它继承了多个父类的特性。C++ 类可以从多个类继承成员，语法如下： 1234class &lt;派生类名&gt;:&lt;继承方式1&gt;&lt;基类名1&gt;,&lt;继承方式2&gt;&lt;基类名2&gt;,…&#123;&lt;派生类类体&gt;&#125;; 其中，访问修饰符继承方式是 public、protected 或 private 其中的一个，用来修饰每个基类，各个基类之间用逗号分隔，如上所示。现在让我们一起看看下面的实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;using namespace std;// 基类 Shapeclass Shape&#123; public: void setWidth(int w) &#123; width = w; &#125; void setHeight(int h) &#123; height = h; &#125; protected: int width; int height;&#125;;// 基类 PaintCostclass PaintCost&#123; public: int getCost(int area) &#123; return area * 70; &#125;&#125;;// 派生类class Rectangle: public Shape, public PaintCost&#123; public: int getArea() &#123; return (width * height); &#125;&#125;;int main(void)&#123; Rectangle Rect; int area; Rect.setWidth(5); Rect.setHeight(7); area = Rect.getArea(); // 输出对象的面积 cout &lt;&lt; "Total area: " &lt;&lt; Rect.getArea() &lt;&lt; endl; // 输出总花费 cout &lt;&lt; "Total paint cost: $" &lt;&lt; Rect.getCost(area) &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>继承</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++类和对象]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[C++ 类定义类由内部成员(属性与方法)组成，属性指变量，方法指行为。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include&lt;iostream&gt;using namespace std;#include &lt;iostream&gt;using namespace std;class Box&#123;//属性public: double length; // 长度 double breadth; // 宽度 double height; // 高度 double volume() &#123; return height * length * breadth; &#125;&#125;;//方法int main()&#123; Box Box1; // 声明 Box1，类型为 Box Box Box2; // 声明 Box2，类型为 Box double volume = 0.0; // 用于存储体积 // box 1 详述 Box1.height = 5.0; Box1.length = 6.0; Box1.breadth = 7.0; // box 2 详述 Box2.height = 10.0; Box2.length = 12.0; Box2.breadth = 13.0; // box 1 的体积 volume = Box1.volume(); cout &lt;&lt; "Box1 的体积：" &lt;&lt; volume &lt;&lt; endl; // box 2 的体积 volume = Box2.volume(); cout &lt;&lt; "Box2 的体积：" &lt;&lt; volume &lt;&lt; endl; return 0;&#125; C++ 类访问修饰符数据封装是面向对象编程的一个重要特点，它防止函数直接访问类类型的内部成员（属性和方法）。 修饰符 描述 public 公有成员在程序中类的外部是可访问的。您可以不使用任何成员函数来设置和获取公有变量的值。 private 私有成员变量或函数在类的外部是不可访问的，甚至是不可查看的。只有类和友元函数可以访问私有成员。默认情况下，类的所有成员都是私有的。 protected 保护成员变量或函数与私有成员十分相似，但有一点不同，保护成员在派生类（即子类）中是可访问的。 123456789101112131415class Base &#123; public: // 公有成员 protected: // 受保护成员 private: // 私有成员&#125;; 公有（public）成员12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;using namespace std;class Line&#123; public: double length; void setLength( double len ); double getLength( void );&#125;;// 成员函数定义double Line::getLength(void)&#123; return length ;&#125;void Line::setLength( double len )&#123; length = len;&#125;// 程序的主函数int main( )&#123; Line line; // 设置长度 line.setLength(6.0); cout &lt;&lt; "Length of line : " &lt;&lt; line.getLength() &lt;&lt;endl; // 不使用成员函数设置长度 line.length = 10.0; // OK: 因为 length 是公有的 cout &lt;&lt; "Length of line : " &lt;&lt; line.length &lt;&lt;endl; return 0;&#125; 私有（private）成员123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;using namespace std;class Box&#123; public: double length; void setWidth( double wid ); double getWidth( void ); private: double width;&#125;;// 成员函数定义double Box::getWidth(void)&#123; return width ;&#125;void Box::setWidth( double wid )&#123; width = wid;&#125;// 程序的主函数int main()&#123; Box box; // 不使用成员函数设置长度 box.length = 10.0; // OK: 因为 length 是公有的 cout &lt;&lt; "Length of box : " &lt;&lt; box.length &lt;&lt;endl; // 不使用成员函数设置宽度 // box.width = 10.0; // Error: 因为 width 是私有的 box.setWidth(10.0); // 使用成员函数设置宽度 cout &lt;&lt; "Width of box : " &lt;&lt; box.getWidth() &lt;&lt;endl; return 0;&#125; 保护（protected）成员保护成员变量或函数与私有成员十分相似，但有一点不同，保护成员在派生类（即子类）中是可访问的。 在下一个章节中，您将学习到派生类和继承的知识。现在您可以看到下面的实例中，我们从父类 Box 派生了一个子类 smallBox。下面的实例与前面的实例类似，在这里 width 成员可被派生类 smallBox 的任何成员函数访问。 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;using namespace std;class Box&#123; protected: double width;&#125;;class SmallBox:Box // SmallBox 是派生类&#123; public: void setSmallWidth( double wid ); double getSmallWidth( void );&#125;;// 子类的成员函数double SmallBox::getSmallWidth(void)&#123; return width ;&#125;void SmallBox::setSmallWidth( double wid )&#123; width = wid;&#125;// 程序的主函数int main( )&#123; SmallBox box; // 使用成员函数设置宽度 box.setSmallWidth(5.0); cout &lt;&lt; "Width of box : "&lt;&lt; box.getSmallWidth() &lt;&lt; endl; return 0;&#125; 继承的修饰符有public, protected, private三种继承方式，它们相应地改变了基类成员的访问属性。但无论哪种继承方式，上面两点都没有改变 private 成员只能被本类成员（类内）和友元访问，不能被派生类访问； protected 成员可以被派生类访问。 继承模式 描述 public 继承 基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：public, protected, private protected 继承 基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：protected, protected, private private 继承 基类 public 成员，protected 成员，private 成员的访问属性在派生类中分别变成：private, private, private public 继承12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;iostream&gt;#include&lt;assert.h&gt;using namespace std;class A&#123;public: int a; A()&#123; a1 = 1; a2 = 2; a3 = 3; a = 4; &#125; void fun()&#123; cout &lt;&lt; a &lt;&lt; endl; //正确 cout &lt;&lt; a1 &lt;&lt; endl; //正确 cout &lt;&lt; a2 &lt;&lt; endl; //正确 cout &lt;&lt; a3 &lt;&lt; endl; //正确 &#125;public: int a1;protected: int a2;private: int a3;&#125;;class B : public A&#123;public: int a; B(int i)&#123; A(); a = i; &#125; void fun()&#123; cout &lt;&lt; a &lt;&lt; endl; //正确，public成员 cout &lt;&lt; a1 &lt;&lt; endl; //正确，基类的public成员，在派生类中仍是public成员。 cout &lt;&lt; a2 &lt;&lt; endl; //正确，基类的protected成员，在派生类中仍是protected可以被派生类访问。 cout &lt;&lt; a3 &lt;&lt; endl; //错误，基类的private成员不能被派生类访问。 &#125;&#125;;int main()&#123; B b(10); cout &lt;&lt; b.a &lt;&lt; endl; cout &lt;&lt; b.a1 &lt;&lt; endl; //正确 cout &lt;&lt; b.a2 &lt;&lt; endl; //错误，类外不能访问protected成员 cout &lt;&lt; b.a3 &lt;&lt; endl; //错误，类外不能访问private成员 system("pause"); return 0;&#125; protected 继承123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include&lt;iostream&gt;#include&lt;assert.h&gt;using namespace std;class A&#123;public: int a; A()&#123; a1 = 1; a2 = 2; a3 = 3; a = 4; &#125; void fun()&#123; cout &lt;&lt; a &lt;&lt; endl; //正确 cout &lt;&lt; a1 &lt;&lt; endl; //正确 cout &lt;&lt; a2 &lt;&lt; endl; //正确 cout &lt;&lt; a3 &lt;&lt; endl; //正确 &#125;public: int a1;protected: int a2;private: int a3;&#125;;class B : protected A&#123;public: int a; B(int i)&#123; A(); a = i; &#125; void fun()&#123; cout &lt;&lt; a &lt;&lt; endl; //正确，public成员。 cout &lt;&lt; a1 &lt;&lt; endl; //正确，基类的public成员，在派生类中变成了protected，可以被派生类访问。 cout &lt;&lt; a2 &lt;&lt; endl; //正确，基类的protected成员，在派生类中还是protected，可以被派生类访问。 cout &lt;&lt; a3 &lt;&lt; endl; //错误，基类的private成员不能被派生类访问。 &#125;&#125;;int main()&#123; B b(10); cout &lt;&lt; b.a &lt;&lt; endl; //正确。public成员 cout &lt;&lt; b.a1 &lt;&lt; endl; //错误，protected成员不能在类外访问。 cout &lt;&lt; b.a2 &lt;&lt; endl; //错误，protected成员不能在类外访问。 cout &lt;&lt; b.a3 &lt;&lt; endl; //错误，private成员不能在类外访问。 system("pause"); return 0;&#125; private 继承123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include&lt;iostream&gt;#include&lt;assert.h&gt;using namespace std;class A&#123;public: int a; A()&#123; a1 = 1; a2 = 2; a3 = 3; a = 4; &#125; void fun()&#123; cout &lt;&lt; a &lt;&lt; endl; //正确 cout &lt;&lt; a1 &lt;&lt; endl; //正确 cout &lt;&lt; a2 &lt;&lt; endl; //正确 cout &lt;&lt; a3 &lt;&lt; endl; //正确 &#125;public: int a1;protected: int a2;private: int a3;&#125;;class B : private A&#123;public: int a; B(int i)&#123; A(); a = i; &#125; void fun()&#123; cout &lt;&lt; a &lt;&lt; endl; //正确，public成员。 cout &lt;&lt; a1 &lt;&lt; endl; //正确，基类public成员,在派生类中变成了private,可以被派生类访问。 cout &lt;&lt; a2 &lt;&lt; endl; //正确，基类的protected成员，在派生类中变成了private,可以被派生类访问。 cout &lt;&lt; a3 &lt;&lt; endl; //错误，基类的private成员不能被派生类访问。 &#125;&#125;;int main()&#123; B b(10); cout &lt;&lt; b.a &lt;&lt; endl; //正确。public成员 cout &lt;&lt; b.a1 &lt;&lt; endl; //错误，private成员不能在类外访问。 cout &lt;&lt; b.a2 &lt;&lt; endl; //错误, private成员不能在类外访问。 cout &lt;&lt; b.a3 &lt;&lt; endl; //错误，private成员不能在类外访问。 system("pause"); return 0;&#125; 构造函数类的构造函数是类的一种特殊的成员函数，它会在每次创建类的新对象时执行。 构造函数的名称与类的名称是完全相同的，并且不会返回任何类型，也不会返回 void。构造函数可用于为某些成员变量设置初始值。 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;using namespace std;class Line&#123; public: void setLength( double len ); double getLength( void ); Line(); // 这是构造函数 private: double length;&#125;;// 成员函数定义，包括构造函数Line::Line(void)&#123; cout &lt;&lt; "Object is being created" &lt;&lt; endl;&#125;void Line::setLength( double len )&#123; length = len;&#125;double Line::getLength( void )&#123; return length;&#125;// 程序的主函数int main( )&#123; Line line; // 设置长度 line.setLength(6.0); cout &lt;&lt; "Length of line : " &lt;&lt; line.getLength() &lt;&lt;endl; return 0;&#125; 带参数的构造函数默认的构造函数没有任何参数，但如果需要，构造函数也可以带有参数。这样在创建对象时就会给对象赋初始值，如下面的例子所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;iostream&gt;using namespace std;class Line&#123; public: void setLength( double len ); double getLength( void ); Line(double len); // 这是构造函数 private: double length;&#125;;// 成员函数定义，包括构造函数Line::Line( double len)&#123; cout &lt;&lt; "Object is being created, length = " &lt;&lt; len &lt;&lt; endl; length = len;&#125;void Line::setLength( double len )&#123; length = len;&#125;double Line::getLength( void )&#123; return length;&#125;// 程序的主函数int main( )&#123; Line line(10.0); // 获取默认设置的长度 cout &lt;&lt; "Length of line : " &lt;&lt; line.getLength() &lt;&lt;endl; // 再次设置长度 line.setLength(6.0); cout &lt;&lt; "Length of line : " &lt;&lt; line.getLength() &lt;&lt;endl; return 0;&#125; 类的析构函数类的析构函数是类的一种特殊的成员函数，它会在每次删除所创建的对象时执行。 析构函数的名称与类的名称是完全相同的，只是在前面加了个波浪号（~）作为前缀，它不会返回任何值，也不能带有任何参数。析构函数有助于在跳出程序（比如关闭文件、释放内存等）前释放资源。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;using namespace std;class Line&#123;public: void setLength(double len); double getLength(void); Line(); // 这是构造函数声明 ~Line(); // 这是析构函数声明private: double length;&#125;;// 成员函数定义，包括构造函数Line::Line(void)&#123; cout &lt;&lt; "Object is being created" &lt;&lt; endl;&#125;Line::~Line(void)&#123; cout &lt;&lt; "Object is being deleted" &lt;&lt; endl;&#125;void Line::setLength(double len)&#123; length = len;&#125;double Line::getLength(void)&#123; return length;&#125;// 程序的主函数int main()&#123; Line line; // 设置长度 line.setLength(6.0); cout &lt;&lt; "Length of line : " &lt;&lt; line.getLength() &lt;&lt; endl; return 0; //结束后，对象销毁，此时返回析构函数&#125; C++ 拷贝构造函数拷贝构造函数是一种特殊的构造函数，它在创建对象时，是使用同一类中之前创建的对象来初始化新创建的对象。拷贝构造函数通常用于： 通过使用另一个同类型的对象来初始化新创建的对象。 复制对象把它作为参数传递给函数。 复制对象，并从函数返回这个对象 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;using namespace std;class Line&#123; public: int getLength( void ); Line( int len ); // 简单的构造函数 Line( const Line &amp;obj); // 拷贝构造函数 ~Line(); // 析构函数 private: int *ptr;&#125;;// 成员函数定义，包括构造函数Line::Line(int len)&#123; cout &lt;&lt; "调用构造函数" &lt;&lt; endl; // 为指针分配内存 ptr = new int; *ptr = len;&#125;Line::Line(const Line &amp;obj)&#123; cout &lt;&lt; "调用拷贝构造函数并为指针 ptr 分配内存" &lt;&lt; endl; ptr = new int; *ptr = *obj.ptr; // 拷贝值&#125;Line::~Line(void)&#123; cout &lt;&lt; "释放内存" &lt;&lt; endl; delete ptr;&#125;int Line::getLength( void )&#123; return *ptr;&#125;void display(Line obj)&#123; cout &lt;&lt; "line 大小 : " &lt;&lt; obj.getLength() &lt;&lt;endl;&#125;// 程序的主函数int main( )&#123; Line line1(10); Line line2 = line1; // 这里也调用了拷贝构造函数 display(line1); display(line2); return 0;&#125; C++ 友元函数类的友元函数是定义在类外部，但有权访问类的所有私有（private）成员和保护（protected）成员。尽管友元函数的原型有在类的定义中出现过，但是友元函数并不是成员函数。 友元可以是一个函数，该函数被称为友元函数；友元也可以是一个类，该类被称为友元类，在这种情况下，整个类及其所有成员都是友元。如果要声明函数为一个类的友元，需要在类定义中该函数原型前使用关键字 friend，如下所示：1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;using namespace std;class Box&#123; double width;public: friend void printWidth( Box box ); void setWidth( double wid );&#125;;// 成员函数定义void Box::setWidth( double wid )&#123; width = wid;&#125;// 请注意：printWidth() 不是任何类的成员函数void printWidth( Box box )&#123; /* 因为 printWidth() 是 Box 的友元，它可以直接访问该类的任何成员 */ cout &lt;&lt; "Width of box : " &lt;&lt; box.width &lt;&lt;endl;&#125;// 程序的主函数int main( )&#123; Box box; // 使用成员函数设置宽度 box.setWidth(10.0); // 使用友元函数输出宽度 printWidth( box ); return 0;&#125; C++内联函数调用函数比求解等价表达式要慢得多。在大多数的机器上，调用函数都要做很多工作：调用前要先保存寄存器，并在返回时恢复，复制实参，程序还必须转向一个新位置执行C++中支持内联函数，其目的是为了提高函数的执行效率，用关键字 inline 放在函数定义(注意是定义而非声明，下文继续讲到)的前面即可将函数指定为内联函数。内联函数是通常与类一起使用。如果一个函数是内联的，那么在编译时，编译器会把该函数的代码副本放置在每个调用该函数的地方。对内联函数进行任何修改，都需要重新编译函数的所有客户端，因为编译器需要重新更换一次所有的代码，否则将会继续使用旧的函数。如果想把一个函数定义为内联函数，则需要在函数名前面放置关键字 inline，在调用函数之前需要对函数进行定义。 123456789101112131415161718#include &lt;iostream&gt;using namespace std;inline int Max(int x, int y)&#123; return (x &gt; y)? x : y;&#125;// 程序的主函数int main( )&#123; cout &lt;&lt; "Max (20,10): " &lt;&lt; Max(20,10) &lt;&lt; endl; cout &lt;&lt; "Max (0,200): " &lt;&lt; Max(0,200) &lt;&lt; endl; cout &lt;&lt; "Max (100,1010): " &lt;&lt; Max(100,1010) &lt;&lt; endl; return 0;&#125; C++ this 指针在 C++ 中，每一个对象都能通过 this 指针来访问自己的地址。this 指针是所有成员函数的隐含参数。因此，在成员函数内部，它可以用来指向调用对象。 友元函数没有 this 指针，因为友元不是类的成员。只有成员函数才有 this 指针。1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;iostream&gt;using namespace std;class Box&#123; public: // 构造函数定义 Box(double l=2.0, double b=2.0, double h=2.0) &#123; cout &lt;&lt;"Constructor called." &lt;&lt; endl; length = l; breadth = b; height = h; &#125; double Volume() &#123; return length * breadth * height; &#125; int compare(Box box) &#123; return this-&gt;Volume() &gt; box.Volume(); &#125; private: double length; // Length of a box double breadth; // Breadth of a box double height; // Height of a box&#125;;int main(void)&#123; Box Box1(3.3, 1.2, 1.5); // Declare box1 Box Box2(8.5, 6.0, 2.0); // Declare box2 if(Box1.compare(Box2)) &#123; cout &lt;&lt; "Box2 is smaller than Box1" &lt;&lt;endl; &#125; else &#123; cout &lt;&lt; "Box2 is equal to or larger than Box1" &lt;&lt;endl; &#125; return 0;&#125; C++ 指向类的指针一个指向 C++ 类的指针与指向结构的指针类似，访问指向类的指针的成员，需要使用成员访问运算符 -&gt;，就像访问指向结构的指针一样。123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;using namespace std;class Box&#123; public: // 构造函数定义 Box(double l=2.0, double b=2.0, double h=2.0) &#123; cout &lt;&lt;"Constructor called." &lt;&lt; endl; length = l; breadth = b; height = h; &#125; double Volume() &#123; return length * breadth * height; &#125; private: double length; // Length of a box double breadth; // Breadth of a box double height; // Height of a box&#125;;int main(void)&#123; Box Box1(3.3, 1.2, 1.5); // Declare box1 Box Box2(8.5, 6.0, 2.0); // Declare box2 Box *ptrBox; // Declare pointer to a class. // 保存第一个对象的地址 ptrBox = &amp;Box1; // 现在尝试使用成员访问运算符来访问成员 cout &lt;&lt; "Volume of Box1: " &lt;&lt; ptrBox-&gt;Volume() &lt;&lt; endl; // 保存第二个对象的地址 ptrBox = &amp;Box2; // 现在尝试使用成员访问运算符来访问成员 cout &lt;&lt; "Volume of Box2: " &lt;&lt; ptrBox-&gt;Volume() &lt;&lt; endl; return 0;&#125; C++类的静态成员我们可以使用 static 关键字来把类成员定义为静态的。当我们声明类的成员为静态时，这意味着无论创建多少个类的对象，静态成员都只有一个副本。 静态成员在类的所有对象中是共享的。如果不存在其他的初始化语句，在创建第一个对象时，所有的静态数据都会被初始化为零。我们不能把静态成员的初始化放置在类的定义中，但是可以在类的外部通过使用范围解析运算符 :: 来重新声明静态变量从而对它进行初始化，如下面的实例所示。 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;using namespace std;class Box&#123; public: static int objectCount; // 构造函数定义 Box(double l=2.0, double b=2.0, double h=2.0) &#123; cout &lt;&lt;"Constructor called." &lt;&lt; endl; length = l; breadth = b; height = h; // 每次创建对象时增加 1 objectCount++; &#125; double Volume() &#123; return length * breadth * height; &#125; private: double length; // 长度 double breadth; // 宽度 double height; // 高度&#125;;// 初始化类 Box 的静态成员int Box::objectCount = 0;int main(void)&#123; Box Box1(3.3, 1.2, 1.5); // 声明 box1 Box Box2(8.5, 6.0, 2.0); // 声明 box2 // 输出对象的总数 cout &lt;&lt; "Total objects: " &lt;&lt; Box::objectCount &lt;&lt; endl; return 0;&#125; 输出123Constructor called.Constructor called.Total objects: 2 静态成员函数 如果把函数成员声明为静态的，就可以把函数与类的任何特定对象独立开来。静态成员函数即使在类对象不存在的情况下也能被调用，静态函数只要使用类名加范围解析运算符 :: 就可以访问。 静态成员函数只能访问静态成员数据、其他静态成员函数和类外部的其他函数。 静态成员函数有一个类范围，他们不能访问类的 this 指针。您可以使用静态成员函数来判断类的某些对象是否已被创建。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;iostream&gt;using namespace std;class Box&#123; public: static int objectCount; // 构造函数定义 Box(double l=2.0, double b=2.0, double h=2.0) &#123; cout &lt;&lt;"Constructor called." &lt;&lt; endl; length = l; breadth = b; height = h; // 每次创建对象时增加 1 objectCount++; &#125; double Volume() &#123; return length * breadth * height; &#125; static int getCount() &#123; return objectCount; &#125; private: double length; // 长度 double breadth; // 宽度 double height; // 高度&#125;;// 初始化类 Box 的静态成员int Box::objectCount = 0;int main(void)&#123; // 在创建对象之前输出对象的总数 cout &lt;&lt; "Inital Stage Count: " &lt;&lt; Box::getCount() &lt;&lt; endl; Box Box1(3.3, 1.2, 1.5); // 声明 box1 Box Box2(8.5, 6.0, 2.0); // 声明 box2 // 在创建对象之后输出对象的总数 cout &lt;&lt; "Final Stage Count: " &lt;&lt; Box::getCount() &lt;&lt; endl; return 0;&#125; 输出1234Inital Stage Count: 0Constructor called.Constructor called.Final Stage Count: 2]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>类和对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++结构体]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E7%BB%93%E6%9E%84%E4%BD%93%2F</url>
    <content type="text"><![CDATA[结构体声明1234567891011121314struct stu&#123; char *name; //姓名 int num; //学号 int age; //年龄 char group; //所在学习小组 float score; //成绩&#125; stu1, stu2; //stu1与stu2为结构体变量//给结构体成员赋值 stu1.name = "Tom"; stu1.num = 12; stu1.age = 18; stu1.group = 'A'; stu1.score = 136.5; 结构体数组1234567891011121314151617181920212223242526272829#include &lt;stdio.h&gt;struct&#123; char *name; //姓名 int num; //学号 int age; //年龄 char group; //所在小组 float score; //成绩&#125;class1[] = &#123; &#123; "Li ping", 5, 18, 'C', 145.0 &#125;, &#123; "Zhang ping", 4, 19, 'A', 130.5 &#125;, &#123; "He fang", 1, 18, 'A', 148.5 &#125;, &#123; "Cheng ling", 2, 17, 'F', 139.0 &#125;, &#123; "Wang ming", 3, 17, 'B', 144.5 &#125;&#125;;int main()&#123; int i, num_140 = 0; float sum = 0; for (i = 0; i&lt;5; i++)&#123; sum += class1[i].score; if (class1[i].score &lt; 140) num_140++; &#125; printf("sum=%.2f\naverage=%.2f\nnum_140=%d\n", sum, sum / 5, num_140); for (i = 0; i&lt;5; i++)&#123; cout &lt;&lt; "name:" &lt;&lt; class1[i].name &lt;&lt; endl; &#125; return 0;&#125; 输出12345678sum=707.50average=141.50num_140=2name:Li pingname:Zhang pingname:He fangname:Cheng lingname:Wang ming 结构体枚举数据类型1enum week&#123; Mon, Tues, Wed, Thurs, Fri, Sat, Sun &#125;; //枚举值默认从 0 开始，往后逐个加 1（递增）；也就是说，week 中的 Mon、Tues ...... Sun 对应的值分别为 0、1 ...... 6。 结构体作为函数参数123456789101112131415struct Books&#123; char title[50]; char author[50]; char subject[100]; int book_id;&#125;;void printBook( struct Books book )&#123; printf( "Book title : %s\n", book.title); printf( "Book author : %s\n", book.author); printf( "Book subject : %s\n", book.subject); printf( "Book book_id : %d\n", book.book_id);&#125; 指向结构的指针为了使用指向该结构的指针访问结构的成员，您必须使用 -&gt; 运算符123struct Books *struct_pointer;struct_pointer = &amp;Book1;struct_pointer-&gt;title; //为了使用指向该结构的指针访问结构的成员，您必须使用 -&gt; 运算符]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>结构体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++引用]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[引用变量是一个别名，也就是说，它是某个已存在变量的另一个名字。一旦把引用初始化为某个变量，就可以使用该引用名称或变量名称来指向变量。 C++ 引用 vs 指针引用很容易与指针混淆，它们之间有三个主要的不同： 不存在空引用。引用必须连接到一块合法的内存。 一旦引用被初始化为一个对象，就不能被指向到另一个对象。指针可以在任何时候指向到另一个对象。 引用必须在创建时被初始化。指针可以在任何时间被初始化。 C++ 中创建引用试想变量名称是变量附属在内存位置中的标签，您可以把引用当成是变量附属在内存位置中的第二个标签。因此，您可以通过原始变量名称或引用来访问变量的内容。例如： 123456789101112131415161718192021222324#include &lt;iostream&gt;using namespace std;int main ()&#123; // 声明简单的变量 int i; double d; // 声明引用变量 int&amp; r = i; double&amp; s = d; i = 5; cout &lt;&lt; "Value of i : " &lt;&lt; i &lt;&lt; endl; cout &lt;&lt; "Value of i reference : " &lt;&lt; r &lt;&lt; endl; d = 11.7; cout &lt;&lt; "Value of d : " &lt;&lt; d &lt;&lt; endl; cout &lt;&lt; "Value of d reference : " &lt;&lt; s &lt;&lt; endl; return 0;&#125; 输出1234Value of i : 5Value of i reference : 5Value of d : 11.7Value of d reference : 11.7]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++指针]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E6%8C%87%E9%92%88%2F</url>
    <content type="text"><![CDATA[概括12345int *p //声明指针变量int i //声明变量int temp //声明变量tempp=&amp;i //&amp;为取地址符，获取变量i的地址temp = *p //获取地址p的值 指针的声明12345int *ip; /* 一个整型的指针 */double *dp; /* 一个 double 型的指针 */float *fp; /* 一个浮点型的指针 */char *ch; /* 一个字符型的指针 */int *ptr = NULL; /* 声明空的指针 */ 指针的算术运算指针是一个用数值表示的地址。因此，您可以对指针执行算术运算。可以对指针进行四种算术运算：++、—、+、-。假设 ptr 是一个指向地址 1000 的整型指针，是一个 32 位的整数，让我们对该指针执行下列的算术运算：1234567891011121314151617181920#include &lt;iostream&gt;using namespace std;int main()&#123; // int *ptr; int a; ptr = &amp;a; // 输出返回值 cout &lt;&lt; "&amp;a是：" &lt;&lt; &amp;a &lt;&lt; endl; cout &lt;&lt; "ptr：" &lt;&lt; ptr &lt;&lt; endl; cout &lt;&lt; "ptr++：" &lt;&lt; ptr++ &lt;&lt; endl; cout &lt;&lt; "ptr：" &lt;&lt; ptr &lt;&lt; endl; cout &lt;&lt; "++ptr：" &lt;&lt; ++ptr &lt;&lt; endl; return 0;&#125; 在执行完上述的运算之后，ptr 将指向位置 1004，因为 ptr 每增加一次，它都将指向下一个整数位置，即当前位置往后移 4 个字节。这个运算会在不影响内存位置中实际值的情况下，移动指针到下一个内存位置。如果 ptr 指向一个地址为 1000 的字符，上面的运算会导致指针指向位置 1001，因为下一个字符位置是在 1001。12345&amp;a是：007FFDB0ptr：007FFDB0ptr++：007FFDB0ptr：007FFDB4++ptr：007FFDB8 指针数组一个数组，数组的数据类型为指针 示例一12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;const int MAX = 3;int main()&#123; int var[] = &#123; 10, 100, 200 &#125;; //数组 int i, *ptr[MAX]; //指针数组 for (i = 0; i &lt; MAX; i++) &#123; ptr[i] = &amp;var[i]; /* 将var元素的地址赋值给对应的ptr */ &#125; for (i = 0; i &lt; MAX; i++) &#123; printf("Value of *ptr[%d] = %d\n", i, *ptr[i]); //打印*ptr的值,ptr存储的地址所指向的值 &#125; for (i = 0; i &lt; MAX; i++) &#123; printf("Value of var[%d] = %d\n", i, var[i]); //打印var数组的值 &#125; for (i = 0; i &lt; MAX; i++) &#123; printf("Value of ptr[%d] = %d\n", i, ptr[i]); //打印ptr的值,ptr存储的地址 &#125; return 0;&#125; 输出123456789Value of *ptr[0] = 10Value of *ptr[1] = 100Value of *ptr[2] = 200Value of var[0] = 10Value of var[1] = 100Value of var[2] = 200Value of ptr[0] = 19921128Value of ptr[1] = 19921132Value of ptr[2] = 19921136 示例二1234567891011121314151617181920#include &lt;stdio.h&gt;const int MAX = 4;int main ()&#123; char *names[] = &#123; "Zara Ali", "Hina Ali", "Nuha Ali", "Sara Ali", &#125;; int i = 0; for ( i = 0; i &lt; MAX; i++) &#123; printf("Value of names[%d] = %s\n", i, names[i] ); &#125; return 0;&#125; 输出1234Value of names[0] = Zara AliValue of names[1] = Hina AliValue of names[2] = Nuha AliValue of names[3] = Sara Ali 指针的指针——二级指针二级指针作为函数参数的作用:在函数外部定义一个指针p，在函数内给指针赋值，函数结束后对指针p生效，那么我们就需要二级指针。 1234567891011121314151617181920212223#include&lt;iostream&gt;using namespace std;int a = 10;int b = 100;int *q;void func(int *p)&#123; cout &lt;&lt; "func:&amp;p(p的地址)=" &lt;&lt; &amp;p &lt;&lt; ",p(p指向的地址)=" &lt;&lt; p &lt;&lt; endl; //note:3 p = &amp;b; cout &lt;&lt; "func:&amp;p(p的地址)=" &lt;&lt; &amp;p &lt;&lt; ",p(p指向的地址)=" &lt;&lt; p &lt;&lt; endl; //note:4&#125;int main()&#123; cout &lt;&lt; "&amp;a(a的地址)=" &lt;&lt; &amp;a &lt;&lt; ",&amp;b(b的地址)=" &lt;&lt; &amp;b &lt;&lt; ",&amp;q(q的地址)=" &lt;&lt; &amp;q &lt;&lt; endl; //note:1 q = &amp;a; //q指向的地址为&amp;a ,q=&amp;a;q 指向地址的取值 即a; &amp;q为指针q的取值 cout &lt;&lt; "*q(指向地址的取值)=" &lt;&lt; *q &lt;&lt; ",q(指向的地址)=" &lt;&lt; q &lt;&lt; ",&amp;q(q的地址)=" &lt;&lt; &amp;q &lt;&lt; endl; //note:2 func(q); //p与q指向同一个地址，但是p与q为不同的指针，所以p=q但是&amp;p不封于&amp;q cout &lt;&lt; "*q=" &lt;&lt; *q &lt;&lt; ",q=" &lt;&lt; q &lt;&lt; ",&amp;q=" &lt;&lt; &amp;q &lt;&lt; endl; //note:5 system("pause"); return 0;&#125; 输出12345&amp;a(a的地址)=00C3F000,&amp;b(b的地址)=00C3F004,&amp;q(q的地址)=00C3F364*q(指向地址的取值)=10,q(指向的地址)=00C3F000,&amp;q(q的地址)=00C3F364func:&amp;p(p的地址)=008FFC80,p(p指向的地址)=00C3F000func:&amp;p(p的地址)=008FFC80,p(p指向的地址)=00C3F004*q=10,q=00C3F000,&amp;q=00C3F364 传递指针给函数12345678910111213141516171819202122232425262728293031323334#include &lt;stdio.h&gt;/* 函数声明 */double getAverage(int *arr, int size);int main ()&#123; /* 带有 5 个元素的整型数组 */ int balance[5] = &#123;1000, 2, 3, 17, 50&#125;; double avg; /* 传递一个指向数组的指针作为参数 */ avg = getAverage( balance, 5 ) ; /* 输出返回值 */ printf("Average value is: %f\n", avg ); return 0;&#125;double getAverage(int *arr, int size)&#123; int i, sum = 0; double avg; for (i = 0; i &lt; size; ++i) &#123; sum += arr[i]; &#125; avg = (double)sum / size; return avg;&#125; 从函数返回指针函数返回数组123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;#include &lt;time.h&gt;#include &lt;stdlib.h&gt;/* 要生成和返回随机数的函数 */int * getRandom( )&#123; static int r[10]; int i; /* 设置种子 */ srand( (unsigned)time( NULL ) ); for ( i = 0; i &lt; 10; ++i) &#123; r[i] = rand(); printf("%d\n", r[i] ); &#125; return r;&#125;/* 要调用上面定义函数的主函数 */int main ()&#123; /* 一个指向整数的指针 */ int *p; int i; p = getRandom(); for ( i = 0; i &lt; 10; i++ ) &#123; printf("*(p + [%d]) : %d\n", i, *(p + i) ); &#125; return 0;&#125; 输出123456789101112131415161718192015231980531187214107110830097843049495914213012769309710841232504841069321401604461820149169022*(p + [0]) : 1523198053*(p + [1]) : 1187214107*(p + [2]) : 1108300978*(p + [3]) : 430494959*(p + [4]) : 1421301276*(p + [5]) : 930971084*(p + [6]) : 123250484*(p + [7]) : 106932140*(p + [8]) : 1604461820*(p + [9]) : 149169022 函数指针通常我们说的指针变量是指向一个整型、字符型或数组等变量，而函数指针是指向函数。12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;iostream&gt;using namespace std;/* 要生成和返回随机数的函数 */int *getRandom()&#123; static int r[10]; int i; /* 设置种子 */ srand((unsigned)time(NULL)); for (i = 0; i &lt; 10; ++i) &#123; r[i] = rand(); printf("%d\n", r[i]); &#125; return r;&#125;#include &lt;stdio.h&gt;int max(int x, int y)&#123; return x &gt; y ? x : y; //如果x&gt;y，返回x否则返回y&#125;int main(void)&#123; /* p 是函数指针 */ int(*p)(int, int) = &amp;max; // &amp;可以省略 int a, b, c, d; cout &lt;&lt; "请输入三个数字:" &lt;&lt;endl; cin &gt;&gt; a &gt;&gt; b &gt;&gt; c; /* 与直接调用函数等价，d = max(max(a, b), c) */ d = p(p(a, b), c); printf("最大的数字是: %d\n", d); return 0;&#125; 回调函数函数指针作为某个函数的参数，函数指针变量可以作为某个函数的参数来使用的，回调函数就是一个通过函数指针调用的函数。简单讲：回调函数是由别人的函数执行时调用你实现的函数。1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;// 回调函数void populate_array(int *array, size_t arraySize, int(*getNextValue)(void))&#123; for (size_t i = 0; i&lt;arraySize; i++) array[i] = getNextValue();&#125;// 获取随机值int getNextRandomValue(void)&#123; return rand();&#125;int main(void)&#123; int myarray[10]; populate_array(myarray, 10, getNextRandomValue); for (int i = 0; i &lt; 10; i++) &#123; cout &lt;&lt; myarray[i] &lt;&lt; endl; &#125; return 0;&#125;]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++数组]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[数组123type arrayName [ arraySize ];double balance[5] = &#123;1000.0, 2.0, 3.4, 7.0, 50.0&#125;; 多维数组12345678type name[size1][size2]...[sizeN];//二维数组int a[3][4] = &#123; &#123;0, 1, 2, 3&#125; , /* 初始化索引号为 0 的行 */ &#123;4, 5, 6, 7&#125; , /* 初始化索引号为 1 的行 */ &#123;8, 9, 10, 11&#125; /* 初始化索引号为 2 的行 */&#125;; 指向数组的指针数组名是一个指向数组中第一个元素的常量指针。balance 是一个指向 &amp;balance[0] 的指针，即数组 balance 的第一个元素的地址。因此，下面的程序片段把 p 赋值为 balance 的第一个元素的地址：1234double *p;double balance[10];p = balance; 使用数组名作为常量指针是合法的，反之亦然。因此，*(balance + 4) 是一种访问 balance[4] 数据的合法方式。 传递数组给函数方式 1形式参数是一个指针：123456void myFunction(int *param)&#123;...&#125; 方式 2形式参数是一个已定义大小的数组：123456void myFunction(int param[10])&#123;...&#125; 方式 3形式参数是一个未定义大小的数组：123456void myFunction(int param[])&#123;...&#125; 从函数返回数组C++ 不允许返回一个完整的数组作为函数的参数。但是，您可以通过指定不带索引的数组名来返回一个指向数组的指针。 如果您想要从函数返回一个一维数组，您必须声明一个返回指针的函数，如下123456int * myFunction()&#123;...&#125; 实例12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;cstdlib&gt;#include &lt;ctime&gt;using namespace std;// 要生成和返回随机数的函数int * getRandom( )&#123; static int r[10]; // 设置种子 srand( (unsigned)time( NULL ) ); for (int i = 0; i &lt; 10; ++i) &#123; r[i] = rand(); cout &lt;&lt; r[i] &lt;&lt; endl; &#125; return r;&#125;// 要调用上面定义函数的主函数int main ()&#123; // 一个指向整数的指针 int *p; p = getRandom(); for ( int i = 0; i &lt; 10; i++ ) &#123; cout &lt;&lt; "*(p + " &lt;&lt; i &lt;&lt; ") : "; cout &lt;&lt; *(p + i) &lt;&lt; endl; &#125; return 0;&#125;]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>变量作用域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++数字]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[数学运算为了利用这些函数，您需要引用数学头文件 。 序号 函数 &amp; 描述 1 double cos(double);该函数返回弧度角（double 型）的余弦。 2 double sin(double);该函数返回弧度角（double 型）的正弦。 3 double tan(double);该函数返回弧度角（double 型）的正切。 4 double log(double);该函数返回参数的自然对数。 5 double pow(double, double);假设第一个参数为 x，第二个参数为 y，则该函数返回 x 的 y 次方。 6 double hypot(double, double);该函数返回两个参数的平方总和的平方根，也就是说，参数为一个直角三角形的两个直角边，函数会返回斜边的长度。 7 double sqrt(double);该函数返回参数的平方根。 8 int abs(int);该函数返回整数的绝对值。 9 double fabs(double);该函数返回任意一个十进制数的绝对值。 10 double floor(double);该函数返回一个小于或等于传入参数的最大整数。 C++ 随机数在许多情况下，需要生成随机数。关于随机数生成器，有两个相关的函数。一个是 rand()，该函数只返回一个伪随机数。生成随机数之前必须先调用 srand() 函数 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;ctime&gt;#include &lt;cstdlib&gt;using namespace std;int main ()&#123; int i,j; // 设置种子 srand( (unsigned)time( NULL ) ); /* 生成 10 个随机数 */ for( i = 0; i &lt; 10; i++ ) &#123; // 生成实际的随机数 j= rand(); cout &lt;&lt;"随机数： " &lt;&lt; j &lt;&lt; endl; &#125; return 0;&#125;]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>数字</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++函数]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1234return_type function_name( parameter list )&#123; body of the function&#125; 返回类型：一个函数可以返回一个值。return_type 是函数返回的值的数据类型。有些函数执行所需的操作而不返回值，在这种情况下，return_type 是关键字 void。 函数名称：这是函数的实际名称。函数名和参数列表一起构成了函数签名。 参数：参数就像是占位符。当函数被调用时，您向参数传递一个值，这个值被称为实际参数。参数列表包括函数参数的类型、顺序、数量。参数是可选的，也就是说，函数可能不包含参数。 函数主体：函数主体包含一组定义函数执行任务的语句 函数参数 调用类型 描述 传值调用 该方法把参数的实际值复制给函数的形式参数。在这种情况下，修改函数内的形式参数对实际参数没有影响。 指针调用 该方法把参数的地址复制给形式参数。在函数内，该地址用于访问调用中要用到的实际参数。这意味着，修改形式参数会影响实际参数。 引用调用 该方法把参数的引用复制给形式参数。在函数内，该引用用于访问调用中要用到的实际参数。这意味着，修改形式参数会影响实际参数。 传值调用1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;using namespace std;// 函数声明void swap(int x, int y);int main ()&#123; // 局部变量声明 int a = 100; int b = 200; cout &lt;&lt; "交换前，a 的值：" &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; "交换前，b 的值：" &lt;&lt; b &lt;&lt; endl; // 调用函数来交换值 swap(a, b); cout &lt;&lt; "交换后，a 的值：" &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; "交换后，b 的值：" &lt;&lt; b &lt;&lt; endl; return 0;&#125;// 函数定义void swap(int x, int y)&#123; int temp; temp = x; /* 保存 x 的值 */ x = y; /* 把 y 赋值给 x */ y = temp; /* 把 x 赋值给 y */ return;&#125; 指针调用12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;using namespace std;// 函数声明void swap(int *x, int *y);int main ()&#123; // 局部变量声明 int a = 100; int b = 200; cout &lt;&lt; "交换前，a 的值：" &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; "交换前，b 的值：" &lt;&lt; b &lt;&lt; endl; /* 调用函数来交换值 * &amp;a 表示指向 a 的指针，即变量 a 的地址 * &amp;b 表示指向 b 的指针，即变量 b 的地址 */ swap(&amp;a, &amp;b); cout &lt;&lt; "交换后，a 的值：" &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; "交换后，b 的值：" &lt;&lt; b &lt;&lt; endl; return 0;&#125;// 函数定义void swap(int *x, int *y)&#123; int temp; temp = *x; /* 保存地址 x 的值 */ *x = *y; /* 把 y 赋值给 x */ *y = temp; /* 把 x 赋值给 y */ return;&#125; 引用调用向函数传递参数的引用调用方法，把引用的地址复制给形式参数。在函数内，该引用用于访问调用中要用到的实际参数。这意味着，修改形式参数会影响实际参数。12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;using namespace std;// 函数声明void swap(int &amp;x, int &amp;y);int main ()&#123; // 局部变量声明 int a = 100; int b = 200; cout &lt;&lt; "交换前，a 的值：" &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; "交换前，b 的值：" &lt;&lt; b &lt;&lt; endl; /* 调用函数来交换值 */ swap(a, b); cout &lt;&lt; "交换后，a 的值：" &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; "交换后，b 的值：" &lt;&lt; b &lt;&lt; endl; return 0;&#125;// 函数定义void swap(int &amp;x, int &amp;y)&#123; int temp; temp = x; /* 保存地址 x 的值 */ x = y; /* 把 y 赋值给 x */ y = temp; /* 把 x 赋值给 y */ return;&#125; 参数的默认值当您定义一个函数，您可以为参数列表中后边的每一个参数指定默认值。当调用函数时，如果实际参数的值留空，则使用这个默认值1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;using namespace std;int sum(int a, int b=20)&#123; int result; result = a + b; return (result);&#125;int main ()&#123; // 局部变量声明 int a = 100; int b = 200; int result; // 调用函数来添加值 result = sum(a, b); cout &lt;&lt; "Total value is :" &lt;&lt; result &lt;&lt; endl; // 再次调用函数 result = sum(a); cout &lt;&lt; "Total value is :" &lt;&lt; result &lt;&lt; endl; return 0;&#125; Lambda 函数与表达式1[capture](parameters)-&gt;return-type&#123;body&#125;]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>变量作用域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++循环语句与条件语句]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E5%BE%AA%E7%8E%AF%E8%AF%AD%E5%8F%A5%E4%B8%8E%E6%9D%A1%E4%BB%B6%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[循环语句 循环类型 描述 while 循环 当给定条件为真时，重复语句或语句组。它会在执行循环主体之前测试条件。 for 循环 多次执行一个语句序列，简化管理循环变量的代码。 do…while 循环 除了它是在循环主体结尾测试条件外，其他与 while 语句类似。 嵌套循环 您可以在 while、for 或 do..while 循环内使用一个或多个循环。 循环控制语句 控制语句 描述 break 语句 终止 loop 或 switch 语句，程序流将继续执行紧接着 loop 或 switch 的下一条语句。 continue 语句 引起循环跳过主体的剩余部分，立即重新开始测试条件。 goto 语句 将控制转移到被标记的语句。但是不建议在程序中使用 goto 语句。 判断语句 语句 描述 if 语句 一个 if 语句 由一个布尔表达式后跟一个或多个语句组成。 if…else 语句 一个 if 语句 后可跟一个可选的 else 语句，else 语句在布尔表达式为假时执行。 嵌套 if 语句 您可以在一个 if 或 else if 语句内使用另一个 if 或 else if 语句。 switch 语句 一个 switch 语句允许测试一个变量等于多个值时的情况。 嵌套 switch 语句 您可以在一个 switch 语句内使用另一个 switch 语句。 ? : 运算符1Exp1 ? Exp2 : Exp3; Exp1为真，则计算Exp2 Exp1为假，则计算Exp3]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>循环语句</tag>
        <tag>条件语句</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++运算符]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[运算符 描述 实例 + 把两个操作数相加 A + B 将得到 30 - 从第一个操作数中减去第二个操作数 A - B 将得到 -10 * 把两个操作数相乘 A * B 将得到 200 / 分子除以分母 B / A 将得到 2 % 取模运算符，整除后的余数 B % A 将得到 0 ++ 自增运算符，整数值增加 1 A++ 将得到 11 — 自减运算符，整数值减少 1 A— 将得到 9 == 检查两个操作数的值是否相等，如果相等则条件为真。 (A == B) 不为真。 != 检查两个操作数的值是否相等，如果不相等则条件为真。 (A != B) 为真。 &gt; 检查左操作数的值是否大于右操作数的值，如果是则条件为真。 (A &gt; B) 不为真。 &lt; 检查左操作数的值是否小于右操作数的值，如果是则条件为真。 (A &lt; B) 为真。 &gt;= 检查左操作数的值是否大于或等于右操作数的值，如果是则条件为真。 (A &gt;= B) 不为真。 &lt;= 检查左操作数的值是否小于或等于右操作数的值，如果是则条件为真。 (A &lt;= B) 为真。 &amp;&amp; 称为逻辑与运算符。如果两个操作数都非零，则条件为真。 (A &amp;&amp; B) 为假。 &#124;&#124; 称为逻辑或运算符。如果两个操作数中有任意一个非零，则条件为真。 (A &#124;&#124; B) 为真。 ! 称为逻辑非运算符。用来逆转操作数的逻辑状态。如果条件为真则逻辑非运算符将使其为假。 !(A &amp;&amp; B) 为真。 &amp; 如果同时存在于两个操作数中，二进制 AND 运算符复制一位到结果中。 (A &amp; B) 将得到 12，即为 0000 1100 &#124; 如果存在于任一操作数中，二进制 OR 运算符复制一位到结果中。 (A &#124; B) 将得到 61，即为 0011 1101 ^ 如果存在于其中一个操作数中但不同时存在于两个操作数中，二进制异或运算符复制一位到结果中。 (A ^ B) 将得到 49，即为 0011 0001 ~ 二进制补码运算符是一元运算符，具有”翻转”位效果，即0变成1，1变成0。 (~A ) 将得到 -61，即为 1100 0011，一个有符号二进制数的补码形式。 &lt;&lt; 二进制左移运算符。左操作数的值向左移动右操作数指定的位数。 A &lt;&lt; 2 将得到 240，即为 1111 0000 &gt;&gt; 二进制右移运算符。左操作数的值向右移动右操作数指定的位数。 A &gt;&gt; 2 将得到 15，即为 0000 1111 = 简单的赋值运算符，把右边操作数的值赋给左边操作数 C = A + B 将把 A + B 的值赋给 C += 加且赋值运算符，把右边操作数加上左边操作数的结果赋值给左边操作数 C += A 相当于 C = C + A -= 减且赋值运算符，把左边操作数减去右边操作数的结果赋值给左边操作数 C -= A 相当于 C = C - A *= 乘且赋值运算符，把右边操作数乘以左边操作数的结果赋值给左边操作数 C = A 相当于 C = C A /= 除且赋值运算符，把左边操作数除以右边操作数的结果赋值给左边操作数 C /= A 相当于 C = C / A %= 求模且赋值运算符，求两个操作数的模赋值给左边操作数 C %= A 相当于 C = C % A &lt;&lt;= 左移且赋值运算符 C &lt;&lt;= 2 等同于 C = C &lt;&lt; 2 &gt;&gt;= 右移且赋值运算符 C &gt;&gt;= 2 等同于 C = C &gt;&gt; 2 &amp;= 按位与且赋值运算符 C &amp;= 2 等同于 C = C &amp; 2 ^= 按位异或且赋值运算符 C ^= 2 等同于 C = C ^ 2 &#124;= 按位或且赋值运算符 C &#124;= 2 等同于 C = C &#124; 2 优先级 类别 运算符 结合性 后缀 () [] -&gt; . ++ - - 从左到右 一元 + - ! ~ ++ - - (type)* &amp; sizeof 从右到左 乘除 * / % 从左到右 加减 + - 从左到右 移位 &lt;&lt; &gt;&gt; 从左到右 关系 &lt; &lt;= &gt; &gt;= 从左到右 相等 == != 从左到右 位与 AND &amp; 从左到右 位异或 XOR ^ 从左到右 位或 OR &#124; 从左到右 逻辑与 AND &amp;&amp; 从左到右 逻辑或 OR &#124;&#124; 从左到右 条件 ?: 从右到左 赋值 = += -= *= /= %=&gt;&gt;= &lt;&lt;= &amp;= ^= &#124;= 从右到左 逗号 , 从左到右]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++变量作用域]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[局部变量在函数或一个代码块内部声明的变量，称为局部变量。它们只能被函数内部或者代码块内部的语句使用。下面的实例使用了局部变量： 123456789101112131415161718#include &lt;iostream&gt;using namespace std;int main ()&#123; // 局部变量声明 int a, b; int c; // 实际初始化 a = 10; b = 20; c = a + b; cout &lt;&lt; c; return 0;&#125; 全局变量在所有函数外部定义的变量（通常是在程序的头部），称为全局变量。全局变量的值在程序的整个生命周期内都是有效的。 全局变量可以被任何函数访问。也就是说，全局变量一旦声明，在整个程序中都是可用的。下面的实例使用了全局变量和局部变量： 1234567891011121314151617181920#include &lt;iostream&gt;using namespace std;// 全局变量声明int g;int main ()&#123; // 局部变量声明 int a, b; // 实际初始化 a = 10; b = 20; g = a + b; cout &lt;&lt; g; return 0;&#125; 在程序中，局部变量和全局变量的名称可以相同，但是在函数内，局部变量的值会覆盖全局变量的值。下面是一个实例： 123456789101112131415#include &lt;iostream&gt;using namespace std;// 全局变量声明int g = 20;int main ()&#123; // 局部变量声明 int g = 10; cout &lt;&lt; g; return 0;&#125; 当上面的代码被编译和执行时，它会产生下列结果：110]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>变量作用域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++数据类型]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[数据类型 初始值 数据类型 初始化默认值 int 0 char ‘\0’ float 0 double 0 pointer NULL typedef 声明例如，下面的语句会告诉编译器，feet 是 int 的另一个名称：1typedef int feet; 枚举类型枚举类型(enumeration)是C++中的一种派生数据类型，它是由用户定义的若干枚举常量的集合。 如果一个变量只有几种可能的值，可以定义为枚举(enumeration)类型。所谓”枚举”是指将变量的值一一列举出来，变量的值只能在列举出来的值的范围内。 创建枚举，需要使用关键字 enum。枚举类型的一般形式为：123456enum 枚举名&#123; 标识符[=整型常数], 标识符[=整型常数],... 标识符[=整型常数]&#125; 枚举变量; 如果枚举没有初始化, 即省掉”=整型常数”时, 则从第一个标识符开始。 例如，下面的代码定义了一个颜色枚举，变量 c 的类型为 color。最后，c 被赋值为 “blue”。12enum color &#123; red, green, blue &#125; c;c = blue; 默认情况下，第一个名称的值为 0，第二个名称的值为 1，第三个名称的值为 2，以此类推。但是，您也可以给名称赋予一个特殊的值，只需要添加一个初始值即可。例如，在下面的枚举中，green 的值为 5。1enum color &#123; red, green=5, blue &#125;; 在这里，blue 的值为 6，因为默认情况下，每个名称都会比它前面一个名称大 1，但 red 的值依然为 0。 C++ 中的左值（Lvalues）和右值（Rvalues）C++ 中有两种类型的表达式： 左值（lvalue）：指向内存位置的表达式被称为左值（lvalue）表达式。左值可以出现在赋值号的左边或右边。 右值（rvalue）：术语右值（rvalue）指的是存储在内存中某些地址的数值。右值是不能对其进行赋值的表达式，也就是说，右值可以出现在赋值号的右边，但不能出现在赋值号的左边。 变量是左值，因此可以出现在赋值号的左边。数值型的字面值是右值，因此不能被赋值，不能出现在赋值号的左边。下面是一个有效的语句：···C++int g = 20;···]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++关键字]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[关键字 asm else new this auto enum operator throw bool explicit private true break export protected try case extern public typedef catch false register typeid char float reinterpret_cast typename class for return union const friend short unsigned const_cast goto signed using continue if sizeof virtual default inline static void delete int static_cast volatile do long struct wchar_t double mutable switch while dynamic_cast namespace template]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>关键字</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++环境配置]]></title>
    <url>%2F2017%2F08%2F29%2FC-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[使用 Visual Studio (Graphical Interface) 编译1、下载及安装 Visual Studio Community 2015。2、打开 Visual Studio Community3、点击 File -&gt; New -&gt; Project 4、左侧列表选择 Templates -&gt; Visual C++ -&gt; Win32 Console Application，并设置项目名为 MyFirstProgram。 5、点击 OK。6、在以下窗口中点击 Next 7、在弹出的窗口中选择 Empty project 选项后，点击 Finish 按钮：8、右击文件夹 Source File 并点击 Add —&gt; New Item… : 9、选择 C++ File 然后设置文件名为 main.cpp，然后点击 Add： 10、拷贝以下代码到 main.cpp 中：1234567#include &lt;iostream&gt;int main()&#123; std::cout &lt;&lt; "Hello World!\n"; return 0;&#125; 界面如下所示： 11、点击菜单上的 Debug -&gt; Start Without Debugging (或按下 ctrl + F5) : 12、完成以上操作后，你可以看到以下输出]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[循环链表]]></title>
    <url>%2F2017%2F08%2F28%2F%E5%BE%AA%E7%8E%AF%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[定义将单链表中终端结点的指针端有空指针改为指向头结点，就使整个单链表形成一个环，这种头尾相接的单链表成为单循环链表，简称单链表(circle linked list) 但是循环链表进过改造，不用头指针，而是用指向终端结点的尾指针来表示循环链表，这样查找开始结点和终端结点都很方便。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;/*链式存储结构的定义*/typedef struct CLinkList&#123; int data; struct CLinkList * next;&#125;node;/*** 1.初始化循环链表*/void ds_init(node **pNode)&#123; int item; node * temp; node * target; printf("输入结点的值，输入0完成初始化\n"); while (1) &#123; scanf("%d", &amp;item); fflush(stdin); // 清空输入缓存区 if (item == 0) return; if ((*pNode) == NULL) &#123; // 链表中只有一个结点 *pNode = (node *)malloc(sizeof(struct CLinkList)); if (!(*pNode)) exit(0); (*pNode)-&gt;data = item; (*pNode)-&gt;next = *pNode; &#125;else&#123; //找到next指向第一个结点的结点 for (target = (*pNode); target-&gt;next != (*pNode); target = target-&gt;next); // 生成一个新的结点 temp = (node *)malloc(sizeof(struct CLinkList)); if (!temp) exit(0); temp-&gt;data = item; temp-&gt;next = *pNode; target-&gt;next = temp; &#125; &#125;&#125;/*** 2.插入结点* @param pNode 链表的第一个结点* @param i 插入的位置*/void ds_insert(node **pNode, int i)&#123; node * temp; node * target; node * p; int item; int j = 1; printf("输入要插入加点的值:"); scanf("%d", &amp;item); if (i == 1) &#123; // 插入到第一个位置 // 新插入的结点作为第一个结点 temp = (node *)malloc(sizeof(struct CLinkList)); if (!temp) exit(0); temp-&gt;data = item; //找到最后一个结点 for (target = (*pNode); target-&gt;next != (*pNode); target = target-&gt;next); temp-&gt;next = (*pNode); target-&gt;next = temp; *pNode = temp; &#125;else&#123; // 插入到其他位置 target = *pNode; for (; j&lt;(i-1); j++) &#123; target = target-&gt;next; &#125; temp = (node *)malloc(sizeof(struct CLinkList)); if (!temp) exit(0); temp-&gt;data = item; p = target-&gt;next; target-&gt;next = temp; temp-&gt;next = p; &#125;&#125;/*** 3.删除结点* @param pNode 链表的第一个结点* @param i 删除的位置*/void ds_delete(node **pNode, int i)&#123; node * target; node * temp; int j = 1; if (i ==1) &#123; // 删除的是第一个结点 // 找到最后一个结点 for (target = *pNode; target-&gt;next != *pNode; target = target-&gt;next); temp = *pNode; *pNode = (*pNode)-&gt;next; target-&gt;next = *pNode; free(temp); &#125;else&#123; // 删除其他结点 target = *pNode; for (; j&lt;i-1 ; j++) &#123; target = target-&gt;next; &#125; temp = target-&gt;next; target-&gt;next = temp-&gt;next; free(temp); &#125;&#125;/*** 4.返回结点所在位置* @param pNode 链表的第一个结点* @param elem 结点所在位置*/int ds_search(node *pNode, int elem)&#123; node * target; int i = 1; for (target = pNode; target-&gt;data != elem &amp;&amp; target-&gt;next != pNode; i++) &#123; target = target-&gt;next; &#125; if (target-&gt;next == pNode) return 0; // 表中不存在该元素 else return i;&#125;/*** 5.遍历*/void ds_traverse(node *pNode)&#123; node * temp; temp = pNode; printf("*************链表中的元素**********\n"); do &#123; printf("%4d ", temp-&gt;data); &#125; while ((temp = temp-&gt;next) != pNode); printf("\n");&#125;/*** 5.合并链表*/LinkList Connect(LinkList A, LinkList B)&#123; LinkList p = A-&gt;next; // 保存A表的头结点 A-&gt;next = B-&gt;next-&gt;next; // 将B表的开始结点链接到A表尾部 free(B); // 释放B表的头结点 B-&gt;next = p; return B; // 返回新循环链表的尾指针&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>线性表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单链表]]></title>
    <url>%2F2017%2F08%2F28%2F%E5%8D%95%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[定义数据域：存储数据元素信息的域，指针域：存储直接后继位置的域。指针域中存储的信息成为指针或链。这两部分信息组成数据元素成为存储映像，成为结点(Node)。数据域|指针域-|-data|next 头结点与头指针 头结点头结点是加在单链表之前附设的一个头结点。头结点的数据域一般不存储任何信息，也可以存放一些关于线性表的长度的附加信息。-头指针头指针是指链表指向第一个结点的指针，若链表有头结点，则是指向头结点的指针。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128#include&lt;iostream&gt;#include&lt;stdio.h&gt;#include&lt;math.h&gt;#define NULL 0typedef int ElemType;void Error(char *s)&#123; cout&lt;&lt;s&lt;&lt;endl; exit(1);&#125;typedef struct LNode&#123; ElemType data; struct LNode *next;&#125;LNode;typedef LNode *Linklist; //头指针void Error(char *s) //错误指示&#123; std::cout &lt;&lt; s &lt;&lt; endl; exit(1);&#125;//创建链表,生成一个新节点作为头结点,并设其指针域为空Void InitList(Linklist &amp;L)&#123; L=new LNode; L-&gt;next=NULL;&#125;//销毁链表,从链表的头结点开始依次释放表中每一个结点所占用的存储空间Void DestroyList(Linklist L)&#123; while(L)&#123; LNode *p=L; L=L-&gt;next; delete p; &#125;&#125;//设置单链表为空表，并释放所有结点空间void ClearList(Linklist &amp;L)&#123; LNode *p = L-&gt;next; L-&gt;next=NULL; L.length=0; while (p) &#123; Linklist *q=p; p = p-&gt;next; delete q; &#125;&#125;//求链表长度int ListLength(Linklist L)&#123; int length=0; LNode *p = L; while(p-&gt;next)&#123; length++; p=p-&gt;next; &#125; return length;&#125;//返回链表中第i个结点数据域的值，1&lt;=i&lt;=表长;若i值不合法，则给出错误信息ElemType GetElem_L(Linklist L, int i)&#123; int count = 1; LNode *p = L-&gt;next; while (p &amp;&amp; (count &lt; i)) &#123; p = p-&gt;next; count++; &#125; if (!(p-&gt;next)||count&gt;i) Error("position Error") ; else return p-&gt;data;&#125;//查找链表中第一个数据域值和x相等的结点LNode *LocateElem_L(Linklist L, ElemType x)&#123; LNode *p =L-&gt;next; //将指针头传给指针p int j = 1; while (p &amp;&amp; (p-&gt;data != x)) //向后扫描查找 &#123; p = p-&gt;next; &#125; return p;&#125;//向链表中第i个位置插入元素x ,1&lt;=x&lt;=表长+1//若插入位置不合理则给出相应信息void ListInsert(Linklist &amp;L, int i, int stu)&#123; LNode *p =L; //将指针头传给指针p int j = 0; while (p &amp;&amp; (j &lt; i - 1)) //向后扫描查找 &#123; p = p-&gt;next; j++; &#125; if (!p || (j &gt; i - 1)) //输入参数不对，给出错误提示，并跳出 Error("Postion Eeeor!"); else &#123; LNode *s = new LNode; s-&gt;next = p-&gt;next; p-&gt;next = s; &#125;&#125;//删除链表L中第i个结点数据域值，并用返回,1&lt;=x&lt;=表长+1//若删除位置不合理则给出相应信息ElemType ListDelete_L(Linklist &amp;L, int i)&#123; LNode *p = L; //将指针头传给指针p int j = 0; while ((p -&gt;next) &amp;&amp; (j &lt; i - 1)) &#123; p = p-&gt;next; j++; &#125; if (!(p-&gt;next) || (j &gt; i - 1)) Error("Postion Eeeor!"); // 输入参数不对，给出错误提示，并跳出 else &#123; LNode *q = p-&gt;next; ElemType x= p-&gt;data; p-&gt;next = q-&gt;next; delete q; return x; &#125;&#125; 单链表正标操作整表创建头插法12345678910111213141516171819202122/*** 头插法*/void CreatListHead(LinkList *L, int n)&#123; LinkList p; int i; srand(time(0)); // 初始化随机数种子 *L = (LinkList)malloc(sizeof(Node)); (*L)-&gt;next = NULL; for (i = 0; i &lt; n; i++) &#123; p = (LinkList)malloc(sizeof(Node)); // 生成新节点 p -&gt;data = rand()%100 + 1; p-&gt;next = (*L)-&gt;next; (*L)-&gt;next = p; &#125;&#125; 尾插法1234567891011121314151617181920212223/*** 尾插法*/void CreatListTail(LinkList *L, int n)&#123; LinkList p, r; int i; srand(time(0)); *L = (LinkList)malloc(sizeof(Node)); r = *L; for (i = 0; i &lt; n; i++) &#123; p = (Node *)malloc(sizeof(Node)); p-&gt;data = rand()%100+1; r-&gt;next = p; r = p; &#125; r-&gt;next = NULL;&#125; 整表删除12345678910111213141516Status ClearList(LinkList *L)&#123; LinkList p,q; p = (*L)-&gt;next; while (p) &#123; q = p-&gt;next; free(p); p = q; &#125; (*L)-&gt;next = NULL; return OK;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>线性表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[顺序表]]></title>
    <url>%2F2017%2F08%2F28%2F%E9%A1%BA%E5%BA%8F%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[定义顺序存储结构是指用一段地址连续的存储单元依次存储线性表的数据元素。通常都用数组来描述数据结构中的顺序存储结构。 代码数据结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187# include &lt;stdio.h&gt;# include &lt;stdlib.h&gt;# define LIST_INIT_SIZE 100# define OK 1# define ERROR -1typedef int ElemType;typedef int Status;typedef struct &#123; ElemType elem[LIST_INIT_SIZE]; //顺序表大小，可根据实际需要而定 int length; //线性表长度 int listsize; //数组长度（以ElemType为单位）&#125; SqList; //由于c语言中数组的下标从0开始，因此线性表的第一个元素a1和最后一个元素a0分别存储在L.elem[0]和L.elem[L.length-1]Status InitList_Sq(SqList &amp; L);Status ListInsert_Sq(SqList &amp; L, int i, ElemType e);Status ListDelete_Sq(SqList &amp; L, int i, ElemType &amp; e);int LocateElem_Sq(SqList L, ElemType e, Status (* compare)(ElemType, ElemType));void MergeList_Sq(SqList La, SqList Lb, SqList &amp;Lc);Status ShowList_Sq(SqList &amp; L);Status compare(ElemType a, ElemType b);/*******************Status InitList(SqList &amp;L)功能：初始化顺序线性表参数： SqList &amp;L：顺序线性表L返回值: Status类型：OK为执行正确，ERROR为出错*******************/Status InitList_Sq(SqList &amp; L) //&amp;L 对线性表L的引用&#123; //算法2.3 List_Size L.elem = new ElemType[LIST_INIT_SIZE]; //elem 数组的名称是一个指针，(ElemType *)返回一个指针 if(!L.elem) &#123; exit(OVERFLOW); &#125; L.length = 0; L.listsize = LIST_INIT_SIZE; return OK;&#125;/*******************Void DestroyList_Sq(SqList)功能：销毁顺序表：*******************/void DestroyList_Sq(SqList) &#123;//释放顺序表L所占用的存储空间 delete []L.elem; L.length=0; L.listsize=0;&#125;/*******************Status ListEmpty(SqList L)功能： 检测是否为空表限制： 初始条件：顺序线性表L已存在参数： SqList L：顺序线性表L返回值: Status类型：TRUE表示L为空表，否则返回FALSE*******************/Status ListEmpty(SqList L)&#123; if(L.length == 0) return TRUE; else return FALSE;&#125;/*******************Status ClearList(SqList *L)功能： 将L重置为空表限制： 初始条件：顺序线性表L已存在参数： SqList &amp;L：顺序线性表L返回值: Status类型：OK表示执行正确*******************/Status ClearList(SqList &amp;L) L-&gt;length=0; return OK;&#125;/*******************int ListLength(SqList L)功能： 获取L中数据元素个数限制： 初始条件：顺序线性表L已存在参数： SqList L：顺序线性表L返回值: int类型：返回L中数据元素个数*******************/int ListLength(SqList L) &#123; return L.length;&#125;/*******************Status GetElem(SqList L,int i,ElemType *e)功能： 用e返回L中第i个数据元素的值（注意i是指位置）限制： 初始条件：顺序线性表L已存在，1≤i≤ListLength(L)参数： SqList L：顺序线性表L int i：要取出元素的位置 ElemType *e：所取出的元素返回值: Status类型：OK表示执行正确，ERROR为出错*******************/Status GetElem(SqList L,int i,ElemType *e)&#123; if(L.length==0 || i&lt;1 || i&gt;L.length) return ERROR; *e=L.elem[i-1]; return ok;&#125;/*******************Status ListInsert(SqList &amp;L,int i,ElemType x)功能： 在L中第i个位置前插入新的数据元素x限制： 初始条件：顺序线性表L已存在，,1≤i≤ListLength(L)参数： SqList &amp;L：顺序线性表L int i：位置i ElemType x：新的数据元素x返回值: Status类型：OK表示执行正确，ERROR为出错*******************/Status ListInsert_Sq(SqList &amp; L, int i, ElemType x)&#123; //算法2.4 //在顺序线性表L中第i个元素之前插入x if(i &lt; 1 || i &gt; L.length + 1) return ERROR; if(L.length &gt;= L.listsize) return ERROR; ElemType *q = &amp; (L.elem[i - 1]); for(ElemType *p = &amp; (L.elem[L.length - 1]); p &gt;= q; --p) &#123; *(p + 1) = *p; //元素后移 &#125; *q = x; ++L.length; //长度加1 return OK;&#125;/*******************Status ListDelete(SqList —L,int i,ElemType e)功能： 删除L的第i个数据元素限制： 初始条件：顺序线性表L已存在，,1≤i≤ListLength(L)参数： SqList —L：顺序线性表L int i：位置i返回值: Status类型：OK表示执行正确，ERROR为出错*******************/Status ListDelete_Sq(SqList &amp;L, int i)&#123; //算法2.5 if(i &lt; 1 || i &gt; L.length) return ERROR; //输出错误 ElemType *p = &amp; (L.elem[i - 1]); ElemType *q = L.elem + L.length - 1; for(++p; p &lt;= q; ++p) &#123; * (p - 1) = * p; //元素前移 &#125; --L.length; return OK;&#125;/*******************int LocateElem_Sq(SqList L,ElemType x)功能： 找出L中第1个与x满足相等关系的数据元素的位序限制： 初始条件：顺序线性表L已存在参数： SqList L：顺序线性表L ElemType x：元素x返回值: int类型：返回L中第1个与e满足相等关系的数据元素的位序；若这样的数据元素不存在，则返回值为0 *******************/int LocateElem_Sq(SqList L, ElemType x)&#123; //算法2.6 //指向函数的指针的使用 int i = 1; ElemType * p = L.elem; while((i &lt;= L.length )&amp;&amp; (*p++ != x)) ++i; if(i &lt;= L.length) &#123; return i; &#125; else &#123; return 0;&#125; 测试代码12345678910111213141516171819202122232425262728int main(void)&#123; //测试函数 SqList L; InitList_Sq(L); for(int i = 10; i &gt; 0; i--) &#123; ListInsert_Sq(L, 1, i); &#125; ShowList_Sq(L); //print L int e; ListDelete_Sq(L, 4, e); ShowList_Sq(L); //print L printf("%d\n", LocateElem_Sq(L, 3, * compare)); //print location of 3 //函数做参数如何传值呢？？ SqList Lb; InitList_Sq(Lb); for(int i = 10; i &gt; 0; i--) &#123; ListInsert_Sq(Lb, 1, 2 * i); &#125; ShowList_Sq(Lb); //print Lb SqList Lc; InitList_Sq(Lc); ShowList_Sq(Lc); //print Lc MergeList_Sq(L, Lb, Lc); ShowList_Sq(Lc); //print Lc return 0;&#125; 缺点线性表的顺序存储结构，最大的缺点就是插入和删除时需要移动大量的元素，这显然需要耗费时间。导致这个问题的原因是在于相邻元素的存储位置具有邻居关系，它们在内存中的位置是紧挨着的，中间没有间隙，当然无法快速插入和删除。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>线性表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构目录]]></title>
    <url>%2F2017%2F08%2F28%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[一、绪论二、抽象数据类型ADT12345ADT 抽象数据类型名称&#123; 数据对象(Data):&lt;数据对象的定义&gt; 数据关系 :&lt;数据关系的定义&gt; 基本操作(Operation):&lt;基本操作的定义&gt;&#125;ADT抽象数据类型名称 线性表12345678910111213141516171819202122232425262728293031323334353637ADT 线性表（List）Data 线性表的数据对象集合为&#123;a1,a2,…,an&#125;，每个元素的类型均为DataType。 其中，除第一个元素a1外，每一个元素有且只有一个直接前驱元素，除了最后一个元素an外，每一个元素有且只有一个直接后继元素。 数据元素之间的关系是一对一的关系。Operation InitList(): 初始化操作，建立一个空的线性表L。 DestroyList():线性表已经存在，释放该线性表所占用的存储空间 ClearList(): 线性表已经存在，重置线性表为空表，将线性表清空。 ListLength(): 线性表已经存在返回线性表L的元素个数。 GetElem(): 求线性表L中的第i个位置元素值。 初始条件：线性表已存在 输入参数：元素序号i，1&lt;=i&lt;=ListLength(L) 实现功能：取线性表中序号为i的元素值 输出数据：如果序号不合理，则给出错误信息；否则返回序号为i的元素序号值 操作结果：线性表不变 LocateElem(): 在线性表L中查找与给定值x相等的元素，如果查找成功，返回线性表中第1个值等于x的元素序号;否则返回0。 初始条件：线性表已存在 输入参数：元素x 实现功能：查找线性表中第1个值等于x的元素 输出数据：如果查找成功，返回线性表中第1个值等于x的元素序号；否则返回0 操作结果：线性表不变 ListInsert(): 在线性表L中第i个位置插入新元素e。 初始条件：线性表已存在 输入参数：插入位置i,1&lt;=i&lt;=ListLength(L)+1,ListLength(L)表示插入前的表长;待插入元素x 实现功能：插入x到线性表的第i个位置上 输出数据：如果插入不成功，则给出错误信息 操作结果：当插入成功时，线性表中增加了一个元素x，且表长增1 ListDelete(): 删除线性表L中第i个位置元素，并用e返回其值。 初始条件：线性表已存在 输入参数：删除位置i,1&lt;=i&lt;=ListLength(L)+1,ListLength(L)表示插入前的表长 实现功能：删除线性表中的第i个元素 输出数据：如果删除不成功，则给出错误信息。否则返回第i个元素值 操作结果：当删除成功时，线性表中减少了一个元素，且表长减1endADT 顺序表单链表静态链表.md 没有审核代码循环链表双向链表.md 没有审核代码 特殊线性表-栈、队列和串栈：后进先出123456789101112131415161718192021ADT Stack&#123; Data： 栈中元素具有相同类型及后进先出特性，相邻元素具有前驱和后继关系 Operation： InitStack()：初始化栈，构造一个空栈 DestroyStack()：销毁栈，释放栈所占用的存储空间 StackLength()：求栈的长度 GetTop()： 实现功能：读取当前栈顶元素 输出数据：如果栈空，则给出错误信息；否则返回当前栈顶元素值 Push()： 输入参数：待插入元素 实现功能：插入元素到栈顶 输出数据：如果插入不成功，则给出错误信息 操作结果：当插入成功时，栈顶增加了一个元素 Pop()： 输入参数：无 实现功能：删除栈顶元素 输出数据：如果栈为空，则给出错误信息；否则返回栈顶元素 操作结果：当删除成功时，栈顶减少了一个元素&#125; 栈的顺序储结构栈.md栈的链式存储结构.md 队列：先进先出123456789101112131415161718192021ADT Queue&#123; Data： 队列中元素具有相同类型及先进先出特性，相邻元素具有前驱和后继关系 Operation： InitQueue()：初始化队列，构造一个空队列 DestroyQueue()：销毁队列，释放队列所占用的存储空间 QueueLength()：求队列的长度 GetHead()： 实现功能：读取当前队列头元素 输出数据：如果队列空，则给出错误信息；否则返回当前队列头元素值 EnQueue()： 输入参数：待插入元素 实现功能：插入元素到队列尾 输出数据：如果插入不成功，则给出错误信息 操作结果：当插入成功时，队列尾增加了一个元素 DeQueue()： 输入参数：无 实现功能：删除队列头元素 输出数据：如果队列为空，则给出错误信息；否则返回队列头元素 操作结果：当删除成功时，队列尾减少了一个元素&#125; 队列的顺序存储结构.md队列链式存储结构.md 串123456789101112131415161718ADT 串(string)&#123; Data 串中元素仅由一个字符组成,相邻元素具有前驱和后继关系. Operation StrAssign(T,*chars):生成一个其值等于字符常量chars的串T. StrCopy(T,S):串S存在,由S复制得到T. ClearString(S):串S存在,将串清空. StringEmpty(S): 若串为空,返回true,否则返回false. StrLentgth(S) :返回串S的元素个数,即串的长度. StrCompare(S,L):比较S和T,若S&gt;T,返回&gt;0,S==T返回0, S&lt;T返回&lt;0 SubString(Sub, S, pos, len): 串S存在,1&lt;=pos&lt;=StrLentgth(S),且 0&lt;=len&lt;=StrLentgth(S)-pos+1,用Sub返回串S的第pos个起,长度为len的子串. Index(S,T,pos) Replace(S,T,V)串S,T,V存在,T是非空串,用V替换S中出现的所有与T相等的不重叠的子串. StrInsert(S,T,pos): 在串S的第pos个字符之前插入串T. StrDelete(S,pos,len):从串S中删除第pos个字符起的长度为len的子串.&#125;endADT 串的顺序存储结构.md 数组和广义表数组：由一组类型相同、下标不同的变量构成。根据数组中存储的数据元素之间的逻辑关系，可以将数组分为 : 一维数组、二维数组、…、n维数组。12345678910111213141516171819202122232425ADT Array &#123;Data: 相同类型元素有序集合，每个元素受n(n&gt;=1)个线性关系的约束并由一组下标唯一标识Operation: InitArray(): //构造一个空数组,数组的维数和各维的长度 DestroyArray()：//销毁数组,释放数组所占用的存储空间 GetValue(A,&amp;e,index1,…,indexn) //求值函数,求某个下标元素的值 初始条件:A是维数组,e为元素变量,随后是n个下标值. 操作结果:若各下标不超界,则e赋值为所指定的A的元素值,并返回OK. Assign(&amp;A,e,index1,…,indexn)//赋值函数,给下具体的下标的元素赋值 初始条件:A是n维数组,e为元素变量,随后是n个下标值. 操作结果:若下标不超界,则将e的值赋给所指定的A的元素,并返回OK.&#125;ADT Array 树12345678910111213141516171819202122ADT Tree&#123; Data: 树 Operation： InitTree(&amp;T):构造空树 DestroyTree(&amp;T)：销毁树 CreateTree(&amp;T,dfinition): 初始条件:defination给出树T的定义 操作结果：按defination构造树T ClearTree(&amp;T):将树清为空树 TreeEmpty(T):若树为空树，则返回TRUE,否则FALSE TreeDepth(T)：返回树的深度 Root(T):返回T的根 Value(T,node):树T存在，node是T中的某一个结点，返回node的值 Assign(T,node,value):树T存在，node是T中的某一个结点，结点node赋值为value Parent(T,node):若node是T的非根结点,返回它的双亲，否则函数值为空 LeftChild(T,node):若node是T的非叶子结点,返回它的最左孩子，否则函数值为空 RightSiblingChild(T,node):若node右兄弟,返回它的右兄弟，否则函数值为空 InsertChild(&amp;T,&amp;p,i,c):树T存在，p指向是T中某一个结点，插入c为T中p所指节点的第i颗子树 DeleteChild(&amp;T,&amp;p,i,c):树T存在，p指向是T中某一个结点，删除T中p所指节点的第i颗子树 TraveseTree():遍历树&#125;ADT Tree 树与森林.md树的存储结构.md java版二叉树.md线索二叉树.md哈赫夫曼树.md 图12345678910111213141516171819ADT 图(Graph)Data 顶点的有穷非空集合和边的集合。Operation CreateGraph(*G, V, VR): 按照顶点集V和边弧集VR的定义构造图G。 DestroyGraph(*G): 图G存在则销毁。 LocateVex(G, u): 若图G中存在顶点u，则返回图中的位置。 GetVex(G, v): 返回图G中顶点v的值。 PutVex(G, v, value): 将图G中顶点v赋值value。 FirstAdjVex(G, *v): 返回顶点v的一个邻接顶点，若顶点在G中无邻接顶点返回空。 NextAdjVex(G, v, *w): 返回顶点v相对于顶点w的下一个邻接顶点， 若w是v的最后一个邻接点则返回“空”。 InsertVex(*G, v): 在图G中增添新顶点v。 DeleteVex(*G, v): 删除图G中顶点v及其相关的弧。 InsertArc(*G, v, w): 在图G中增添弧&lt;v,w&gt;，若G是无向图，还需要增添对称弧&lt;w,v&gt;。 DeleteArc(*G, v, w): 在图G中删除弧&lt;v,w&gt;，若G是无向图，则还删除对称弧&lt;w,v&gt;。 DFSTraverse(G): 对图G中进行深度优先遍历，在遍历过程对每个顶点调用。 HFSTraverse(G): 对图G中进行广度优先遍历，在遍历过程对每个顶点调用。endADT 图的术语与定义.md图的抽象结构与存储结构.md(无向图采用多重邻接表，有向图采用十字链表)图的遍历.md最小生成树.md最短路径问题.md拓扑排序.md关键路径.md 区别.md查找 静态查找(Static Search Table)：只作查找操作的查找表。主要操作有： 查询某个“特定的”数据元素是否在查找表中。 检索某个“特定的”数据元素和各种属性。线性表的查找.md 动态查找表(Dynamic Search Table)：在查找过程中同时插入查找表中不存在的数据元素，或者从查找表中删除已经存在的某个特定的数据元素。主要操作有两个： 查找时插入数据元素； 查找时删除数据元素。查找树.md平衡二叉树.md2-3树.mdB树红黑树 哈希表散列查找.md 排序 排序基础代码123456789101112131415161718192021#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAXSIZE 100#define TRUE 1#define FALSE 0typedef struct &#123; int r[MAXSIZE + 1]; // 用于存储要排序的数组， r[0]用作哨兵或者临时变量 int length; // 用于记录顺序表的长度&#125;SqList;/*** 交换数组r中下标i和j的值*/void swap(SqList *L, int i, int j)&#123; int temp = L-&gt;r[i]; L-&gt;r[i] = L-&gt;r[j]; L-&gt;r[j] = temp;&#125; 插入排序.md (直接插入排序，希尔排序。希尔排序根据直接插入排序而来)选择排序.md交换排序.md归并排序.md(从0开始) 排序代码集合(从0开始).md 性能比较 排序方法 平均情况 最好情况 最坏情况 辅助空间 稳定性 冒泡排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ 稳定 简单选择排序 $O(n^2)$ $O(n^2)$ $O(n^2)$ $O(1)$ 稳定 直接插入排序 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ 稳定 希尔排序 $O(nlogn)~O(n^2)$ $O(n^{1.3})$ $O(n^2)$ $O(1)$ 不稳定 堆排序 $O(nlogn)$ $O(nlogn)$ $O(nlogn)$ $O(1)$ 不稳定 归并排序 $O(nlogn)$ $O(nlogn)$ $O(nlogn)$ $O(n)$ 稳定 快速排序 $O(nlogn)$ $O(nlogn)$ $O(n^2)$ $O(nlogn)~O(n)$ 不稳定 附录图-邻接矩阵代码.md邻接表代码.md邻接矩阵与邻接表深度遍历.md邻接矩阵与邻接表的广度优先遍历算法.md普里姆（Prim）算法代码.md克鲁斯卡尔（Kruskal）算法代码.md迪杰斯特拉（Dijkstra）算法代码.md]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>目录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2017%2F08%2F26%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[Bagging（套袋法）bagging的算法过程如下： 从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复） 对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等） 对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同） 随机森林鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，其中每棵树都和其他树略有不同，随机森林背后的思想是，每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对这些树的结果取平均值来降低过拟合。既能减少过拟合又能保证树的预测能力，这可以在数学上严格证明。 从原始训练集中使用Bootstraping方法随机有放回采样选出m个样本，共进行n_tree次采样，生成n_tree个训练集 对于n_tree个训练集，我们分别训练n_tree个决策树模型 对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂 每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝 将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>模型</tag>
        <tag>树模型</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度类方法与对偶算法]]></title>
    <url>%2F2017%2F08%2F18%2F%E6%A2%AF%E5%BA%A6%E7%B1%BB%E6%96%B9%E6%B3%95%E4%B8%8E%E5%AF%B9%E5%81%B6%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[梯度类方法梯度类方法是无约束优化中非常常用的方法，其依据的最根本的事实就是梯度的负方向是函数值下降最快的方向。但是常用的 gradient descent 必须要求函数的连续可导，而对于某些连续不可导的问题（如lasso regression），gradient descent 无能为力，这是需要用到subgradient descent和proximal gradient descent. gradient descent梯度下降法的迭代公式为 x^{(k)} = x^{(k-1)} - t_k\nabla f(x^{(k-1)} )上式中上标 $(k)$ 表示第 $k$ 次迭代, 而 $t_k$ 表示步长，$\nabla f(x^{(k-1)})$表示在点 $x^{(k-1)}$ 的梯度。 这里对于梯度下降主要讨论其步长选择的问题， 最简单直接的方式是固定每次的步长为一个恒定值，但是如果步长过大或过小时，可能会导致结果难以收敛或者收敛速度很慢。因此提出了可变长步长的方法，可变长步长的方法指的是根据每次迭代依照一定的规则改变步长，下面介绍两种：backtracking line search 和 exact line serach。 backtracking line searchbacktracking line search 需要先选择两个固定的参数 $α,β$ , 要求 $0&lt;β&lt;1,0&lt;α&lt;1/2$每次迭代的时候，假如下式成立 f(x - t\nabla f(x)) > f(x) - \alpha t||\nabla f(x)||_2^2则改变步长为 $t=βt$, 否则步长不变。 这种方法的思想是当步长过大的时候(即跨过了最优点)，减小步长，否则保持步长不变，如下式是一个简单的例子 exact line serachexact line serach 则是得到先计算出梯度 $\nabla f(x^{(k-1)} )$,然后代入下面的函数中，此时只有步长 $t_k$ 是未知，因此可对 $t_k$ 进行求导并令其为0，求得的 $t_k$ 即为当前的最优的步长，因为这个步长令当前迭代下降的距离最大。 f(x^{(k-1)} - t_k\nabla f(x^{(k-1)} ))这种方法也被称为最速下降法。 对偶类算法拉格朗日拉格朗日对偶性是解决带约束的最优化问题的方法，在实际应用中，通过拉格朗日对偶原理将原始问题转换成对偶问题，将原来不容易解决的问题转化为一个容易解决的问题，如支持向量机。 拉格朗日函数假设 $f(x)，c_i(x),h_j(x)$ 是定义在 $\mathbb{R}^{n}$ 上的连续可微函数。我们需要求解约束最优化问题： \underset{x\in \mathbb{R}^n}{min} \; f(x) \tag 1\begin{align} \mathbb{s.t.}\quad &c_i(x) \le 0,\quad i=1,2,\cdots,k \tag 2\\ &h_j(x)=0,\quad j=1,2,\cdots,l \tag 3 \end{align}为了求解原始问题，我们首先引入广义拉格朗日函数(generalized Lagrange function)： L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k \alpha_ic_i(x) + \sum_{j=1}^l\beta_jh_j(x) \tag{4}其中，$x=(x_1,x_2,\cdots,x_n)^T \in \mathbb{R}^n$，$\alpha_i$和$\beta_j$是拉格朗日乘子，特别要求$\alpha_i\geqslant 0$ 极小极大问题如果把$L(x,\alpha,\beta)$看作是$\alpha、\beta$的函数，求其最大值，即 \theta_p(x)=\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta) \tag 5确定$\alpha、\beta$使$L(x,\alpha,\beta)$取得最大值，（此过程中把$x$看做常量）下面通过$x$是否满足约束条件两方面来分析这个函数 如果 $x$ 满足原始问题中约束，由(2)、(3)、(4)、(5)可知 $θ(x)=f(x)$。（少的两项一个是非正的，一个是0，要取最大值的话当然得令两者都为0 如果$x$不满足原始问题中的约束，那么$θ(x)=+∞$。若某个$i$使约束$c_i(x)&gt;0$，则可令则可令$\alpha \rightarrow +∞$，若某个$j$使得$h_j(x)\neq 0,$,则可令$\beta_j h_j(x) \rightarrow +∞$，而将其余各$\alpha _i、\beta_j$均取为0。 综上： \theta_p(x)=\left\{\begin{matrix} f(x),&x 满足原始问题约束\\ +\infty,&其他 \end{matrix}\right.求解原问题的最小值 \underset{x\in \mathbb{R}^n}{min}; \theta_p(x)= \underset{x\in \mathbb{R}^n}{min}\; f(x)=\underset{x\in \mathbb{R}^n}{min}\;\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)极大极小问题（对偶问题）\theta _D(\alpha ,\beta )=\underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta)\max_{\alpha,\beta:\alpha_i\ge0}\theta _D(\alpha ,\beta )=\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta)可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题： \begin{align*} &\underset{\alpha,\beta}{max}\; \min_{x\in\mathbb{R}^{n}}L(x,\alpha,\beta) \\ \mathbb{s.t.}&\quad\alpha_i\ge0,\quad i=1,2,\cdots,k \end{align*}原始问题和对偶问题的关系定理1若原始问题和对偶问题都有最优值，则 \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) \leqslant L(x,\alpha,\beta) \leqslant \max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) \leqslant \underset{x\in \mathbb{R}^n}{min}\;\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta) ={min}\; f(x)推论：设 $x^{\ast}$ 和 $a^{\ast},β^{\ast}$ 分别是原始问题 $\underset{x\in \mathbb{R}^n}{min}\;\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta) $ 和对偶问题 $\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta) $ 的可行解，并且 $\underset{x\in \mathbb{R}^n}{min}\;\underset{\alpha,\beta:\alpha_i\ge0}{max}L(x,\alpha,\beta) =\underset{\alpha,\beta:\alpha_i\ge0}{max}\; \underset{x\in\mathbb{R}^{n}}{min}\; L(x,\alpha,\beta)$，则 $x^{\ast}$和 $a^{\ast}$,$β^{\ast}$ 分别是原始问题和对偶问题的最优解。 定理2：KKT条件(原始问题与对偶问题的解相等的条件)假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，并且不等式约束$c_i(x)$是严格可行的，则$x^{\ast}$和$a^{\ast},β^{\ast}$分别是原始问题和对偶问题的解的充分必要条件是$x^{\ast},a^{\ast},β^{\ast}$满足下面的Karush-Kuhn-Tucker(KKT)条件：（判断极值、其余项为0） \begin{align*} \nabla_xL(x^*,\alpha^*,\beta^*)&=0\\ \nabla_{\alpha}L(x^*,\alpha^*,\beta^*) &=0 \\ \nabla_{\beta}L(x^*,\alpha^*,\beta^*)&=0 \end{align*}\begin{align*} \alpha_i^*c_i(\boldsymbol{x}^*)&=0,\quad i=1,2,\cdots,k\\ c_i(\boldsymbol{x}^*)&\le0,\quad i=1,2,\cdots,k \\ \alpha_i^*&\ge0,\quad i=1,2,\cdots,k \\ h_j(\boldsymbol{x}^*)&=0,\quad j=1,2,\cdots,l \end{align*} 仿射函数f(x)=A\cdot x+b仿射函数就是一个线性函数，其输入是$n$ 维向量，参数 $A$ 可以是常数，也可以是 $m×n$ 的矩阵，$b$ 可以是常数，也可以是 $m$ 维的列向量，输出是一个 $m $维的列向量。在几何上，仿射函数是一个线性空间到另一个线性空间的变换。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[凸集，凸函数和凸优化]]></title>
    <url>%2F2017%2F08%2F18%2F%E5%87%B8%E9%9B%86%EF%BC%8C%E5%87%B8%E5%87%BD%E6%95%B0%E5%92%8C%E5%87%B8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[凸优化也可以解释为目标函数 $f(x)$ 为凸函数而起约束围成的可行域为一个凸集。 凸集对于集合 $K$ ，$\forall x_1,x_2 \in K$,若 $\alpha x_1 + (1-\alpha)x_2 \in K$,其中$α∈[0,1])$,则 $K$ 为凸集，即集合中任意两点的连线均在凸集中，如在下图所示 有时候需要对某个凸集进行放缩转换等操作，对凸集进行以下操作后，得到的集合依然是凸集 凸集的重叠（intersection）部分任然为凸集 若 $C$ 为凸集，则aC+b = \lbrace ax+b , x \in C, \forall a, b\rbrace也为凸集 对于函数 $f(x)=Ax+b$, 若 $C$ 为凸集，则下面得到的转换也为凸集，注意这里的 $A$ 是矩阵f(C) = \lbrace f(x):x\in C\rbrace而当 $D$ 是一个凸集的时候，下面得到的转换也是凸集f^{-1}(D) = \lbrace x: f(x)\in D\rbrace这两个转换互为逆反关系 常见的凸集有下面这些(下式中 $a,x,b$ 均为向量, $A$ 为矩阵) 点（point）、线（line）、面（plane） norm ball: $\{x:||x||≤r\}$ hyperplane: $\{x:a^Tx=b\}$ halfspace: $\{x:a^Tx≤b\}$ affine space: $\{x:Ax=b\}$ polyhedron: $\{x:Ax&lt;b\}$ ![polyheron的图像](## 凸函数的性质 凸函数有几个非常重要的性质，对于一个凸函数 $f$, 其重要性质 一阶特性（First-order characterization）：f(y) \ge f(x) + \nabla f(x)(y - x) 二阶特性（Second-order characterization）：函数的$∇^2f(x)$是半正定的。 Jensen不等式（Jensen’s inequality）：f(E(x))≤E(f(x))这里的 $E$ 表示的是期望，这是从凸函数拓展到概率论的一个推论，这里不详细展开。 sublevel sets，即集合 $\{x:f(x)≤t\}$ 是一个凸集/凸集，凸函数和凸优化-a587ad27.png) 凸函数凸函数的定义如下 设 $f(x)$ 为定义在 $n$ 维欧氏空间中某个凸集 $S$ 上的函数，若对于任何实数 $α(0&lt;α&lt;1)$ 以及 $S$ 中的任意不同两点 $x$ 和 $y$，均有 f(\alpha x^{(1)}+ (1-\alpha)x^{(2)}) \le \alpha f(x^{(1)}) + (1-\alpha)f(x^{(2)})则称 $f(x)$ 为定义在凸集 $S$ 上的凸函数。假如上面不等式中的 $≤$ 改为 $&lt;$， 则称其为严格凸函数。 判断凸函数根据凸函数的定义来判断一个函数是否为凸函数往往比较困难，这里分别通过一阶条件和二阶条件判断凸函数。 一阶条件设 $f(x)$ 在凸集 $S$上有一阶连续偏导数，则 $f(x)$ 为 $S$ 上的凸函数的充要条件为：对于 任意不同两点 $x^{(1)}$和 $x^{(2)}$，均有 f(x^{(2)}) \ge f(x^{(1)}) + \nabla f(x^{(1)})^T(x^{(2)} - x^{(1)})二阶条件设 $f(x)$ 在凸集 $S$上有二阶连续偏导数，则 $f(x)$ 为 $S$上的凸函数的充要条件为：$f(x)$ 的海塞矩阵 $∇^2f(x)$在 $S$ 上处处半正定(为凹函数的充要条件为处处半负定)。注意：假如海塞矩阵 $∇^2f(x)$在 $S$ 上处处正定，则 $f(x)$ 为严格凸函数，但是反过来不成立。 顺序主子式的定义 海塞矩阵判断凸函数例子 凸函数的性质凸函数有几个非常重要的性质，对于一个凸函数 $f$, 其重要性质 一阶特性（First-order characterization）：f(y) \ge f(x) + \nabla f(x)(y - x) 二阶特性（Second-order characterization）：\nabla^2f(x) \succeq 0这里的 $⪰0$ 表示 Hessian 矩阵是半正定的。 Jensen不等式（Jensen’s inequality）：f(E(x))≤E(f(x))这里的 $E$ 表示的是期望，这是从凸函数拓展到概率论的一个推论，这里不详细展开。 sublevel sets，即集合 $\{x:f(x)≤t\}$ 是一个凸集 常见的凸函数有下面这些 仿射函数( Affine function ): $a^Tx+b$ 二次函数( quadratic function),注意这里的 $Q$ 必须为半正定矩阵: $\frac{1}{2}x^TQx + b^Tx+c(Q \succeq 0)$ 最小平方误差( Least squares loss ): $||y-Ax||_2^2$ (总是凸的，因为 $A^TA$ 总是半正定的) 示性函数（Indicator function）：I_C(X) = \begin{cases} 0&x \in C\\ \infty & x \notin C\end{cases} max function: $f(x) = max \lbrace x_1,…x_n \rbrace$ 范数（Norm）：范数分为向量范数和矩阵范数，任意范数均为凸的，各种范数的定义如下 向量范数0范数：$||x||_0$ 向量中非零元素的个数1范数： $||x||_1 = \sum_{i=1}^n |x_i|$$p$ 范数：$||x||_p = (\sum_{i=1}^nx_i^p)^{1/p}~~(p &gt; 1)$无穷范数: $||x||_{\infty} = max_{i=1,…n} |x_i|$ 矩阵范数核(nuclear)范数: $||X||_{tr} = \sum_{i=1}^{r}\sigma_i(X)$ , ($\sigma_i(X)$是矩阵分解后的奇异值,核范数即为矩阵所有奇异值之和)谱（spectral）范数：$||X||_{op} = max_{i=1,…r}\sigma_i(X)$, 即为最大的奇异值 凸优化对于下面的优化问题 \begin{align*} &\min_x\quad f(x)\\ &\begin{array}\\ s.t.&g_i(x) \le 0,~i=1,\ldots,m\\ &h_j(x)=0,~j=1,\ldots,r \end{array} \end{align*}当 $f(x),gi(x$ 均为凸函数， 而 $h_j(x)$ 为仿射函数（affine function）时，该优化称为凸优化,注意上面的 min 以及约束条件的符号均要符合规定。凸优化也可以解释为目标函数 $f(x)$ 为凸函数而起约束围成的可行域为一个凸集。 常见的一些凸优化问题常见的一些凸优化问题有：线性规划（linear programs），二次规划（quadratic programs），半正定规划（semidefinite programs），且 $LP∈QP∈SDP$, 即后者是包含前者的关系。 线性规划问题一般原型如下($c$为向量，$D,A$为矩阵) \begin{align*} &\min_x\quad c^Tx\\ &\begin{array}\\ s.t.&Dx \le d\\ &Ax=b \end{array} \end{align*} 二次规划问题一般原型如下（要求矩阵 $Q$ 半正定） \begin{align*} &\min_x\quad \frac{1}{2}x^TQx+c^Tx\\ &\begin{array}\\ s.t.&Dx \le d\\ &Ax=b \end{array} \end{align*} 而半正定规划问题一般原型如下(X 在这里表示矩阵) \begin{align*} &\min_X\quad CX\\ &\begin{array}\\ s.t.&A_iX \le b_i, i=1,…m\\ &X \succeq 0 \end{array} \end{align*}]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>凸优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵的基础概念]]></title>
    <url>%2F2017%2F08%2F18%2F%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[零向量变换后落在原点的向量的集合被称为矩阵的‘零空间’或者‘核’。 零向量一定在列空间中 对于一个满秩变换来说，唯一能在变换后落在原点的就是零向量自身 对于一个非满秩的矩阵来说，它将空间压缩到一个更低的维度上，变换后的已给向量落在零向量上，而“零空间”正是这些向量所构成的空间 对角矩阵在方阵中，对角线（从左上到右下）上的值称为对角元素。非对角元素全部为0的矩阵称为对角矩阵。对角矩阵表示的映射是沿着坐标轴伸缩，其中对角元素就是各坐标轴伸缩的倍率 矩阵的秩矩阵的秩，为变换后的空间的维数 单位矩阵方阵中，如果除了对角线（从左上到右下）上的元素为1，其余元素都为0，则该矩阵称为单位矩阵，记为 $I$ 。 $I_{n}$ 表示 $n$ 阶单位矩阵。单位矩阵表示的映射是“什么都不做”的映射。 I_{n}=\begin{bmatrix} 1 & 0 & \cdots & 0\\ 0 & 1 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \end{bmatrix}奇异矩阵行列式为零的矩阵 逆矩阵AA^{-1}=A^{-1}A=I零矩阵所有元素都为0的矩阵称为零矩阵，记为 $O$ 。零矩阵表示的映射是将所有的点都映射到原点的映射 行列式线性变换的行列式即线性变换改变面积的比例。 det(M_1M_2) = det(M_1)det(M_2) 检验一个矩阵的行列式是否为0，就能了解这个矩阵所代表的变换是否将空间压缩到更小的维度上 在三维空间下，行列式可以简单看作这个平行六面体的体积，行列式为0则意味着整个空间被压缩为零体积的东西，也就是一个平面或者一条直线，或者更极端情况下的一个点 特征分解如果说一个向量 $v$ 是方阵 $A$ 的特征向量，将一定可以表示成下面的形式： Av=\lambda v$\lambda$ 为特征向量 $v$ 对应的特征值。特征值分解是将一个矩阵分解为如下形式： A=Q\Sigma Q^{-1}其中， $Q$ 是这个矩阵 $A$ 的特征向量组成的矩阵， $\Sigma$ 是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵A的信息可以由其特征值和特征向量表示。 对于矩阵为高维的情况下，那么这个矩阵就是高维空间下的一个线性变换。可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）。 总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。 奇异值分解特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个 $N \ast M $的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法： 分解形式：假设A是一个 $M*N$ 的矩阵，那么得到的U是一个$M\ast M$ 的方阵（称为左奇异向量），$Σ$ 是一个 $M\ast N$ 的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），$V^T$ ($V$的转置)是一个 $N\ast N$ 的矩阵（称为右奇异向量）。 LU分解给定矩阵 $A$，将$A$表示成下三角矩阵L和上三角矩阵U的乘积，称为LU分解]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>线性代数</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性方程求解]]></title>
    <url>%2F2017%2F08%2F18%2F%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[\begin{align*} 2x+5y+3z &= -3\\ 4x+8z &= 0\\ x+3y &= 2 \end{align*}\Leftrightarrow \overset{A}{\overbrace{\begin{bmatrix} 2 & 5 & 3\\ 4 & 0 & 8\\ 1 & 3 & 0 \end{bmatrix}}}\overset{\overrightarrow{x}}{\overbrace{\begin{bmatrix} x\\ y\\ z \end{bmatrix}}}=\overset{\overrightarrow{v}}{\overbrace{\begin{bmatrix} -3\\ 0\\ 2 \end{bmatrix}}}A\overrightarrow{x}=\overrightarrow{v}可以看做向量 $\overrightarrow{x}$ 经过空间变换后变为向量 $\overrightarrow{v}$ ，矩阵 $A$ 的变换过程有两种情况 空间变换之后，空间没有发生压缩。即$det(A)\neq0$ ，矩阵 $A$ 满秩 空间变换之后，空间发生压缩，即$det(A)=0$ ，矩阵 $A$ 不是满秩 空间变换之后，空间没有发生压缩空间变换之后，空间没有发生压缩。即 $det(A)\neq0$ ，矩阵 $A$ 满秩。 \begin{align*} A\overrightarrow{x}&=\overrightarrow{v} \\ \overrightarrow{x} &= A^{-1}\overrightarrow{v} \end{align*} 空间变换之后，空间发生压缩空间变换之后，空间发生压缩，即 $det(A)=0$ ，矩阵A不是满秩。此时你不能通过矩阵的逆变换将空间从一个低维状态复原到高维状态（即原空间）。 此时方程的解如下，当 $\overrightarrow{v}$ 在压缩的平面上方程有解，此时方程组可能有无穷个解；当 $\overrightarrow{v}$ 不在压缩的平面上方程无解]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>线性代数</tag>
        <tag>矩阵</tag>
        <tag>向量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征值与特征向量]]></title>
    <url>%2F2017%2F08%2F18%2F%E7%89%B9%E5%BE%81%E5%80%BC%E4%B8%8E%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%2F</url>
    <content type="text"><![CDATA[定义特征向量矩阵在空间变换过程中，没有发生旋转的向量。因此特征向量可以看过空间变换的旋转轴。 特征值特征向量在变换中拉伸或压缩比例的因子 计算$\vec{v}$为特征向量，$\lambda $为特征值。求解如下所示： \begin{align*} A\vec{v}&=\lambda \vec{v} \\ A\vec{v}- \lambda \vec{v} &= \vec{0}\\ (A- \lambda I)\vec{v} &= \vec{0} \end{align*}存在一个非零向量 $\vec{v}$ 使得 $(A- \lambda I)\vec{v} = \vec{0}$ 则 det(A- \lambda I)=0求出$\lambda $即特征值，带入特征值$\lambda $解得特征向量$\vec{v}$。 对角矩阵对角矩阵的所有基向量都是特征向量 \begin{bmatrix} -5 & 0 & 0 & 0\\ 0 & -2 & 0 & 0\\ 0 & 0 & -4 & 0\\ 0 & 0 & 0 & 4 \end{bmatrix}对角矩阵的优势 对角矩阵的使用求出特征向量，利用基变换将原空间变换为以特征向量为基的空间，再利用基变换将以特征向量为基的空间变换为原空间]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>线性代数</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正定二次型和正定矩阵]]></title>
    <url>%2F2017%2F08%2F18%2F%E6%AD%A3%E5%AE%9A%E4%BA%8C%E6%AC%A1%E5%9E%8B%E5%92%8C%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[二次型矩阵对于一个方阵$A∈ R^{n×n}$和一个向量$x∈ R^n$，标量$x^TAx$被称作一个二次型。显式地写出来， x^TAx=\sum_{i=1}^{n}x_i(Ax)_i=\sum_{i=1}^{n}x_i\left ( \sum_{j=1}^{n}A_{ij}x_j \right )=\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}x_ix_j上式实际为一个二次多项式例如： \left [ x_1 x_2 \right ] \begin{bmatrix} 1 & 3\\ 2 & 4 \end{bmatrix}\begin{bmatrix} x_1\\ x_2 \end{bmatrix}=[x_1,x_2] \begin{bmatrix} x_1+3x_2\\ 2x_1+4x_2 \end{bmatrix}=x_1^2+5x_1x_2+4x_2^2 分类对于任一个n元实二次型 $f(x_1,x_2,\cdots ,x_n)=X^TAX$，作为一个 $n$ 元二次齐次多项式，我们往往需要考虑它的取值问题，显然当 $x_1=x_2=\cdots=x_n=0$ 时，二次型 $f$ 的值为 $f(0,0,\cdots,0)=0$ ，下面我们根据当 $x_1,x_2,\cdots,x_n$ 取不全为零的 $n$ 个数时，即当 $X$ 为任一个非零列向量时，取值的不同情况，给出以下定义： 定义2.1：设有n元实二次型 $f(x_1,x_2,\cdots ,x_n)=X^TAX$。 如果对于任何非零列向量 $X$ ，都有 $X^TAX&gt;0$，则称为正定二次型，称对称矩阵 $A$ 为正定矩阵。 如果对于任何非零列向量 $X$ ，都有 $X^TAX\geqslant 0$，则称为半正定二次型，称 $A$ 为半正定矩阵。 如果对于任何非零列向量 $X$ ，都有 $X^TAX&lt;0$，则称为负定二次型，称 $A$ 为负定矩阵。 如果对于任何非零列量 $X$ ，都有 $X^TAX\leqslant 0$，则称为半负定二次型，称 $A$ 为半负定矩阵。 其它的实二次型称为不定二次型，其矩阵称为不定矩阵。 例子 正定矩阵性质 正定矩阵的行列式恒为正； 两个正定矩阵的和是正定矩阵； 正实数与正定矩阵的乘积是正定矩阵。正定等价命题 $A$是正定矩阵； $A$的一切顺序主子式均为正； $A$的一切主子式均为正； $A$的特征值均为正； 存在实可逆矩阵$C$，使$A=C’C$； 存在秩为$n$的$m×n$实矩阵$B$，使$A=B’B$； 存在主对角线元素全为正的实三角矩阵$R$，使$A=R’R$ 正定矩阵判别方法 求出A的所有特征值。若A的特征值均为正数，则A是正定的；若A的特征值均为负数，则A为负定的。 计算A的各阶顺序主子式。若A的各阶顺序主子式均大于零，则A是正定的；若A的各阶顺序主子式中，奇数阶主子式为负，偶数阶为正，则A为负定的。 半正定矩阵性质 半正定矩阵的行列式是非负的； 两个半正定矩阵的和是半正定的； 非负实数与半正定矩阵的数乘矩阵是半正定的。半正定等价命题 $A$是是半正定的； $A$的一切顺序主子式均为非负的； $A$的一切主子式均为非负的； $A$的特征值均为正； 存在$n$阶实矩阵$C$，使$A=C’C$； 存在秩为$r$的$r×n$实矩阵$B$，使$A=B’B$]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>线性代数</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行列式]]></title>
    <url>%2F2017%2F08%2F18%2F%E8%A1%8C%E5%88%97%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[行列式行列式在数学中，是一个函数，其定义域为det的矩阵A，取值为一个标量，写作$det(A)$或 $| A |$。对于行列式的几何描述如下所示： 行列式求得是矩阵的面积。 行列式等于0时候，说明矩阵的空间操作把空间压缩为低维空间 行列式为负数的时候，说明空间发生了翻转 方阵特征值之积等于行列式值也可以如下这样理解 行列式的计算]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>线性代数</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性代数基础]]></title>
    <url>%2F2017%2F08%2F17%2F%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[标量，向量，矩阵和张量标量（scalar）：一个标量就是一个单独的数。用斜体表示标量，如 $s∈R$. 向量（vector）：一个向量是一列数，我们用粗体的小写名称表示向量。比如 $x$，将向量x 写成方括号包含的纵柱： {\bf x}= \begin {bmatrix} x_1\\x_2\\ \vdots \\x_n\\ \end{bmatrix}矩阵（matrix）：矩阵是二维数组，我们通常赋予矩阵粗体大写变量名称，比如 $A​$ 。如果一个矩阵高度是 $m​$，宽度是 $n$​，那么说 $\bf A\in \bf R ^{m \times n}​$​ 。一个矩阵可以表示如下： {\bf A}= \begin{bmatrix} x_{11} &x_{12}\\ x_{21} & x_{22}\\ \end{bmatrix}张量（tensor）：某些情况下，我们会讨论不止维坐标的数组。如果一组数组中的元素分布在若干维坐标的规则网络中，就将其称为张量。用 $A$​ 表示，如张量中坐标为 $(i,j,k)$ ​的元素记作 $A_{i,j,k}$ ​。转置（transpose）：矩阵的转置是以对角线为轴的镜像，这条从左上角到右下角的对角线称为主对角线（main diagonal）。将矩阵 $A$ 的转置表示为 $A^⊤$ 。定义如下： ({\bf A^\top})_{i,j}=\bf A_{j,i}{\bf A} = \begin{bmatrix} x_{11} &x_{12}\\ x_{21} & x_{22}\\ x_{31} & x_{32} \end{bmatrix} \implies {\bf A^\top}= \begin{bmatrix} x_{11} &x_{21}&x_{31} \\ x_{21} & x_{22}& x_{32} \end{bmatrix} 向量运算加法设 $\overrightarrow{v}=\begin{bmatrix} 1 \\ 2 \end{bmatrix}$，$\overrightarrow{w}=\begin{bmatrix}3\\ -1\end{bmatrix}$则$ \overrightarrow{v}+\overrightarrow{w}=\begin{bmatrix}4\\ 1\end{bmatrix}$ 数乘设 $\overrightarrow{v}=\begin{bmatrix} 3 \\ 1 \end{bmatrix}$，则$2\overrightarrow{v}=\begin{bmatrix} 3\times 2 \\ 1\times 2 \end{bmatrix}=\begin{bmatrix} 6 \\ 2 \end{bmatrix}$ 矩阵-向量的乘积矩阵是空间的线性变换，矩阵与向量相乘便是将向量进行线性变换的结果。 A\overrightarrow{v}=\begin{bmatrix} \overset{i}{1} & \overset{j}{3}\\ -2 & 0 \end{bmatrix}\begin{bmatrix} -1\\ 2 \end{bmatrix}=\begin{bmatrix} -1\times 1+2\times 3\\ -1\times -2+2\times 0 \end{bmatrix}=\begin{bmatrix} 5\\ 2 \end{bmatrix} 向量-向量的乘积向量可以看成一个矩阵，这个矩阵将空间压缩到了一维空间。 \overrightarrow{w}\cdot \overrightarrow{v}=|\overrightarrow{w}|| \overrightarrow{v}|cos\theta =\begin{bmatrix} 1\\ -2 \end{bmatrix}\cdot\begin{bmatrix} 4\\ 3 \end{bmatrix}=4\times 1+3\times (-2)=-2向量$\overrightarrow{v}$在向量$\overrightarrow{w}$上的投影乘以向量$\overrightarrow{w}$的长度 向量的叉积向量的叉积是一个向量，向量 $\overrightarrow{p}$ 的方向垂直于 $\overrightarrow{w}$ 与 $\overrightarrow{v}$ 的平面，向量 $\overrightarrow{p}$ 的模是 $\overrightarrow{w}$ 与 $\overrightarrow{v}$ 所围成平面的面积(行列式)。 \overrightarrow{w}\times \overrightarrow{v}=|\overrightarrow{w}|| \overrightarrow{v}|sin\theta =\overrightarrow{p} 高维情况$\overrightarrow{w}\times \overrightarrow{v}=\overrightarrow{p}$则 \begin{align*} p_1 &=v_2\cdot w_3-v_3\cdot w_2 \\ p_2 &=v_3\cdot w_1-v_1\cdot w_3 \\ p_3 &=v_1\cdot w_2-v_2\cdot w_1 \end{align*} 矩阵的运算对向量的线性空间变换 矩阵加减法对应位置的元素相加相减 矩阵数乘 矩阵转置把矩阵A的行和列互相交换所产生的矩阵称为A的转置矩阵 矩阵共轭 矩阵共轭转置 矩阵-向量的乘积矩阵是空间的线性变换，矩阵与向量相乘便是将向量进行线性变换的结果。 A\overrightarrow{v}=\begin{bmatrix} \overset{i}{1} & \overset{j}{3}\\ -2 & 0 \end{bmatrix}\begin{bmatrix} -1\\ 2 \end{bmatrix}=\begin{bmatrix} -1\times 1+2\times 3\\ -1\times -2+2\times 0 \end{bmatrix}=\begin{bmatrix} 5\\ 2 \end{bmatrix} 矩阵-矩阵的乘积矩阵是一种线性变换，矩阵与矩阵相乘便是将空间进行线性变换之后再次进行线性变换。矩阵与矩阵的乘积是一种对于空间的复合线性变换。变换吸纳从右侧开始一次向左侧进行变换。线性变换的过程如下图所示： 矩阵-矩阵的计算过程如下所示\begin{bmatrix} 0 & 2\\ 1 & 0 \end{bmatrix}\begin{bmatrix} 1 & -2\\ 1 & 0 \end{bmatrix}=\left\{\begin{matrix}1\times \begin{bmatrix}0\\ 1\end{bmatrix}+1\times \begin{bmatrix}2\\ 0\end{bmatrix} \\-2\times \begin{bmatrix}0\\ 1\end{bmatrix}+0\times \begin{bmatrix}2\\ 0\end{bmatrix}\end{matrix}\right.=\begin{bmatrix} 2 &0 \\ 1 & -2 \end{bmatrix}计算过程去下图所示 矩阵-矩阵乘积的性质 结合律即 $(AB)C = A(BC)$ 分配率即 $A(B + C) = AB + AC$ 注意哦，矩阵乘法没有交换律，即 $AB ≠BA$ 矩阵的逆矩阵$A ∈ R^{n×n}$的逆,写作$A^{−1}$，是一个矩阵，并且是唯一的。是对矩阵$A$空间操作的逆变换。同时$AA^{−1}$也可以理解为矩阵$A$除以矩阵$A$ 等于单位矩阵$I$，A^{−1}A = I = AA^{−1}. 注意不是所有的矩阵都有逆。例如非方阵，是没有逆的。然而，即便对于一些方阵，它仍有可能不存在逆。如果$A^{−1}$存在，我们称矩阵$A$ 是可逆的或非奇异的，如果不存在，则称矩阵$A$不可逆或奇异。如果一个方阵$A$有逆$A^{−1}$，它必须满秩 矩阵的秩矩阵的秩实际上的经过矩阵操作后空间的维数。在矩阵A中，非零子式的最高阶数称为矩阵A的秩，记为或秩。规定零矩阵的秩为零。 求解思路$矩阵A\xrightarrow{初等行变换}阶梯形矩阵B。$则$r(A)=r(B)=$($B$中非零行的行数) 例子-求矩阵A=\begin{bmatrix} 2 &-3 &8 & 2\\ 2 &12 &-2 & 12\\ 1 &3 & 1 & 4 \end{bmatrix}的秩。 解： A= \begin{bmatrix} 2 &-3 &8 & 2\\ 2 &12 &-2 & 12\\ 1 &3 & 1 & 4 \end{bmatrix}\rightarrow \begin{bmatrix} 1 &3 &1 & 4\\ 2 &-3 &8 & 2\\ 2 &12 &-2 & 12 \end{bmatrix}\rightarrow \begin{bmatrix} 1 &3 &1 & 4\\ 0 &-9 &6 & -6\\ 0 &6 &-6 & 4 \end{bmatrix}\rightarrow \begin{bmatrix} 1 &3 &1 & 4\\ 0 &3 &-2 &2\\ 0 &0 &-0 & 0 \end{bmatrix}因此$r(A)=2$ 矩阵的迹方阵$A ∈ R^{n×n}$的迹，记作$tr(A)$，或可以省略括号表示成$trA$，是矩阵的对角线元素之和:trA=\sum_{i=1}^{n}A_{ii} 性质 对于$A ∈ R^{n×n}， trA = trA^T .$ 对于$A，B ∈ R^{n×n}， tr(A + B) = trA + trB.$ 对于$A ∈ R^{n×n}， t ∈ R， tr(t\ast A) = t\ast trA.$1.对于方阵 $A,B,C，trABC = trBCA = trCAB$，即使有更多的矩阵相乘，这个性质也不变. 正交矩阵如果$x^Ty = 0$，则两个向量 $x，y ∈ R^n$是正交的。对于一个向量$x ∈ R^n$，如果 $|x|=1$ 则是 $x$ 归一化的。对于一个方阵$U ∈ R^{n×n}$，如果所有列都是彼此正交和归一化的，（列就称为标准正交）则这个方阵是正交的（注意在讨论向量或矩阵时，正交具有不同的含义）。 根据正交和归一化的定义可得： U^T U = I = UU^T一个正交矩阵的逆矩阵的是它转置,正交矩阵必须是方阵]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>线性代数</tag>
        <tag>矩阵</tag>
        <tag>向量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵的逆]]></title>
    <url>%2F2017%2F08%2F17%2F%E7%9F%A9%E9%98%B5%E7%9A%84%E9%80%86%2F</url>
    <content type="text"><![CDATA[定义设A是数域上的一个n阶方阵，若在相同数域上存在另一个n阶矩阵$A^{-1}$，使得： $AA^{-1}=A^{-1}A=I$。 则我们称$A^{-1}$是A的逆矩阵，而A则被称为可逆矩阵。注：单位矩阵 I_{n}=\begin{bmatrix} 1 & 0 & \cdots & 0\\ 0 & 1 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \end{bmatrix} 求解A^{-1}=\frac{1}{|A|}A^{*}设矩阵$A$为 A=\begin{bmatrix} A_{11} & A_{12} & A_{13}\\ A_{21} & A_{22} & A_{23}\\ A_{31} & A_{32} & A_{33} \end{bmatrix}伴随矩阵$A^{*}$代数余子式 $M_{ij}$那么第i行，第j列的代数余子式为去掉A第i行，第j列之后的矩阵的行列式，记为$M_{ij}$那上面的矩阵A为例子，那么M_{11}=\begin{vmatrix} A_{22} & A_{23}\\ A_{32} & A_{33} \end{vmatrix} 代数余子式那么代数余子式为$C_{ij}=(-1)^{i+j}M_{ij}$ 伴随矩阵矩阵A的伴随矩阵是A的余子矩阵的转置矩阵 A^{*}=\begin{bmatrix} C_{11} & C_{12} & C_{13}\\ C_{21} & C_{22} & C_{23}\\ C_{31} & C_{32} & C_{33} \end{bmatrix}^{T} \\\begin{bmatrix} (-1)^{(1+1)}\begin{vmatrix} A_{22} & A_{23}\\ A_{32} & A_{33} \end{vmatrix} & (-1)^{(1+2)}\begin{vmatrix} A_{21} & A_{23}\\ A_{31} & A_{33} \end{vmatrix} & (-1)^{(1+3)}\begin{vmatrix} A_{21} & A_{22}\\ A_{31} & A_{32} \end{vmatrix}\\ (-1)^{(2+1)}\begin{vmatrix} A_{12} & A_{13}\\ A_{32} & A_{33} \end{vmatrix} & (-1)^{(2+2)}\begin{vmatrix} A_{11} & A_{13}\\ A_{31} & A_{33} \end{vmatrix} & (-1)^{(2+3)}\begin{vmatrix} A_{11} & A_{12}\\ A_{31} & A_{32} \end{vmatrix}\\ (-1)^{(3+1)}\begin{vmatrix} A_{12} & A_{13}\\ A_{21} & A_{23} \end{vmatrix} & (-1)^{(3+2)}\begin{vmatrix} A_{11} & A_{13}\\ A_{21} & A_{23} \end{vmatrix} & (-1)^{(3+3)}\begin{vmatrix} A_{11} & A_{12}\\ A_{21} & A_{22} \end{vmatrix} \end{bmatrix}^{T}求解行列式$|A|$det(A)=|A|=A_{11}det\left (\begin{bmatrix} A_{22} & A_{23}\\ A_{32} & A_{33} \end{bmatrix}\right )-A_{12}det\left (\begin{bmatrix} A_{21} & A_{23}\\ A_{31} & A_{33} \end{bmatrix}\right )+A_{13}det\left (\begin{bmatrix} A_{21} & A_{22}\\ A_{31} & A_{32} \end{bmatrix}\right )]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>线性代数</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机变量的统计特征]]></title>
    <url>%2F2017%2F08%2F16%2F%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[随机变量的统计特征主要包括期望，方差，协方差以及相关系数。 期望 离散型随机变量：E(X) = \sum_{k=1}^{ +\infty}p_kx_k 连续型随机变量：E(X) = \int_{-\infty}^{ +\infty} {xf(x)dx} 期望有以下性质(C为常数,其他均为随机变量): $E(C)=C$ $E(CX)=CE(X)$ $E(X+Y)=E(X)+E(Y)$ $E(XY)=E(X)E(Y)$ （$X,Y$ 相互独立） 随机变量 $X$ 的函数的期望前面讨论随机变量的分布函数时，同时讨论了随机变量的函数的分布函数，这里同样对于随机变量 $X$ 的函数的期望进行讨论，其定义及求法如下所示。 设 $Y$ 是随机变量 $X$ 的函数：$Y=g(X)$ ( $g$ 是连续函数) 如果 $X$ 是离散型随机变量，它的分布律为P(X=x_k) = p_k, k = 1,2,…若 $\sum_{k=1}^{\infty}g(x_k)p_k$ 绝对收敛，则有E(Y) = E[g(X)] = \sum_{k=1}^{\infty}g(x_k)p_k 如果 $X$ 是连续型随机变量，它的概率密度函数为 $f(x)$, 若 $\int_{-\infty}^{\infty}g(x)f(x)dx$ 绝对收敛，则有E(Y) = E[g(X)] = \int_{-\infty}^{\infty}g(x)f(x)dx 这个定理的重要意义在于求 $E(Y)$ 的时候，不用再求 $Y$ 的分布律或概率密度函数，直接利用 $X$ 的分布律或概率密度函数即可。 方差方差的原始定义为 D(X) = E[(X-E(X))^2] = E(X^2) - E(X)^2方差有以下性质： $D(C)=0$ $D(CX)=C^2D(X)$ $D(X+Y)=D(X)+D(Y)+2E([X−E(X)][Y−E(Y)])$如果 $X，Y$ 是相互独立的，那么 $E([X−E(X)][Y−E(Y)])=0$ , 当这一项不为0的时候，称作变量 $X,Y$ 的协方差。 常见分布的期望和方差前面我们提到了若干种典型的离散分布和连续分布，下面是这几种分布的期望和方差，记住这些常用的期望和方差能够在使用的时候省去推导过程。 分布类型 概率密度函数 期望 方差 伯努利分布~$B(1,p)$ $p = p^x(1-p)^{1-x}$ $p$ $p(1−p)$ 二项分布~$B(n,p)$ $p_i = C_n^i p^i(1-p)^{n-i}(i=1,2)$ $np$ $np(1−p)$ 泊松分布~$P(λ)$ $p_i = \frac{\lambda^ki e^{-\lambda}}{i!}(i = 1,2,…)$ $λ$ $λ$ 均匀分布~$U(a,b)$ $f(x) = \frac{1}{b-a}$ $\frac{a+b}{2}$ $\frac{(b-a)^2}{12}$ 正态分布~$N(μ,σ^2)$ $f(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ $μ$ $σ^2$ 指数分布~$E(λ)$ $f(x) = \begin{cases} \lambda e^{-x\lambda} &amp;{x&gt;0} \\ 0&amp;{其他}\end{cases}$ $\frac{1}{\lambda}$ $\frac{1}{\lambda^2}$ 切比雪夫不等式切比雪夫不等式的定义如下： 设随机变量 $X$ 具有数学期望 $E(X)=μ$, 方差 $D(X)=σ^2$, 则对于任意正数 $ϵ$, 下面的不等式成立 P(|X-\mu|\ge \epsilon) \le \frac{\sigma^2}{\epsilon^2}从定义可知，切比雪夫不等式也可写成如下的形式： P(|X-\mu| \le \epsilon) \ge 1 - \frac{\sigma^2}{\epsilon^2}切比雪夫不等式的一个重要意义在于当随机变量 $X$ 的分布未知，只知道 $E(X)$ 和 $D(X)$ 的情况下，对于事件 $(|X−μ|≤ϵ)$ 概率的下限的估计。 协方差协方差表达了两个随机变量的相关性，正的协方差表达了正相关性，负的协方差表达了负相关性。协方差为0 表示两者不相关，对于同样的两个随机变量来说，计算出的协方差的绝对值越大，相关性越强。 协方差的定义入下: Cov(X,Y)=E[X−E(X)][Y−E(Y)]协方差有以下性质： $Cov(X,Y)=Cov(Y,X)$ $Cov(X,Y)=E(XY)−E(X)E(Y)$ $Cov(aX,bY)=abCov(X,Y)$（a，b是常数） $Cov(X_1+X_2, Y) = Cov(X_1, Y) + Cov(X_2,Y)$ 相关系数假如我们现在有身高和体重这两个未知变量，对于一系列的样本我们算出的的协方差为30，那这究竟是多大的一个量呢？如果我们又发现，身高与鞋号的协方差为5，是否说明，相对于鞋号，身高与体重的的相关性更强呢？ 为了能进行这样的横向对比，我们计算相关系数(correlation coefficient)， 相关系数相当于是“归一化”的协方差。 \rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)D(Y)}}相关系数是用协方差除以两个随机变量的标准差。相关系数的大小在-1和1之间变化，等于0表示不相关。再也不会出现因为计量单位变化，而数值变化较大的情况，而相关系数的大小的含义与协方差是一样的。 需要注意的是上面提到的相关均指线性相关，$X,Y$ 不相关是指 $X,Y$ 之间不存在线性关系，但是他们还可能存在除线性关系以外的关系。因此，有以下结论: $X,Y$ 相互独立则 $X,Y$ 一定不相关；反之 $X,Y$ 不相关，两者不一定相互独立。 矩和协方差矩阵矩下面介绍概率论中几种矩的定义 设 $X,Y$ 为随机变量,则 $E(X^k), k=1,2,3….$ 称为 $X$ 的 $k$ 阶原点矩，简称 $k$ 阶矩 $E((X-E[X])^k), k=1,2,3….$ 称为 $X$ 的 $k$ 阶中心距 $E(X^kY^l),k,l=1,2,…$ 称为 $X$ 和 $Y$ 的 $k+l$ 阶混合矩 $E((X-E[X])^k(Y-E[Y])^l)),k,l=1,2,…$称为 $X$ 和 $Y$ 的 $k+l$ 阶混合中心矩 由以上定义我们可以知道，随机变量的期望是其一阶原点矩，方差是其二阶中心距，协方差是其二阶混合中心矩。 协方差矩阵除此之外，另外一个常用的概念是协方差矩阵， 其定义如下： 对于 $n$ 维随机变量 ($(X_1,X_2,X_3…,X_n)$) 构成的矩阵 C= \begin{bmatrix} c_{11} & c_{12} & \cdots & c_{1n} \\ c_{21} & c_{22} & \cdots & c_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ c_{n1} & c_{n2} & \cdots & c_{nn} \\ \end{bmatrix}其中各个元素为 c_{ij} = Cov(X_i,X_j) = E((X_i - E[X_i])(X_j - E[X_j]))，i,j=1,2,3..n则称矩阵 $C$ 为协方差矩阵，由于 $c_{ij} = c_{ji}$ ， 因此上面的矩阵为一个对称矩阵。 协方差矩阵其实是将二维随机变量的协方差一般化后拓展到了 $n$ 维随机变量上的一种表示形式，但是除了作为一种表示形式以外，协方差矩阵还存在着某些性质使得其在多个领域均有应用，如主成成分分析。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>概率</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二维随机变量]]></title>
    <url>%2F2017%2F08%2F15%2F%E4%BA%8C%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[联合分布函数假设 $X$ 和 $Y$ 都是随机变量，那么我们定义其分布函数如下： F(x,y) = P ((X \le x)\cap(Y \le y)) = P (X \le x, Y \le y )上面的 $F(x,y)$ 称作随机变量$(X,Y)$的分布函数，也叫作联合分布函数。 离散型随机变量联合分布如果上面的 $X$ 和 $Y$ 都是离散随机变量，那么对于 $(X,Y)$ 的所有取值可记为 P(X=x_i, Y=y_i) = p_{ij},i,j=1,2,….上面的所有P的取值为二维离散随机变量的分布律，也叫联合分布律。直观用表格表示如下所示 连续型随机变量联合分布类似地，如果上面的X和Y都是连续随机变量，那么分布函数可定义为 F(x,y) = \int_{-\infty}^y\int_{-\infty}^xf(u,v)dudv其中 $f(x,y)$ 被称为概率密度函数，也叫联合概率密度函数。 其性质与一维随机变量的概率密度函数非常相似 $f(x,y)≥0$ $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y)dxdy = F(\infty,\infty)$ 设 $G$ 是 $xOy$ 平面上的区域，点 $(X,Y)$ 落在$G$内的概率为\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y)dxdy = F(\infty,\infty) 若 $f(x,y)$ 在点 $(x,y)$ 连续，则\frac{\partial^2F(X,Y)}{\partial x \partial y} = f(x, y) 边缘分布函数二维随机变量 $(X,Y)$ 作为一个整体的时候，其分布函数为联合分布函数，但是 $X$ 和 $Y$ 是随机变量，各自也有分布函数，将其分别记为 $F_X(x),F_Y(y)$，称为随机变量 $(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘分布函数。 边缘分布函数可通过联合分布函数确定，关系如下 F_X(x) = P(X \le x) = P(X \le x,Y \lt \infty) = F(x, \infty)F_X(x)=F(x,∞)\\ F_Y(y)=F(∞,y)离散型随机变量边缘分布假如 $X$ 和 $Y$ 是离散型随机变量，那么随机变量 $(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘分布定义下 p_{i.} = \sum_{j=1}^{\infty} p_{ij} = P(X = x_i), i=1,2,3…..n \\ p_{.j} = \sum_{i=1}^{\infty} p_{ij} = P(Y = y_j), j=1,2,3…..n上面的式子分别称为随机变量 $(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘分布率。 连续型随机变量边缘分布假如 $X$ 和 $Y$ 分别是连续性随机变量，那么随机变量 $(X,Y)$ 关于 $X$ 的边缘分布函数定义为 F_X(x) = F(x,\infty) = \int_{-\infty}^{x}(\int_{-\infty}^{\infty}f(x,y)dy)dx=\int_{-\infty}^{\infty}f(x,y)dy则被称为随机变量 $(X,Y)$ 关于 $Y$ 的 边缘概率密度函数 条件分布由条件概率可以比较容易推导出条件分布的含义，其定义如下： 离散型随机变量的条件分布对于离散型随机变量，条件分布的定义如下： 设 $(X,Y)$ 是二维离散型随机变量，对于固定的 $j$，若 $P(Y=y_j)&gt;0$, 则称 P(X = x_i|Y= y_j) = \frac{P(X = x_i, Y=y_j)}{P(Y=y_j)} = \frac{p_{ij}}{p_{.j}}, i = 1,2,3为在 $Y=y_j$ 条件下随机变量X的条件分布律。同理，交换 $X$ 和 $Y$ 的位置得到的是在 $X=x_i$ 条件下随机变量 $Y$ 的条件分布律。 连续型随机变量的条件分布对于连续型的随机变量，条件分布的定义如下： 设二维随机变量 $(X,Y)$ 的概率密度函数为 $f(x,y),(X,Y)$ 关于 $Y$ 的边缘概率密度为 $f_Y(y)$ .若对于固定的 $y，f_Y(y)&gt;0$ ，则称 $\frac{f(x,y)}{f_Y(y)}$ 为在 $Y=y$ 的条件下 $X$ 的条件概率密度。记为 f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)}有了条件概率密度(就是条件概率密度函数)，我们也可以定义出条件分布函数如下 \int_{-\infty}^x f_{X|Y}(x|y)dx = \int_{-\infty}^x \frac{f(x,y)}{f_Y(y)}dx上面的函数为在 $Y=y$ 的条件下 $X$ 的条件分布函数，记为 F_{X|Y}(x|y) = P(X \le x| Y=y)相互独立的随机变量两个随机变量 $X,Y$ 相互独立的充要条件如下： F(x,y) = F_X(x)F_Y(y)上面的 $F(x,y),F_X(x),F_Y(y)$ 分别是二维随机变量的联合分布函数及关于 $X$ 和 $Y$ 的边缘分布函数。 除了通过分布函数，对于具体的连续型随机变量或离散型随机变量，还可通过概率密度函数和分布律来定义相互独立的条件。 对于连续型随机变量，上面的式子等价于 f(x,y) = f_X(x)f_Y(y)式子中的 $f(x,y),f_X(x),f_Y(y)$ 分别为 随机变量 $(X,Y)$ 的条件概率密度函数和边缘概率密度函数。 对于离散型随机变量则有： P(X = x_i, Y = y_j) = P(X=x_i)P(Y=y_j)二维随机变量的函数的分布在讨论一维随机变量的分布函数的时候，也讨论了一维随机变量的函数的分布函数，同样对于二维随机变量，我们也可以讨论其函数的分布函数。下面主要讨论 $Z=X+Y，Z=XY，Z=Y/X，M=max(X,Y)，N=min(X,Y)$ 这几个函数的分布函数（X，Y 为相互独立的随机变量），这里主要给出具体的公式，证明省略。 $Z=X+Y$ 的分布设 $(X,Y)$ 是二维连续型随机变量，其概率密度函数为 $f(x,y)， Z=X+Y$ 仍然为连续性随机变量，其概率密度函数为 f_{X+Y}(z) = \int_{-\infty}^{\infty} f(z-y,y)dy或 f_{X+Y}(z) = \int_{-\infty}^{\infty} f(x,z-x)dx当 $X,Y$ 相互独立时，其边缘概率密度函数具有以下性质 f(x,y) = f_X(x)f_Y(y)因此上面的式子也可以化成下面的形式 f_{X+Y}(z) = \int_{-\infty}^{\infty} f_X(z-y)f_Y(y)dy\\ f_{X+Y}(z) = \int_{-\infty}^{\infty} f_X(x)f_Y(z-x)dx$Z=XY$ 和 $Z=Y/X$ 的分布设 $(X,Y)$ 是二维连续型随机变量，其概率密度函数为 $f(x,y)$， $Z = \frac{Y}{X},Z = XY$仍然为连续性随机变量，其概率密度函数为 f_{Y/X}(z) = \int_{-\infty}^{\infty} |x|f(x,xz)dx\\ f_{XY}(z) = \int_{-\infty}^{\infty} \frac{1}{|x|}f(x,z/x)dx当 $X,Y$ 相互独立时，同样有下面的性质 f_{Y/X}(z) = \int_{-\infty}^{\infty} |x|f_X(x)f_Y(xz)dx\\ f_{XY}(z) = \int_{-\infty}^{\infty} \frac{1}{|x|}f_X(x)f_Y(z/x)dx$M=max(X,Y)$ 和 $N=min(X,Y)$ 的分布讨论 $max(X,Y)$ 和 $min(X,Y)$ 的分布的时候， 一般假设 $X,Y$ 相互独立，因为这样才有下面的性质。 对于 $M=max(X,Y)$ 的分布有 F_{max}(z) = P(M \le z) = P(X \le z, Y \le z) = P(X \le z)P(Y \le z)由于 $X$ 和 $Y$ 相互独立，因此有 F_{max}(z) = F_X(z)F_Y(z)同样对 $N=min(X,Y)$ 有 F_{min}(z) = P(N \le z) = 1 - P(N \gt z) = 1 - P(X > z)P(Y>z)即 F_{min}(z) = 1 - (1 - F_X(z))(1 - F_Y(z))推广到 n 个相互独立的随机变量有下面的性质 $M = max \lbrace X_1,X_2…,X_n \rbrace$ 及 $N = min\lbrace X_1,X_2…,X_n \rbrace$ 的分布函数分别为 F_{max}(z) = F_{X_1}(z)F_{X_2}(z)…F_{X_n}(z) \\ F_{min}(z) = 1 - (1 - F_{X_1}(z))(1 - F_{X_2}(z))…(1 - F_{X_n}(z))而当 $X_1,X_2…,X_n$ 独立同分布的时候，上式变为如下所示 F_{max}(z) = [F(z)]^n\\ F_{min}(z) = 1 - (1 - F(z))^n]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>概率</tag>
        <tag>分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一维随机变量]]></title>
    <url>%2F2017%2F08%2F15%2F%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[本文主要讲述三种离散型随机变量的分布(伯努利分布,二项分布,泊松分布)和三种连续型随机变量的分布(均匀分布,指数分布,正态分布)。 离散型随机变量的分布伯努利分布伯努利分布又名两点分布或者0-1分布，只能取两种结果，一般记为0或1。设取1的概率为$p$，其分布规律为 P(X=k) = p^k(1-p)^{1-k}, k = 0,1 (1]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>概率</tag>
        <tag>分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[先验概率与后验概率]]></title>
    <url>%2F2017%2F08%2F15%2F%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%8E%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[本文主要讲述先验概率，后验概率，共轭分布和共轭先验这几个概念。 众所周知，概率论中有两大学派：频率学派和贝叶斯学派。先验概率，后验概率，共轭分布和共轭先验是贝叶斯学派中的几个概念。原因是贝叶斯学派认为分布存在先验分布和后验分布的不同，而频率学派则认为一个事件的概率只有一个。 下面先以一个直观的例子来说明先验概率和后验概率的概念 比如说，你来到一个山洞,这个山洞里可能有熊也可能没有熊, 记你觉得山洞有熊的为事件 $Y$. 然后,你也许听到山洞里传来熊的吼声, 记听到熊吼声为事件 $X$. 你一开始认为山洞有熊的概率是 $P(Y)$; 听到熊的吼声之后,你认为有熊的概率是 $P(Y|X)$。在这里，$P(Y)$ 就是先验概率, $P(Y|X)$ 是后验概率. 回到概率论中一个经典的例子:抛硬币。抛硬币时抛出正面的概率为多大？假如事前关于这枚硬币没有任何额外信息，那么一般都会认为是 1/2，这时候的 1/2 就是正面朝上的先验概率 。但是在经过一系列实验确认后再得到的正面朝上的概率很可能就不是1/2了(受到到硬币的质地，重量分布等因素的影响)，这个概率便是后验概率。 简单理解就是在事件发生之前，根据以往的经验推测的与该事件相关的概率就是先验概率，而在事件(试验)真正发生后，通过事件(试验)的结果可以修正先验概率，从而得到后验概率。 贝叶斯学派那么对于抛硬币这个事件来说，抛出正面硬币的概率就应该是一个概率的概率，也就是说它的结果不是一个单一的值 1/2，而是一个概率分布，可能有很高的概率是1/2，但是也有一定的概率是100%（比如抛100次结果还真都100次都是正面）。那么在这里这个概率的分布用函数来表示就是一个似然函数，所以似然函数也被称为“分布的分布”。用公式来表示就是： 后验概率（posterior probability）$∝$ 似然函数（likelyhood function）*先验概率（prior probability） 即： P(θ|X)∝P(θ|X)∗P(θ)这里 $X$ 表示一组观测实验(比如我扔了五次硬币得到5次正反面的结果)，$θ$ 表示随机函数里面的参数（在这里就是硬币掷为正面的概率）。 注意这里是正比于而不是等于，这个是理解似然函数的一个关键，右侧直接的乘积其实是不满足概率分布归一化的条件的（就是右侧的积分最后不会等于1）那么这个正比符号怎样才能变成等号呢？其实只要再除以一个系数进行归一化就可以了： P(θ|x) = ( P(x|θ) * P(θ) ) / P(x) $P(θ|x)$ 是后验概率，一般是我们求解的目标。 $P(x|θ)$ 是条件概率，又叫似然概率，一般是通过历史数据统计得到。一般不把它叫做先验概率，但从定义上也符合先验定义。 $P(θ)$ 是先验概率，一般都是人主观给出的。贝叶斯中的先验概率一般特指它。 $P(x)$ 其实也是先验概率，只是在贝叶斯的很多应用中不重要（因为只要最大后验不求绝对值），需要时往往用全概率公式计算得到。 频率学派频率学派认为每个事件的概率是一个客观存在的常数值，只是我们不知道而已。比如抛硬币，在实验估计之前我们不知道它是多少，频率学派也不会管之前大家说抛硬币出现正面的概率是1/2还是多少，所谓“眼见为实，耳听为虚”，他们的最终结论只和在实验中观测到的数据有关系。但是它肯定是一个确定的常数，然后我们通过观察实验，获得一组样本值 $D$，再将这组样本值代入似然函数 $P(D|X)$ ，求解使得似然函数最大的值就是估计出来的（当然由于实验的结果不同，这个估计出来的也很可能不是1/2，实验不同得到的结果也不同，但是根据大数定律，理论上实验次数足够多以后，求出来的是会越来越接近真实的概率的）。也就是说频率学派认为答案只有一个，我们不断地通过各种估计法来猜测这个值。 而贝叶斯学派并不会完全拒绝大家之前所说的“硬币扔出正面的概率是1/2”的说法，只是贝叶斯学派认为最终硬币扔出正面反面的概率并不是一个常数值，不是一个有唯一答案的真理，这个值本身应该也是一个随机变量，是在不断变化的一个数值，如何得到这个值，贝叶斯学派认为也需要通过实验在“硬币扔出正面的概率是1/2”的说法（先验概率）的基础上通过实验数据（似然函数）不断去预估这个扔出正面概率的实际分布（后验分布）。 举例说明举个例子：假如我扔了5次硬币，先出现了3次正面，后出现了两次反面，那么这时的似然函数就应该是 $P(X|θ)=L(θ)=θ∗θ∗θ∗(1−θ)∗(1−θ)$ ($θ$ 是硬币抛正面的概率，在似然函数里就相当于概率分布函数里的随机变量一样变成一个随机变化的值了） 如果用我们以前统计课本上的频率学派的最大似然估计法，对$L(θ)$求导求最大值，得到 $θ=3/5$， 那么得出结论就是最后抛硬币为正面的概率就是 $3/5$，当然还要附上一个参数估计的置信度，表示这个结论自然不是100%准确的 但是如果采用贝叶斯学派的后验概率$P(θ|X)=P(X|θ)∗P(θ)/P(X)=L(θ)∗P(θ)/P(X)$， 其中 $P(X)$ 可以简单地由古典概型算出来：$P(X)=1/=1/32=0.03125$。如果 $θ$ 取了 $3/5$，代入上式那么抛硬币为正面的概率就是 $0.60.60.60.40.4*0.5/0.03125=0.55296$其中先验概率$P(θ)=0.5$，而不是1/2，当然贝叶斯学派最终得到的后验概率是一个随 $θ$ 变化的分布，只不过在这种情况这个分布取到 0.55296 这个值的概率最大而已 清楚似然函数、先验概率、后验概率的几个贝叶斯学派的基本概念，要明白共轭分布和共轭先验就很简单了，所谓共轭分布就是先验概率和后验概率具有一样函数形式的分布形式， 那么共轭先验又是什么概念呢？因为在现实建模问题中，往往我们先得到和固定的反而是似然函数（其实也很好理解，客观的实验观察数据才是第一手最solid的材料），这时先验函数（可以理解为先验知识或者是对后验分布的一种假设和猜测）是可以选择的。这时如果我选的先验分布最后乘上这个似然函数，使得后验分布与先验分布共轭，那么我们就称这个先验函数为似然函数的共轭先验。基于上面说到的共轭分布的好处，往往选择先验函数时都会让先验概率分布和后验概率分布共轭。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>概率</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[样本以及抽样分布]]></title>
    <url>%2F2017%2F08%2F15%2F%E6%A0%B7%E6%9C%AC%E4%BB%A5%E5%8F%8A%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[概率论与数理统计的主要区别为，在概率论中所研究的随机变量，其分布都是假设已知的，在这一前提下去研究它的性质（数字特征，分布函数等）；而在数理统计中研究的随机变量其分布是未知的，通过对所研究的随机变量进行重复独立的试验和观察，得到许多观察值，再对观察值进行分析，从而对所研究的随机变量的分布做出各种推断 因此数理统计的主要内容包括两部分，一是如何收集，整理数据资料，二是如何对得到的数据资料进行分析和研究，从而对所研究的对象的性质和特点做出推断。第二部分其实就是统计推断的问题，也是后面主要讲述的内容。本文主要讲述数理统计中的两个基本概念：样本和抽样分布。 样本从前面可知，数理统计就是通过数据来推断变量的分布，比如说现在要求求出全国成年男人的身高的一个分布，那只需要测出每个成年男人的身高后进行统计即可。 但是在实际中，受限于人力物力和测试的难度，我们往往不会对每个成年男人进行身高的测试，而是在全国男人中选择部分的男人进行测试(如根据每个地区的人口数量按比例测试)，然后用这部分男人的身高分布来推断全国男人的分布，这样的推断肯定会存在误差，但是通过增加样本的数量，可以减少这种误差(大数定理)。 上面其实就是一个很简单的数理统计过程，当中有几个概念需要注意，例子中的全国男人的身高是一个总体，选择出来实际测试身高的男人是一个样本，测试得到的身高称为样本值（观测值），总体和样本中的数目分别称为他们的容量。 其严格定义如下： 设 $X$ 是具有分布函数 $F$ 的随机变量, 若 $X_1, X_2, …,X_n$ 是具有同一分布函数 $F$ 的相互独立的随机变量，则称 $X_1, X_2, …,X_n$ 为从分布函数 $F$ 得到的容量为 $n$ 的简单随机样本，简称样本，他们的观测值 $x_1, x_2,…x_n$ 称为样本值，又称为 $X$ 的 $n$ 个独立的观测值。 由定义可知样本 $X_1,X_2,…,X_n$ 相互独立，且他们的分布函数均为 $F$ , 所以 ( $X_1,X_2,…,X_n$ )的分布函数为 F^*(x_1,x_2,…,x_n) = \prod_{i=1}^nF(x_i)同样,(X1,X2,…,Xn)的概率密度函数为： f^*(x_1,x_2,…,x_n) = \prod_{i=1}^nf(x_i)抽样分布统计量样本是进行统计推断的依据，但是在应用中，往往不是直接使用样本本身，而是针对不同问题构造适当的样本的函数，利用这些样本的函数进行统计推断。 当这些样本的函数中不含未知变量时，我们称其为统计量，如下面就是几个常用的统计量，其中 $X_1,X_2,….,X_n$ 为总体的一个样本。 样本平均值： \overline X = \frac{1}{n} \sum_{i=1}^{n} X_i样本方差： S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline X)^2样本标准差： S = \sqrt {S^2}样本 k 阶原点矩： A_k = \frac{1}{n} \sum_{i=1}^{n} X_i^k (k=1,2,…)样本 k 阶中心矩: B_k = \frac{1}{n} \sum_{i=1}^{n}(X_i - \overline X)^k (k=2,3,4…..)这些统计量的定义与概率论中的基本相似，唯一比较奇怪的是为什么样本方差的分母是 $n−1$ 而不是 $n$，原因是通过数学证明可以得到只有当分母取n-1时，用样本来估计总体才是无偏的(无偏指的是估计量的期望与总体的参数一致)，下面是分母取n时得到的有偏估计的证明过程（$S^2_1$为样本方差） 统计量的分布使用统计量进行统计推断时，常常需要知道其分布，统计量的分布也称为抽样分布，下面介绍三种来自正态分布的抽样分布： $χ^2$ 分布，$t$ 分布和 $F$ 分布。 $χ^2$ 分布$χ^2$ 分布的定义如下 设 $X_1, X_2,…X_n$ 是来自总体 $N(0,1)$ 的样本，则称统计量 \chi^2 = X_1^2 + X_2^2 +….X_n^2为服从自由度为 $n$ 的 $χ^2$ 分布 上面的自由度指的是右端独立变量的个数。 $χ^2(n)$ 的概率密度函数为 f(y) = \begin{cases} \frac{1}{2^{\frac{n}{2}}\Gamma(n/2)}y^{n/2-1}e^{-y/2} &{y>0} \\ 0&{其他}\end{cases}上式的 $Γ$ 函数定义为 \Gamma = \int_{0}^{\infty} \frac{t^z - 1}{e^t} dt$f(y)$ 的图像如下所示 关于 $χ^2(n)$ 有以下几个有用的结论： 可加性设 $\chi_1^2$~$\chi^2(n_1), \chi_2^2$ ~ $\chi_2^2$ , 并且 $\chi_1^2, \chi_2^2$ 相互独立，则有\chi_1^2 + \chi_2^2 \sim \chi^2(n_1 + n_2) 期望和方差若$\chi^2(n)$，则χ2的期望和方差如下所示E(\chi^2) = n, D(\chi^2)=2n 分位点分位点的定义如下，给定正数 $a,0&lt;a&lt;1$, 称满足下面条件 P(\chi^2 \gt \chi_a^2(n)) = \int_{\chi_a^2(n)}^{\infty}f(y)dy= a的 $\chi_a^2(n)$ 为 $\chi^2(n)$ 上的 $a$ 分位点，其图像如下所示 由定义可知，分位点由 $a,n$ 共同决定，因此对于不同的 $a，n$ 可以查阅表格得到其 $a$ 分位点。 $t$ 分布t分布的定义如下： 设 $X \sim N(0,1), Y \sim \chi^2(n)$, 且 X,Y 相互独立，则称随机变量 t = \frac{X}{\sqrt{Y/n}}服从自由度为 $n$ 的 $t$ 分布, 记为 $t∼t(n)$其概率密度函数和对应的图像如下所示： 其分位点的定义与上面讲述的一样， P(t \gt t_a(n)) = \int_{t_a(n)}^{\infty}h(t)dt= a 且由于其概率密度函数的对称性可知,总是存在这样对称的两个分位点 ： $t_{1-a}(n) = -t_a(n)$ $F$ 分布$F$ 分布的定义如下 设 $U∼χ^2(n_1),V∼χ^2(n_2$， 且 $U,V$ 相互独立，则称随机变量 F = \frac{U/n_1}{V/n_2}服从自由度为 $(n_1,n_2)$ 的 $F$ 分布，记为$F∼F(n_1,n_2)$其概率密度函数为： \psi(y) = \begin{cases} \frac{\Gamma((n_1+n_2)/2)(n_1/n_2)^{n_1/2}y^{n_1/2-1}}{\Gamma(n_1/2)\Gamma(n_2/2)[1+(n_1y/n_2)]^{(n_1+n_2/)2}} &{y>0} \\ 0&{其他}\end{cases}概率密度函数的图像如下所示 其分位点定义同上 P(F \gt F_a(n_1,n_2)) = \int_{F_a(n_1,n_2)}^{\infty}\psi(y)dy= a 且具有以下性质 F_{1-a}(n_1,n_2) = \frac{1}{F_a(n_2,n_1)}上面只是简单地介绍了三大抽样分布，并未介绍其作用，实际上三大抽样分布主要用于参数的区间估计中，而这主要基于从正态分布中抽取的样本所构造的统计量服从这三大分布这一事实，从下面要介绍的定理中可以看到了这三大抽样分布的作用。更详细的作用会在区间估计中进一步体现。 正态总体的样本均值与样本方差的分布(统计量的应用)由于正态分布的普遍性，这里特意指出从服从正态分布的总体中抽取出的样本的所服从的分布。 假设上面的 $X$ 服从正态分布 $N(μ,σ^2)$, 则有以下几条定理，这几条定理在数理统计的区间估计中起了重要作用。 定理一 定理一： 设 $X_1, X_2,….X_n$ 服从 $N(μ,σ^2)$，$\overline X$是样本均值，则有\overline X \sim N(\mu,\sigma^2/n) 证明如下： E(\overline X) = E(\frac{1}{n}\sum_{i=1}^{n} X_i) = \frac{1}{n}E(\sum_{i=1}^{n} X_i) = \frac{1}{n}n E(X) = \muD(\overline X) = D(\frac{1}{n}\sum_{i=1}^{n} X_i) = \frac{1}{n^2}D(\sum_{i=1}^{n} X_i) = \frac{1}{n}n D(X) = \sigma^2/n定理一通常用于区间估计中已知总体（服从正态分布）的期望$μ$来估计其未知的方差 $σ^2$ ,或已知方差 $σ^2$ 来估计未知的期望 $μ$。 定理二 定理二 ：设 $X_1, X_2,….X_n$ 服从 $N(μ,σ^2)$，$\overline X$是样本均值，$S^2$ 是样本的方差，则 $\overline X$ 和 $S^2$ 相互独立，且有\frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1) 由于该定理的证明部分较为冗长，这里略去证明过程，感兴趣的读者可参考相关书籍。定理二主要用于区间估计中总体（服从正态分布）的期望、方差均未知时，估计其方差的范围，这也是 $χ^2$ 分布的作用之一。 定理三 定理三：设 $X_1, X_2,….X_n$ 服从 $N(μ,σ^2)$，$\overline X$是样本均值，$S^2$ 是样本的方差，则\frac{\overline X - \mu}{S/\sqrt{n}} \sim t(n-1) 定理三主要用于区间估计中总体（服从正态分布）的期望、方差均未知时，估计其期望的范围，这也是 $t$ 分布的作用之一,注意前面讲到的 $χ^2$ 分布估计的是方差。 证明：根据定理一，易知 $\overline X - \mu \sim N(0, \sigma^2/n)$ , 则 $\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1)$ , 从定理二可知 \frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)则根据t分布的定义有 \frac{\frac{\overline X - \mu}{\sqrt{\sigma^2/n}}} {\sqrt{\frac{(n-1)S^2}{\sigma^2(n-1)}}} \sim t(n-1)化简可得 \frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim t(n-1)定理三主要用于区间估计中总体（服从正态分布）的期望、方差均未知时，估计其期望的范围，这也是 $t$ 分布的作用之一,注意前面讲到的 $χ^2$ 分布估计的是方差。 定理四 定理四：设 $X_1, X_2…X_n$ 与 $Y_1,Y_2,…Y_n$ 分别是来自正态总体 $N(\mu_1, \sigma_1^2)$ 和 $N(\mu_2, \sigma_2^2)$ 的样本, $\overline X, \overline Y$ 分别是其样本均值，$S_1^2, S_2^2$ 分别是其样本方差。则有\frac{S_1^2/S_2^2}{\sigma_1^2/ \sigma_2^2} \sim F(n_1 - 1, n_2 - 1)且当 $\sigma_1^2 = \sigma_2^2 = \sigma^2$ 时，\frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{S_w\sqrt{1/n_1+1/n_2}} \sim t(n_1+n_2-2)其中，$S_w = \sqrt{\frac{(n_1 -1)S_1^2+(n_2 -1)S_2^2}{n_1+n_2-2}}$ 定理四的作用是在区间估计时估计两个均服从正态分布的总体的方差的比值（期望未知）以及两者期望的差距（方差未知）证明如下:由定理二可知 \frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2(n_1-1), \frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)由 $F$ 分布的定义可知 \frac{(n_1-1)S_1^2}{\sigma_1^2(n_1-1)} / \frac{(n_2-1)S_2^2}{\sigma_2^2(n_2-1)} \sim F(n_1-1, n_2-1)化简可得 \frac{S_1^2/S_2^2}{\sigma_1^2/ \sigma_2^2} \sim F(n_1 - 1, n_2 - 1)当 $\sigma_1^2 = \sigma_2^2 = \sigma^2$ 时, 易知 (\overline X - \overline Y) \sim N(\mu_1 - \mu_2,\sigma_1^2/n_1 + \sigma_2^2/n_2)则 $\frac{(\overline X - \overline Y)- (\mu_1 - \mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}} \sim N(0,1)$由定理二可知 \frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2(n_1-1), \frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_2-1), 由 $χ^2$ 分布的可加性可知： \frac{(n_1-1)S_1^2}{\sigma_1^2} + \frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_1+n_2-2)由t分布的定义可知： \frac{(\overline X - \overline Y)- (\mu_1 - \mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}} / (\sqrt{(\frac{(n_1-1)S_1^2}{\sigma_1^2} + \frac{(n_2-1)S_2^2}{\sigma_2^2})/(n_1+n_2-2)}) \sim t(n_1+n_2-2)将 $\sigma_1^2 = \sigma_2^2 = \sigma^2$ 代入到上式化简即可得到 \frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{S_w\sqrt{1/n_1+1/n_2}} \sim t(n_1+n_2-2)其中，$S_w = \sqrt{\frac{(n_1 -1)S_1^2+(n_2 -1)S_2^2}{n_1+n_2-2}}$ 小结$χ^2$ 分布主要解决总体期望未知时估计其方差的问题， $t$ 分布主要解决总体方差未知时估计其期望的问题，$F$ 主要解决期望未知时两个正态分布的方差比值问题。需要注意的是上面估计的前提是总体服从正态分布]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>统计</tag>
        <tag>抽样分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见分布]]></title>
    <url>%2F2017%2F08%2F15%2F%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[本文主要讲述三种离散型随机变量的分布(伯努利分布,二项分布,泊松分布)和三种连续型随机变量的分布(均匀分布,指数分布,正态分布)。三大抽样分布 离散型随机变量的分布伯努利分布伯努利分布又名两点分布或者0-1分布，只能取两种结果，一般记为0或1。设取1的概率为$p$，其分布规律为 P(X=k) = p^k(1-p)^{1-k}, k = 0,1 (1s) = P(X>t)该性质也称为无记忆性，假设 $X$ 是某一原件的寿命，上面的式子表示的就是该元件在使用了 s 个小时后，至少还能使用 t 个小时的条件概率。而这一条件概率又等于该元件从刚开始使用的算起至少能使用 t 个小时的概率。也就是说原件对使用过的s个小时无记忆性，这个特性与随机过程中的平稳过程非常相似，而这个特性也是指数分布有广泛应用的重要原因。 正态分布正态分布也叫高斯分布，其概率密度函数为 f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}则称 $X$ 服从参数为 $μ,σ$ 的正态分布，记为$N(\mu,\sigma^2)$，而且 $μ,σ$ 分别是正态分布的期望和标准差。其图像如下所示 从图像可知，当$X=μ$时，取值最大，也就是说随机变量落在这个值附近的概率最大，而这个值也就是正态分布的期望 抽样分布统计量样本是进行统计推断的依据，但是在应用中，往往不是直接使用样本本身，而是针对不同问题构造适当的样本的函数，利用这些样本的函数进行统计推断。 当这些样本的函数中不含未知变量时，我们称其为统计量，如下面就是几个常用的统计量，其中 $X_1,X_2,….,X_n$ 为总体的一个样本。 样本平均值： \overline X = \frac{1}{n} \sum_{i=1}^{n} X_i样本方差： S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline X)^2样本标准差： S = \sqrt {S^2}样本 k 阶原点矩： A_k = \frac{1}{n} \sum_{i=1}^{n} X_i^k (k=1,2,…)样本 k 阶中心矩: B_k = \frac{1}{n} \sum_{i=1}^{n}(X_i - \overline X)^k (k=2,3,4…..)这些统计量的定义与概率论中的基本相似，唯一比较奇怪的是为什么样本方差的分母是 $n−1$ 而不是 $n$，原因是通过数学证明可以得到只有当分母取n-1时，用样本来估计总体才是无偏的(无偏指的是估计量的期望与总体的参数一致)，下面是分母取n时得到的有偏估计的证明过程（$S^2_1$为样本方差） 统计量的分布使用统计量进行统计推断时，常常需要知道其分布，统计量的分布也称为抽样分布，下面介绍三种来自正态分布的抽样分布： $χ^2$ 分布，$t$ 分布和 $F$ 分布。 $χ^2$ 分布$χ^2$ 分布的定义如下 设 $X_1, X_2,…X_n$ 是来自总体 $N(0,1)$ 的样本，则称统计量 \chi^2 = X_1^2 + X_2^2 +….X_n^2为服从自由度为 $n$ 的 $χ^2$ 分布 上面的自由度指的是右端独立变量的个数。 $χ^2(n)$ 的概率密度函数为 f(y) = \begin{cases} \frac{1}{2^{\frac{n}{2}}\Gamma(n/2)}y^{n/2-1}e^{-y/2} &{y>0} \\ 0&{其他}\end{cases}上式的 $Γ$ 函数定义为 \Gamma = \int_{0}^{\infty} \frac{t^z - 1}{e^t} dt$f(y)$ 的图像如下所示 关于 $χ^2(n)$ 有以下几个有用的结论： 可加性设 $\chi_1^2$~$\chi^2(n_1), \chi_2^2$ ~ $\chi_2^2$ , 并且 $\chi_1^2, \chi_2^2$ 相互独立，则有\chi_1^2 + \chi_2^2 \sim \chi^2(n_1 + n_2) 期望和方差若$\chi^2(n)$，则χ2的期望和方差如下所示E(\chi^2) = n, D(\chi^2)=2n 分位点分位点的定义如下，给定正数 $a,0&lt;a&lt;1$, 称满足下面条件 P(\chi^2 \gt \chi_a^2(n)) = \int_{\chi_a^2(n)}^{\infty}f(y)dy= a的 $\chi_a^2(n)$ 为 $\chi^2(n)$ 上的 $a$ 分位点，其图像如下所示 由定义可知，分位点由 $a,n$ 共同决定，因此对于不同的 $a，n$ 可以查阅表格得到其 $a$ 分位点。 $t$ 分布t分布的定义如下： 设 $X \sim N(0,1), Y \sim \chi^2(n)$, 且 X,Y 相互独立，则称随机变量 t = \frac{X}{\sqrt{Y/n}}服从自由度为 $n$ 的 $t$ 分布, 记为 $t∼t(n)$其概率密度函数和对应的图像如下所示： 其分位点的定义与上面讲述的一样， P(t \gt t_a(n)) = \int_{t_a(n)}^{\infty}h(t)dt= a 且由于其概率密度函数的对称性可知,总是存在这样对称的两个分位点 ： $t_{1-a}(n) = -t_a(n)$ $F$ 分布$F$ 分布的定义如下 设 $U∼χ^2(n_1),V∼χ^2(n_2$， 且 $U,V$ 相互独立，则称随机变量 F = \frac{U/n_1}{V/n_2}服从自由度为 $(n_1,n_2)$ 的 $F$ 分布，记为$F∼F(n_1,n_2)$其概率密度函数为： \psi(y) = \begin{cases} \frac{\Gamma((n_1+n_2)/2)(n_1/n_2)^{n_1/2}y^{n_1/2-1}}{\Gamma(n_1/2)\Gamma(n_2/2)[1+(n_1y/n_2)]^{(n_1+n_2/)2}} &{y>0} \\ 0&{其他}\end{cases}概率密度函数的图像如下所示 其分位点定义同上 P(F \gt F_a(n_1,n_2)) = \int_{F_a(n_1,n_2)}^{\infty}\psi(y)dy= a 且具有以下性质 F_{1-a}(n_1,n_2) = \frac{1}{F_a(n_2,n_1)}上面只是简单地介绍了三大抽样分布，并未介绍其作用，实际上三大抽样分布主要用于参数的区间估计中，而这主要基于从正态分布中抽取的样本所构造的统计量服从这三大分布这一事实，从下面要介绍的定理中可以看到了这三大抽样分布的作用。更详细的作用会在区间估计中进一步体现。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>概率</tag>
        <tag>分布</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参数估计]]></title>
    <url>%2F2017%2F08%2F15%2F%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[在数理统计中，常常需要通过样本来估计总体的参数，估计可划分为两大类：点估计和区间估计。点估计就是估计总体中某个参数的值，而区间估计是估计总体的某个参数落在某个区间的概率大小。本文主要讲述点估计中的矩估计法和最大似然估计法，以及针对服从正态分布的期望和方差进行区间估计。 参数估计定义 已知一个随机变量的分布函数 $Xf_θ(x)$, 其中 $θ = (θ_1,··· ,θ_k)$ 为未知参数. 样本 $X_1,··· ,X_n$ 利用样本对参数 $θ$ 做出估计，或者估计 $θ$ 的某个函数 $g(θ)$ 点估计: 用样本的一个函数 $T(X_1,··· ,X_n)$ 去估计 $g(θ)$ 区间估计: 用一个区间去估计 $g(θ)$ 点估计点估计一般解决的问题是总体 $X$ 的分布函数 $F(X,θ)$ 形式为已知，但是 $θ$ 参数未知。点估计的目的就是通过样本 $X_1,X_2,…X_n$ 构造一个适当的统计量 $θ′(X_1,X_2,…X_n)$，用于作为未知参数 $θ$ 的近似值。由于 $θ′$ 是样本的函数，因此对于不同的样本，$θ′$ 的值一般不同。 点估计中一般用到的方法包括矩估计法和最大似然估计法。 矩估计法矩估计法的核心思想是样本矩总是收敛于相应的总体矩，因此可通过样本矩作为相应的总体矩的估计量，进而根据总体矩与待估参数的关系求出待估参数。 矩估计法的一般描述如下：设 $X$ 为连续型随机变量，其概率密度函数为 $f(x;\theta_1, \theta_2,..\theta_n)$ ；离散型随机变量，其分布律为 $P(X=x) = p(x; \theta_1, \theta_2,..\theta_n)$ ；则总体的 $n$ 阶矩分别为 \begin{align*} E(X^n) &= \int_{-\infty}^{\infty} x^nf(x;\theta_1, \theta_2…\theta_n) dx \\ E(X^n) &= \sum_{x \in R_x} x^np(x;\theta_1, \theta_2….\theta_n) \end{align*}而样本 $X_1, X_2…X_k$ 的 $n$ 阶矩的定义为 A_n = \frac{1}{k} \sum_{i=1}^{k}X_i^n由于总体的 $n$ 阶矩往往是未知参数 $θ$ 的函数，因此常常先用总体的 $n$ 阶矩 $E(X^n)$ 将参数 $θ$ 表示出来，然后用样本矩 $A_n$ 代替总体的 $n$ 阶矩 $E(X^n)$ ,进而得出估计的 $θ$ 的值。下面是一个简单的例子 最大似然估计法最大似然估计的思想是既然当前取得了这组样本，那么有理由相信已取得的样本出现的概率是很大的。因此通过极大化这组样本的联合概率来估计未知参数的值。 离散型总体单总体为离散型的时候，设当前样本为 $X_1,X_2,…X_n$， 则其联合概率为 $\prod_{i=1}^{n} p(x_i;\theta)$, 其中 $x_i$ 是 $X_i$ 相应的观测值，则上面的联合概率实际上是参数 $θ$ 的函数，记为 L(\theta) = \prod_{i=1}^{n} p(x_i;\theta)上面的 $L(θ)$ 被称为样本的似然函数。 选择 $θ$ 的值使得 $L(θ)$ 最大便是最大似然估计做的事情。一般通过对 似然函数求导便可求得其最大值对应的 $θ$。如下是一个简单的例子 上面最后求解的结果是 $p’ = \overline x$ 。同时也注意到求解似然函数最大化时会先对似然函数取 $log$ , 目的是将连乘变为连加，方便运算，同时这种方法也被称为对数极大似然估计。 连续型总体若总体是连续型，设其概率密度函数为 $f(x,θ)$ ，则当前样本 $X_1,X_2,…X_n$ 的联合概率密度函数为 \prod_{i=1}^{n}f(x_i;\theta)其中 $x_1,x_2,…x_n$ 是相应于样本的一个样本值，则随机点落在 （$x_1,x_2,…x_n$）的领域（边长为 $dx_1, dx_2,…dx_n$ 的n维立方体）内的概率近似为 \prod_{i=1}^{n}f(x_i;\theta)dx_i同样我们要让上式取到最大，但是因子 $\prod_{i=1}^{n}dx_i$ 不随 $θ$ 改变，因此只需考虑函数 $L(\theta) = \prod_{i=1}^{n}f(x_i;\theta)$ 最大即可，这里 $L(θ)$ 被称为似然函数，极大化也是通过求导来解决。 下面是一个连续型总体进行极大似然估计的例子 评选标准对于同一参数，不同的估计方法求出的估计量可能不一样，那么如何判断不同的估计量之间的优劣，无偏性，有效性和相合性是常用的三个指标。 相合性 (consistency): 当样本数量趋于无穷时，估计量收敛 于参数真实值. 无偏性 (bias): 对于有限的样本，估计量所符合的分布之期 望等于参数真实值. 有效性 (efficiency): 估计值所满足的分布方差越小越好. 相合性当样本数目 $n→∞$ 时，估计量 $\theta’(X_1，X_2…X_n)$ 收敛于真正的 $θ$ ,则称 $θ′$ 为 $θ$ 的相合估计量。即有以下式子成立 \lim_{n \rightarrow \infty}P(|\theta’ - \theta| < \epsilon) = 1相合性是一个估计量的基本要求，如果估计量没有相合性，那么无论样本数量 n 取多大，这些估计量都无法准确估计正确参数，都是不可取的。 无偏性无偏性指的是从样本中得到的估计量 $θ′$ 的期望与总体的参数 $θ$ 相等，也就是 E(θ′)=θ此时称 $θ′$ 是 $θ$ 的无偏估计量。无偏估计量的意义是对于某些样本值，这一估计量得到的估计值比真实值要打，而对于另外一些样本则偏小，反复将这一估计量使用多次，就平均来说其偏差为零。 有效性当两个估计量 $\theta_1’, \theta_2’$ 均是无偏估计量时，就要通过比较他们的有效性来决定选取哪个估计量。有效性指的是在样本容量 $n$ 相同的情况下，假如 $θ_1’$ 的观察值较 $θ_2’$ 的值更密集在真值 $θ$ 附近，那么认为 $θ_1’$ 比 $θ_2’$ 更为理想。 实际上，上面比较的就是两个估计量的方差大小，方差越小，则越有效，因此当两个总体的样本数相同的时候，若 $D(θ_1’)&lt;D(θ_2’)$ 时， 就称 $θ_1’$ 比 $θ_2’$ 更有效。 区间估计对于总体中的未知参数，我们的估计总是存在着一定的误差的，如何去衡量这个误差是一个需要考虑的事情。同时，除了上面的点估计，在实际中我们往往还希望估计出参数的一个范围，同时参数落在这个范围的概率，或者是说可信程度。 估计参数落在某个范围以及落在这个范围的可信程度就是区间估计干的事情。 其严格定义如下 设总体的分布中存在一个未知参数 $θ$, 对于给定的值 $α(0&lt;α&lt;1)$, 若通过样本 $X_1,X_2,X_3…X_n$ 估计的两个统计量 $θ_1′$ 和 $θ_2′$ 满足下面不等式时 P(\theta’_1 < \theta < \theta’_2) \ge 1 - \alpha则称区间 $(\theta’_1, \theta’_2)$ 是参数 $θ$ 置信水平为 $1−α$ 的置信区间, $\theta’_1, \theta’_2$ 分别称为置信下限和置信上限。 上面式子的含义是若反复抽样多次（每次得到的样本的容量相等），每个样本值确定一个区间$(\theta’_1, \theta’_2)$，这个区间要么包含 $θ$ 的真值，要么不包含 $θ$ 的真值,在这么多的区间中，包含 $θ$ 真值的概率约为 $1−α$. 正态分布均值与方差的区间估计由于正态分布的普遍性，下面主要讲述对正态分布的期望和方差进行区间估计的方法，而这里会用到我们前面讲到的统计量的三大分布： $χ^2$ 分布， $t$ 分布， $F$ 分布，以及对其拓展的一些定理，具体的定理及其证明可参考抽样与统计.md。 下面会讲述单个正态分布的期望和方差的估计，以及两个正态分布的期望差和方差比的估计。 单个正态分布下面的关于单个正态分布的讨论都是基于以下假设：给定置信水平为 $1−α$ , 设 $X_1,X_2,X_3…X_n$ 为总体 $N(μ,σ^2)$ 的样本，$\overline X，S^2$ 分别是样本的期望和方差。 估计期望 $μ$ 的置信区间通过样本 $X_1,X_2,X_3…X_n$ 估计总体 $N(μ,σ^2)$ 的期望 $μ$ 时可以分为两种情况： 总体的方差 $σ^2$ 已知 总体的方差 $σ^2$ 未知总体的方差 $σ^2$ 已知-&gt;定理一若已知总体的方差，则因为 $\overline X \sim N(\mu , \sigma^2/n)$ , 即 $\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1)$ , 下面都会这样不加证明给出这些统计量服从的分布，具体的证明参考这篇文章。 按照标准正态分布的上 $α$ 分位点的定义有 P(|\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} | < z_{\alpha/2}) = 1 - \alpha从概率密度函数上直观看为： 进一步化简有 P(\overline X - \frac{\sigma}{\sqrt{n}}z_{\alpha/2} < \mu < \overline X + \frac{\sigma}{\sqrt{n}}z_{\alpha/2}) = 1 - \alpha给定 $\alpha, z_{\alpha/2}$ 的值可以通过查表获得。这样便得到了期望 $μ$ 的一个估计区间为 $(\overline X - \frac{\sigma}{\sqrt{n}}z_{\alpha/2}, \overline X + \frac{\sigma}{\sqrt{n}}z_{\alpha/2})$, 其置信度为 $1−α$。注意置信水平为 $1−α$ 的置信区间并不是唯一的，假如说给定 $α=0.05$, 则上面的式子可写为 P(\overline X - \frac{\sigma}{\sqrt{n}}z_{0.025} < \mu < \overline X + \frac{\sigma}{\sqrt{n}}z_{0.025}) = 1 - \alpha同时也可写为 P(\overline X - \frac{\sigma}{\sqrt{n}}z_{0.04} < \mu < \overline X + \frac{\sigma}{\sqrt{n}}z_{0.01}) = 1 - \alpha但是写成不对称的形式计算出来的区间长度要更长，显然，置信度相同的情况下，置信区间肯定是越小越好，所以对于正态分布的分位点往往选择对称形式。 下面的求解方法与这方法类似，只是构造的统计量不同，因而服从的分布也不同。 总体的方差 $σ^2$ 未知-&gt;定理三当总体方差未知时，就无法利用上面标准正态分布。但是回忆 $t$ 分布的作用及其定理，可知 \frac{\overline X - \mu}{S/\sqrt{n}} \sim t(n-1)同样按照 $t$ 分布的上 $α$ 分位点的定义有 P(|\frac{\overline X - \mu}{S/\sqrt{n}}| < t_{\alpha/2}(n-1)) = 1 - \alpha其对应的概率密度函数如下所示进一步化简可得 P(\overline X - \frac{S}{\sqrt{n}}t_{\alpha/2}(n - 1) < \mu < \overline X + \frac{S}{\sqrt{n}}t_{\alpha/2}(n - 1)) = 1 - \alpha则期望 $μ$ 的一个置信水平为 $1−α$ 的置信区间为 (\overline X - \frac{S}{\sqrt{n}}t_{\alpha/2}(n - 1), \overline X + \frac{S}{\sqrt{n}}t_{\alpha/2}(n - 1))估计方差 $σ^2$ 的置信区间估计方差 $σ^2$ 的置信区间也可分为两种情况 总体的期望 $μ$ 已知 总体的期望 $μ$ 未知总体的期望 $μ$ 已知-&gt;定理一 当期望 $μ$ 已知时，求解方差 $σ^2$ 的置信区间的方法跟上面已知方差 $σ^2$ 求解期望 $μ$ 的一样，都是利用 $\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1)$，然后写出对应未知量的区间，这里就不详细讲述已知 $μ$ 求解方差 $σ^2$ 的详细过程了。 总体的期望 μ 未知-&gt;定理二当期望 $μ$ 未知时，求解方差 $σ^2$ 的区间估计就再也不能利用上面的 $\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1)$。结合 $χ^2$ 分布的特性及其推导的定理可知 \frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)同样按照 $χ^2$ 分布的 $α$ 分位点的定义有 P( \chi^2_{1 - \alpha/2}(n-1) < \frac{(n-1)S^2}{\sigma^2} < \chi^2_{\alpha/2}(n-1)) = 1 - \alpha注意这里不能用绝对值了，原因是 $χ^2$ 分布的概率密度函数不像标准正态分布或 $t$ 分布那样是对称的。其对应的概率密度函数如下所示进一步化简可得 P(\frac{(n-1)S^2}{\chi^2_{\alpha/2}(n-1)} < \sigma^2 < \frac{(n-1)S^2}{\chi^2_{1 - \alpha/2}(n-1)}) = 1 - \alpha即给定样本，总体期望 $μ$ 未知的时候，总体方差 $σ^2$ 的一个置信水平为 $1−α$ 的置信区间为 (\frac{(n-1)S^2}{\chi^2_{\alpha/2}(n-1)}, \frac{(n-1)S^2}{\chi^2_{1 - \alpha/2}(n-1)})实际上， $χ^2$ 分布的一个作用就是在正态总体分布中期望未知时估计其方差的置信区间。 两个正态分布-&gt;定理四下面讲述两个正态分布的期望差值的区间估计以及方差比的估计。考虑以下问题：已知产品的某一质量指标服从正态分布，但由于原料、操作人员不同，或工艺过程的改变等因素，引起总体均值、方差有所变化。我们需要知道这些变化有多大，就需要考虑两个正态分布均值差或方差比的估计问题。 下面的讨论都是假设给定了置信水平为 $1−α$, 并设 $X_1, X_2,….X_n$ 是来自第一个总体 $N_1(\mu_1, \sigma_1^2)$ 的样本，$Y_1, Y_2,….Y_n$ 是来自第二个总体 $N_2(\mu_2, \sigma_2^2)$ 的样本，并假设 $\overline X, \overline Y$ 是第一、第二个样本的均值， $S_1^2, S_2^2$ 是第一、第二个样本的方差。 估计 $μ_1−μ_2$ 的置信区间估计 $μ_1−μ_2$ 的置信区间时也可以分为两种情况 总体的方差 $\sigma_1^2, \sigma_2^2$ 已知 总体的方差 $\sigma_1^2, \sigma_2^2$ 未知，但是知道 $\sigma_1^2 = \sigma_2^2 = \sigma^2$（$σ$未知）总体的方差 $\sigma_1^2, \sigma_2^2$ 已知-&gt;定理一由 $\overline X \sim N(\mu_1, \sigma_1^2/n_1), \overline Y \sim N(\mu_2, \sigma_2^2/n_2)$ 可知\overline X - \overline Y \sim N(\mu_1 - \mu_2, \sigma_1^2/n_1 + \sigma_2^2/n_2) \\ \frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}} \sim N(0, 1)与上面相同，按照标准正态分布的上 $α$ 分位点的定义有P(|\frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}} | < z_{\alpha/2}) = 1 - \alpha同样可解得 $μ_1−μ_2$ 置信度为 $1−α$ 的区间。 总体的方差 $\sigma_1^2, \sigma_2^2$ 未知，但 $\sigma_1^2 = \sigma_2^2 = \sigma^2$（$σ$未知）-&gt;定理四根据 $t$ 分布的作用及其推导的定理可知 \frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{S_w\sqrt{1/n_1+1/n_2}} \sim t(n_1+n_2-2)其中 $S_w = \sqrt{\frac{(n_1 -1)S_1^2+(n_2 -1)S_2^2}{n_1+n_2-2}}$同样根据 $t$ 分布的上 $α$ 分位点的定义有 P(|\frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{S_w\sqrt{1/n_1+1/n_2}}| < t_{\alpha/2}(n_1+n_2-2)) = 1 - \alpha通过查表同样可以求出 $μ_1−μ_2$ 置信度为 $1−α$ 的区间，结合上面 $t$ 分布在单个正态总体分布参数估计的问题可知， $t$ 分布专门用于解决正态分布中方差未知时估计其期望的问题。 计 $\sigma_1^2 / \sigma_2^2$ 的置信区间估计 $\sigma_1^2 / \sigma_2^2$ 的置信区间同样可以分为两种情况 总体期望 $μ_1,μ_2$ 已知 总体期望 $μ_1,μ_2$ 未知 总体期望 $μ_1,μ_2$ 已知-&gt;定理一总体期望 $μ_1,μ_2$ 已知时可以先通过标准正态分布求出 $\sigma_1^2, \sigma_2^2$ 各自的范围, 然后求解 $\sigma_1^2 / \sigma_2^2$ 的范围。 总体期望 $μ_1,μ_2$ 未知时，如何估计 $\sigma_1^2 / \sigma_2^2$ 的范围-&gt;定理四由 $F$ 分布的定义以及推导的定理可知 \frac{S_1^2/S_2^2}{\sigma_1^2/ \sigma_2^2} \sim F(n_1 - 1, n_2 - 1)根据 $F$ 分布的 $α$ 分位点的定义有 P( F_{1 - \alpha/2}(n_1 - 1, n_2 - 1) < \frac{S_1^2/S_2^2}{\sigma_1^2/ \sigma_2^2} < F_{\alpha/2}(n_1 - 1, n_2 - 1)) = 1 - \alpha化简可得 P( \frac{S_1^2}{S_2^2}\frac{1}{F_{\alpha/2}(n_1 - 1, n_2 - 1)} < \frac{\sigma_1^2}{ \sigma_2^2} < \frac{S_1^2}{S_2^2}\frac{1}{F_{1 - \alpha/2}(n_1 - 1, n_2 - 1)}) = 1 - \alpha即 $\sigma_1^2 / \sigma_2^2$ 一个置信度为 $1−α$ 的置信区间为 (\frac{S_1^2}{S_2^2}\frac{1}{F_{\alpha/2}(n_1 - 1, n_2 - 1)} < \frac{\sigma_1^2}{ \sigma_2^2}, \frac{S_1^2}{S_2^2}\frac{1}{F_{1 - \alpha/2}(n_1 - 1, n_2 - 1)})小结在上面对正态分布总体进行参数估计中，用到了数理统计中的三大分布： $χ^2$ 分布， $t$ 分布和 $F$ 分布， 其中 $χ^2$ 分布主要解决总体期望未知时估计其方差的问题， $t$ 分布主要解决总体方差未知时估计其期望的问题，$F$ 主要解决期望未知时两个正态分布的方差比值问题。 单侧置信区间上面均是讨论未知参数 $θ$ 的双侧置信区间，但是在实际问题中，往往考虑的只是一个上限或下限，比如说设备、原件的寿命我们关心的是平均寿命 $θ$ 的下限。这就引出了单侧置信区间的概念。单侧置信区间跟双侧置信区间的概念非常类似。 总体的参数 $θ$ 未知, 对于给定的 $α$ ,若由样本 $X_1, X_2..X_n$ 确定的统计量 $θ′$满足 P(\theta > \theta’) = 1 - \alpha则称 $(θ′,∞)$ 是参数 $θ$ 的置信水平为 $1−α$ 的单侧置信区间，而 $θ′$ 是单侧置信下限，将 $θ&gt;θ′$ 变为 $θ&lt;θ′$ 后，相应地变为单侧置信上限。 单侧置信区间的计算方法与上面提到的双侧置信区间的计算方法已知，都是根据给定的 $α$ 值和统计量服从的分布去查表，找到相应的分位点后带入不等式求解目标估计量的范围即可。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>统计</tag>
        <tag>参数估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率]]></title>
    <url>%2F2017%2F08%2F14%2F%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[随机变量与概率概率密度函数的积分 离散随机变量假设随机变量 $X$ 的取值域为 $Ω = \{x_i\}_{i=1}^\infty$，那么对于任何一个 $x_i$，事件 $X = x_i$ 的概率记为 $P(x_i)$. 对于 $Ω$ 的任何一个子集 $S = \{x_i\}_{i=1}^\infty =1$，事件 $X ∈ S$ 的概率为 P(S) = \sum_{i=1}^{\infty }P(x_1)对于离散随机变量，概率为概率函数的求和 连续随机变量假设随机变量 $X$ 的取值域为 $\mathbb{R}$，那么对于几乎所有 $x ∈R$, 事件 $X = x$ 的概率 $P(X = x)$ 都等于 0. 所以我们转而定义概率密度函数 $f : \mathbb{R}→ [0,∞)$. 对于任何区间 $(a,b)$, 事件 $X ∈ (a,b)$ 的概率为 P((a,b)) = \int_{a}^{b}f(x)dx 条件概率与贝叶斯公式条件概率条件 $A$ 下事件 $S$ 发生的概率 P(S|A) = \frac{P(S\cap A)}{P(A)}贝叶斯公式P(A|B) = \frac{P(B|A)P(A)}{P(B)}大数定律和中心极限定理随机变量的矩 设 $X,Y$ 为随机变量,则$E(X^k), k=1,2,3….$ 称为 $X$ 的 $k$ 阶原点矩，简称 $k$ 阶矩$E((X-E[X])^k), k=1,2,3….$ 称为 $X$ 的 $k$ 阶中心距$E(X^kY^l),k,l=1,2,…$ 称为 $X$ 和 $Y$ 的 $k+l$ 阶混合矩$E((X-E[X])^k(Y-E[Y])^l)),k,l=1,2,…$称为 $X$ 和 $Y$ 的 $k+l$ 阶混合中心矩 当 $n = 1$ 时，$E(X)$ 为随机变量的期望 当 $n = 2$ 时，$Var(x)=\int_{-\infty }^{\infty }[x_i-\mu ]^2f(x)dx=E(X^2)−E(X)^2=\sum_{i=1}^nx_i^2p_i-\mu ^2$ 为随机变量的方差 特征函数， 当 $n=3$时，偏度$S(x)=\int_{-\infty }^{\infty }[x_i-\mu ]^3f(x)dx$ 当 $n=4$时，峰度$K(x)=\frac{\int_{-\infty }^{\infty }[x_i-\mu ]^4f(x)dx}{\sigma ^2}-3$ 归一化n阶中心矩为 $\frac{E[(x-\mu)^n ]}{\sigma ^n}$ 混合矩：混合矩是多个变量的矩，比如协方差，协偏度，协峰度。虽然协方差只有一个，但协偏度和协峰度存在多个 几何意义 一阶矩：矩可以描述随机变量的一些特征，期望是 $X$ “中心”位置的一种 描述， 二阶矩：方差可以描述 $X$ 的分散程度, 特征函数可以全面描述概率分布. 三阶矩：任何对称分布偏态为0，向左偏斜（分布尾部在左侧较长）具有负偏度；向右偏斜分布（分布尾部在右侧较长）具有正偏度 四阶矩：峰度表示分布的波峰和尾部与正态分布的区别。完全符合正态分布的数据峰度值为0,且正态分布曲线被称为基线。如果样本峰度显著偏离0，就可判断此数据不是正态分布。 切比雪夫不等式设 $X$ 为随机变量，期望值为 $µ$, 标准差为 $σ$, 对于任何实数 $k &gt; 0$ P(|X −µ|≥ kσ) ≤\frac{1}{k^2} 随机变量的协方差与相关系数 X、Y的协方差： $cov(X,Y ) =E\left ( (X-\mu _x)(Y-\mu _y) \right )= E(XY )−E(X)E(Y ) $协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减） X、Y的相关系数： $cov(X,Y)/\sqrt{Var(X)Var(Y)}$协方差的标准化 特征函数重要极限$\underset{n→∞}{lim}(1 + 1/n)^n$存在，且定义 $e = \underset{n→∞}{lim}(1 + 1/n)^n$ 于是定义 $e = \underset{n→∞}{lim}(1 + x/n)^n$ 大数定律$X$ 是随机变量，$µ$ 是 $X$ 的期望，$σ$ 是 $X$ 的方差.$\{X_k\}^∞ _{k=1}$ 是服从 $X$ 的独立同分步随机变量，那么 $\overline{X}_k=\frac{\sum_{k=1}^{n}X_k}{n}$ 依概率收敛于 $µ$. 也就是说对于任何 $ϵ &gt; 0$ 有 \lim_{n\rightarrow \infty }P(|\overline{X}_n-\mu |>\epsilon )=0 当大量重复同一个实验时，实验的平均结果会接近于期望值 重复次数越多越接近 中心极限定理中心极限定理指的是给定一个任意分布的总体。我每次从这些总体中随机抽取 n 个抽样，一共抽 m 次。 然后把这 m 组抽样分别求出平均值。 这些平均值的分布接近正态分布 几种常见的分布 离散概率分布（Discrete）离散均匀分布（discrete uniform）、伯努利分布（Bernoulli）、二项式分布（Binomial）、泊松分布（Poisson）、超几何分布（Hypergeometric） 连续概率分布（continuous）均匀分布（Uniform）、正态分布（Normal/Gaussian）、指数分布（Exponential）、Gamma分布 、Beta分布、Gumbel分布 抽样分布卡方分布、F分布、t分布]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>概率</tag>
        <tag>中心极限定理</tag>
        <tag>大数定律</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[积分学]]></title>
    <url>%2F2017%2F08%2F14%2F%E7%A7%AF%E5%88%86%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[定义令 $f(x)$ 为开区间 $(a,b)$ 上的一个连续函数，对于任何一个正整 数 $n$ 定义,$x_i = a + \frac{i(b-a)}{n}$ 求和式： S_n(f) =\sum_{i=0}^{n-1}f(x_i)(x_{i+1}-x_i)如果极限 $\underset{n→∞}{lim}S_n(f)$ 存在, 那么函数 $f(x)$ 在这个区间上的黎曼积分为 \int_{a}^{b}f(x)dx=\lim_{n\rightarrow \infty }S_n(f) 代数意义: 无穷求和 几何意义: 函数与 X 轴之间的有向面积 牛顿-莱布尼茨公式如果 $f(x)$ 是定义在闭区间 $[a,b]$ 上的可微函数, 那么就有 \int_{a}^{b}f'(x)dx=f(b)-f(a)不定积分表示为 \int_{a}^{b}f'(x)dx=f(x)+C 牛顿-莱布尼茨公式展示了微分与积分的基本关系: 在一定程度上微分与积分互为逆运算.f'(x)=\frac{df(x)}{dx}\Rightarrow df(x)=f'(x)dx\Rightarrow f(x)=\int df(x)=\int f'(x)dx 示例函数 $ln(x)$ 的不定积分令 $f(x) = xln(x)−x$，则 $f′(x) = 1·ln(x) + x·\frac{1}{x} −1 = ln(x)$.根据牛顿 -莱布尼茨公式我们得到 \int ln(t)dt=\int f'(t)dt=xln(x)-x+C多变量积分如果一个函数 $f(x,y)$ 有多个变量，那么在矩形 $[a,b]×[c,d]$ 上 的多重积分可以看成是每一个变量的依次积分 \int_{c}^{d}\int_{a}^{b}f(x,y)dxdy 如果积分区域形状不规则，可以用一个矩形把积分区域包起 来，并令函数在积分区域外边等于 0. 二重积分的几何意义是积分函数与 X −Y 坐标平面之间部 分的有向体积.]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>高数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微积分]]></title>
    <url>%2F2017%2F08%2F14%2F%E5%BE%AE%E7%A7%AF%E5%88%86%2F</url>
    <content type="text"><![CDATA[极限定义函数 $f$ 在 $x_0$ 处的极限为 $L$ \lim_{x\rightarrow 0} f(x) = L 如何比较无穷小无穷也分大小，如何描述与比较无穷大和无穷小 。通过相除比较无穷小。例如： \lim_{x\rightarrow 0} \frac{sin (x)}{tan (x)} = \lim_{x\rightarrow x_0} cos(x) = cos(0) = 1所以当 $x\rightarrow x_0$ 的时候，$sin(x)$ 与 $tan(x)$ 是同样级别的无穷小. 无穷小阶数 定义当 $x\rightarrow 0$ 时， 如果 $\lim_{x\rightarrow 0} f(x) = 0$ 而且 $\lim_{x\rightarrow 0} \frac{f (x)}{x^n} = 0$那么此时 $f(x)$ 为$n$ 阶以上无穷小，记为f(x) = o(x^n),x → 0 如果 $\lim_{x\rightarrow 0} f(x) = 0$ 而且 $\lim_{x\rightarrow 0} \frac{f (x)}{x^n} $ 存在且不等于零，那么 此时 $f(x)$ 为 $n$ 阶无穷小，记为f(x) = O(x^n),x → 0 为了方便，在不至于引起误解的时候我们回省略掉 $x → 0.$ Proposition (三明治/两边夹/夹逼原理)如果三个函数满足 $f(x) ≤ g(x) ≤ h(x)$, 而且他们都在 $x_0$ 处有极 限，那么 \lim_{x\rightarrow x_0} f(x) ≤ \lim_{x\rightarrow x_0} g(x) ≤ \lim_{x\rightarrow x_0} h(x)重要极限\begin{align*} &\lim_{x\rightarrow 0} sin(x)/x = 1 \\ &\lim_{x\rightarrow ∞} x^α/e^x = 0, 对于任意正数 α \\ &\lim_{x\rightarrow ∞} ln(x)/x^α = 0, 对于任意正数 α \\ &\lim_{x\rightarrow ∞} (1 + 1/x)^x = e \end{align*}微积分函数的导数定义如果一个函数 $f(x)$ 在 $x_0$ 附近有定义，而且存在极限 L=\lim_{x\rightarrow x_0} \frac{f (x)-f(x_0)}{x-x_0}那么 $f(x)$ 在 $x_0$ 处可导且导数 $f′(x_0) = L.$ 函数的高阶导数如果函数的导数函数仍然可导，那么导数函数的导数是二阶导数，二阶导数函数的导数是三阶导数. 一般地记为 f^{(n)}(x) = \frac{d}{dx} f^{(n−1)}(x)或者进一步 f^{(n)}(x) = \frac{d^n}{dx^n} f(x)多元函数 - 偏导数$f(x,y) = ln(x+y^2)$,则 \frac{\partial }{\partial x} f(x,y) = \frac{1}{x+y^2},\;\;\;\;\frac{\partial }{\partial x} f(x,y) = \frac{2y}{x+y^2}初等函数的导数\begin{align*} &\frac{d}{dx} sin(x)=cos(x) \\ &\frac{d}{dx} cos(x)=sin(x)\\ &\frac{d}{dx} sinh(x)=cosh(x) \\ &\frac{d}{dx} cosh(x)=sinh(x) \\ &\frac{d}{dx} x^n = nx^{n-1} \\ &\frac{d^n}{dx^n} x^n = n! \\ &\frac{d}{dx} e^x = e^x \\ &\frac{d}{dx} ln(x) = \frac{1}{x} \end{align*}求导法则 链式法则: $\frac{d}{dx} (g \circ f) = \frac{d}{dx} (f)· \frac{d}{dx} $ 加法法则: $\frac{d}{dx} (g + f) = \frac{dg}{dx} + \frac{df}{dx} $ 乘法法则: $\frac{d}{dx} (g·f) = \frac{g}{dx} ·f + g· \frac{df}{dx} $ 除法法则: $\frac{d}{dx} (\frac{g}{f} ) = \frac{\frac{g}{dx}·f-\frac{f}{dx}·g}{f^2}$ 反函数求导： $\frac{d}{dx} (f^{−1}) = \frac{1}{\frac{df}{dx} (f^{−1})} $ 求$f(x)=x^x=exp(ln(x))^x =exp(ln(x)·x). $的导数$g(x) = exp(x),h(x) = xln(x)$, 则$ f(x) = (g \circ h)(x) $ \begin{align*} f'(x) &= g'(h(x))\cdot h'(x) = g(h(x))·h'(x) \\ &= f(x)·(ln(x) + x·\frac{1}{x}) \\ &= f(x)·(ln(x) + 1) \\ &= x^x(ln(x) + 1) \end{align*}泰勒级数定义：泰勒/迈克劳林级数: 多项式逼近如果 $f(x)$ 是一个无限次可导的函数，那么在任何一点 $x_0$ 附近 我们可以对 $f(x)$ 做多项式逼近:$f(x_0 + ∆x) =f(x_0) + f’(x_0)∆x +\frac{f’’(x_0)}{2}∆_x^2 +\cdots +\frac{f^{(n)}(x_0)}{n!}∆_x^n + o(∆^n_x)$在本课中我们不关注对于尾巴上的余项 $o(∆^n_x)$ 的大小估计 一元函数在点$x_k$处的泰勒展开式为：f(x) = f(x_k)+(x-x_k)f'(x_k)+\frac{1}{2!}(x-x_k)^2f''(x_k)+o^n二元函数在点$(x_k,y_k)$处的泰勒展开式为：f(x,y)=f(x_k,y_k)+(x-x_k)f'_x(x_k,y_k)+(y-y_k)f'_y(x_k,y_k)\\ +\frac1{2!}(x-x_k)^2f''_{xx}(x_k,y_k)+\frac1{2!}(x-x_k)(y-y_k)f''_{xy}(x_k,y_k)\\ +\frac1{2!}(x-x_k)(y-y_k)f''_{yx}(x_k,y_k)+\frac1{2!}(y-y_k)^2f''_{yy}(x_k,y_k)+o^n多元函数(n)在点$x_k$处的泰勒展开式为：f(x^1,x^2,\ldots,x^n)=f(x^1_k,x^2_k,\ldots,x^n_k)+\sum_{i=1}^n(x^i-x_k^i)f'_{x^i}(x^1_k,x^2_k,\ldots,x^n_k)\\ +\frac1{2!}\sum_{i,j=1}^n(x^i-x_k^i)(x^j-x_k^j)f''_{ij}(x^1_k,x^2_k,\ldots,x^n_k)+o^n一般的泰勒级数\begin{align*} &e^x = 1 + x + x^2/2 +\cdots + x^n/n! + o(x^n) \\ &ln(1 + x) = x-x^2/2 + x^3/3 +\cdots +(-1)^{n−1} x^n/n + o(x^n)\\ &sin(x) = x-x^3/6 +\cdots +(-1)^nx^{2n+1}/(2n + 1)! + o(x^{2n+1}) \\ &cos(x) = 1-x^2/2 + x^4/24 +\cdots + (-1)^nx^{2n}/(2n)! + o(x^{2n+1}) \end{align*}罗比塔法则如果 $f,g$ 是两个无穷阶可导的函数，而且 $f(x_0) = g(x_0) = 0, g’(x_0) \neq 0$, 则 $\lim_{x→x_0} f(x)/g(x) = lim x→x0 f’(x)/g’(x)$. 推导 牛顿法与梯度下降法数学原理：牛顿法使用二阶逼近，梯度下降法使用一阶逼近 牛顿法对局部凸的函数找到极小值，对局部凹的函数找到极 大值，对局部不凸不凹的可能会找到鞍点.x_n = x_{n-1}-\frac{f'(x_{n-1})}{ f''(x_{n-1})} 梯度下降法一般不会找到最大值，但是同样可能会找到鞍点.x_n = x_{n−1} −γ_{n−1}∇f(x_{n−1}) 极值点条件 全局极小值: 如果对于任何 $\tilde{x}$, 都有$ f(x_∗) ≤ f(\tilde{x})$，那么$ x_∗$ 就是全局极小值点. 局部极小值: 如果存在一个正数 $δ$使得，对于任何满足$ | \tilde{x}−x_∗| &lt; δ $的 $\tilde{x}$, 都有$ f(x_∗) ≤ f(\tilde{x})$，那么$ x_∗ $就是局部极 小值点.（方圆$ δ$ 内的极小值点） 不论是全局极小值还是局部极小值一定满足一阶导数/梯度 为零，$f′ = 0 $或者$ ∇f = 0.$]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>高数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA主成分分析]]></title>
    <url>%2F2017%2F08%2F13%2FPCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1 特征中心化。即每一维的数据都减去该维的均值。 2 计算协方差矩阵.协方差就是衡量两个变量相关性的变量 3 计算协方差矩阵的特征值和特征向量 4 选取从大到小依次选取若干个的特征值对应的特征向量，映射得到新的样本集：样本乘以特征向量 PCA简介PCA的思想是将$n$维特征映射到$k$维空间上$k&lt;n$，这$k$维特征是全新的正交特征，是重新构造出来的$k$维特征，而不是简单地从$n$维特征中去除其余$n−k$维特征。那么如何衡量投影向量的优劣呢？在数学上有三种方法衡量投影的优劣！PCA可以被定义为数据在低维线性空间上的正交投影。如下图所示，将3维空间中的数据映射到2维空间。 映射的基本原则 使得投影数据的⽅差被最⼤化（Hotelling, 1933），即最大方差理论。即数据映射之后差异性最大，方法最大 使得平均投影代价最⼩的线性投影，即最小误差理论。平均投影代价是指数据点和它们的投影之间的平均平⽅距离 假设三维空间中有一系列点，这些点分布在一个过原点的斜面上，如果你用自然坐标系x,y,z这三个轴来表示这组数据的话，需要使用三个维度，而事实上，这些点的分布仅仅是在一个二维的平面上。我们所需要做得就是旋转x、y平面使其与数据点重合 PCA实现步骤 特征中心化。即每一维的数据都减去该维的均值。 计算协方差矩阵.协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）PCA为什么使用协方差.md cov=\bigl(\begin{smallmatrix} cov(x,x) & cov(x,y)\\ cov(y,x) & cov(y,y) \end{smallmatrix}\bigr) 计算协方差矩阵的特征值和特征向量。 选取从大到小依次选取若干个的特征值对应的特征向量，映射得到新的样本集。样本乘以特征向量 PCA实例现在假设有一组数据如下： 解决步骤：1、 分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91。 2、 求特征协方差矩阵，如果数据是3维，那么协方差矩阵是C = \begin{pmatrix} cov(x,x) & cov(x,y) & cov(x,z)\\ cov(y,x) & cov(y,y) & cov(y,z)\\ cov(z,x) & cov(z,y) & cov(z,z) \end{pmatrix} 这里只有x和y，求解得cov=\bigl(\begin{smallmatrix} cov(x,x) & cov(x,y)\\ cov(y,x) & cov(y,y) \end{smallmatrix}\bigr)=\bigl(\begin{smallmatrix} 0.5549 &0.5539 \\ 0.5539 &0.6449 \end{smallmatrix}\bigr)对角线上分别是x和y的方差，非对角线上是协方差。协方差是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。3、 求协方差的特征值和特征向量，得到eigenvalues = \begin{pmatrix} 0.0490833989\\ 1.28402771 \end{pmatrix} eigenvectors = \begin{pmatrix} -0.735178656 & -0.677873399\\ 0.677873399 & -0.735178656 \end{pmatrix}4、 将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。5、 将样本点投影到选取的特征向量上参考文献]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>特征工程</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F08%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues 1npm install hexo-renderer-kramed --save niahoQuick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment lim_{1\to+\infty}P(|\frac{1}{n}\sum_i^nX_i-\mu|]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>testing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++存储类]]></title>
    <url>%2F2017%2F07%2F29%2FC-%E5%AD%98%E5%82%A8%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[存储类 描述 auto 声明变量时根据初始化表达式自动推断该变量的类型、声明函数时函数返回值的占位符 register register 存储类用于定义存储在寄存器中而不是 RAM 中的局部变量。这意味着变量的最大尺寸等于寄存器的大小（通常是一个词），且不能对它应用一元的 ‘&amp;’ 运算符（因为它没有内存位置）。 static static 存储类指示编译器在程序的生命周期内保持局部变量的存在，而不需要在每次它进入和离开作用域时进行创建和销毁。因此，使用 static 修饰局部变量可以在函数调用之间保持局部变量的值。static 修饰符也可以应用于全局变量。当 static 修饰全局变量时，会使变量的作用域限制在声明它的文件内。 extern 当您有多个文件且定义了一个可以在其他文件中使用的全局变量或函数时，可以在其他文件中使用 extern 来得到已定义的变量或函数的引用。可以这么理解，extern 是用来在另一个文件中声明一个全局变量或函数。 mutable mutable 说明符仅适用于类的对象，这将在本教程的最后进行讲解。它允许对象的成员替代常量。也就是说，mutable 成员可以通过 const 成员函数修改。 thread_local 使用 thread_local 说明符声明的变量仅可在它在其上创建的线程上访问。 变量在创建线程时创建，并在销毁线程时销毁。 每个线程都有其自己的变量副本。可以将 thread_local 仅应用于数据声明和定义，thread_local 不能用于函数声明或定义。thread_local 说明符可以与 static 或 extern 合并。 auto 存储类1234auto f=3.14; //doubleauto s("hello"); //const char*auto z = new auto(9); // int*auto x1 = 5, x2 = 5.0, x3='r';//错误，必须是初始化为同一类型 register 存储类1register int miles; static 存储类1234567891011121314151617181920212223#include &lt;iostream&gt;// 函数声明void func(void);static int count = 10; /* 全局变量 */int main()&#123; while(count--) &#123; func(); &#125; return 0;&#125;// 函数定义void func( void )&#123; static int i = 5; // 局部静态变量 i++; std::cout &lt;&lt; "变量 i 为 " &lt;&lt; i ; std::cout &lt;&lt; " , 变量 count 为 " &lt;&lt; count &lt;&lt; std::endl;&#125; 输出12345678910变量 i 为 6 , 变量 count 为 9变量 i 为 7 , 变量 count 为 8变量 i 为 8 , 变量 count 为 7变量 i 为 9 , 变量 count 为 6变量 i 为 10 , 变量 count 为 5变量 i 为 11 , 变量 count 为 4变量 i 为 12 , 变量 count 为 3变量 i 为 13 , 变量 count 为 2变量 i 为 14 , 变量 count 为 1变量 i 为 15 , 变量 count 为 0 extern 存储类第一个个文件12345678910#include &lt;iostream&gt;int count ;extern void write_extern();int main()&#123; count = 5; write_extern();&#125; 第二个文件12345678#include &lt;iostream&gt;extern int count;void write_extern(void)&#123; std::cout &lt;&lt; "Count is " &lt;&lt; count &lt;&lt; std::endl;&#125; mutable 存储类thread_local 存储类使用 thread_local 说明符声明的变量仅可在它在其上创建的线程上访问。 变量在创建线程时创建，并在销毁线程时销毁。 每个线程都有其自己的变量副本。可以将 thread_local 仅应用于数据声明和定义，thread_local 不能用于函数声明或定义。thread_local 说明符可以与 static 或 extern 合并。1234567891011thread_local int x; // 命名空间下的全局变量class X&#123; static thread_local std::string s; // 类的static成员变量&#125;;static thread_local std::string X::s; // X::s 是需要定义的void foo()&#123; thread_local std::vector&lt;int&gt; v; // 本地变量&#125;]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>语言</tag>
        <tag>存储类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CART]]></title>
    <url>%2F2017%2F07%2F21%2F%E5%9B%9E%E5%BD%92%E6%A0%91%2F</url>
    <content type="text"><![CDATA[简介决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。CART算法有两步：决策树生成和剪枝。 回归树的生成一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为$M$个单元$R_1,R_2,…,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c$,$f(x)=\sum _{i=1}^M c_mI(x \in R_m)$ 输入：训练数据集$D{(x_1,y_1),…,(x_N,y_N)}$； 输出：回归树$f(x)=\sum _{i=1}^M c_mI(x \in R_m)$在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树 当输入空间的划分确定时，可以用平方误差$\sum _{x_i\in R_m}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，选择最优切分变量$j$与切分点$s$，使得预测误差最小，求解\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2] 用选定的对$（j,s）$划分区域并决定相应的输出值：R_1(j,s)=\{x|x^{(j)}\leqslant s\},R_1(j,s)=\{x|x^{(j)}> s\}\\ \hat{c}_m=\frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i ,\; \; x\in R_m ,\, \, m=1,2 继续对两个子区域调用以上两个步骤，直至满足停止条件。 将输入空间划分为$M$个区域$R_1,R_2,…R_m$，生成决策树f(x)=\sum _{m=1}^M\hat{c}_mI(x \in R_m) 计算实例训练数据见下表，x的取值范围为区间[0.5,10.5],y的取值范围为区间[5.0,10.0],学习这个回归问题的最小二叉回归树 $x_i$ 1 2 3 4 5 6 7 8 9 10 $y_i$ 5.56 5.70 5.91 6.40 6.80 7.05 8.90 8.70 9.00 9.05 根结点$T_1(x)$求训练数据的切分点，根据所给数据，求取使得预测误差最小的切分点： $x_i$ 1 2 3 4 5 6 7 8 9 10 $y_i$ 5.56 5.70 5.91 6.40 6.80 7.05 8.90 8.70 9.00 9.05 切分点 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 例如当 $s=1.5$ 时，$R_1 = \lbrace 1\rbrace$ , $R_2 = \lbrace 2, 3 , \ldots , 10\rbrace$ , $c_1=5.56$ , $c_2=7.50$ , m(s)=\min_{j,s}[\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2] = 0+15.72 = 15.72由上表可知，当$x=6.5$的时候达到最小值，此时$R_1 = \lbrace 1 ,2 , \ldots , 6\rbrace$ , $R_2 = \lbrace 7 ,8 ,9 ,10 ,\rbrace$ , $c_1=6.24$ , $c_2=8.9$ , 所以回归树$T_1(x)$为： T_1(x) = \begin{cases} 6.24, & x\lt 6.5 \\ 8.91, & x \ge 6.5 \\ \end{cases}f_1(x) = T_1(x) 用$f_1(x)$拟合训练数据的平方误差： L(y,f_1(x)) = \sum_{i=1}^{10}(y_i-f_1(x_i))^2 = 1.93 次结点$T_2(x)$用$f_1(x)$拟合训练数据的残差见下表，表中$r_{2i} = y_i - f_1(x_i),i=1,2,\ldots , 10$ $x_i$ 1 2 3 4 5 6 7 8 9 10 $y_i$ 5.56 5.70 5.91 6.40 6.80 7.05 8.90 8.70 9.00 9.05 $r_{2i}$ -0.68 -0.54 -0.33 0.16 0.56 0.81 -0.01 -0.21 0.09 0.14 求$T_2(x)$.方法与求$T_1(x)$一样，只是拟合的数据是上表的残差，可以得到 T_2(x) = \begin{cases} -0.52, & x\lt 3.5 \\ 0.22, & x \ge 3.5 \\ \end{cases}f_2(x) = f_1(x) + T_2(x)= \begin{cases} 5.72, & x\lt 3.5 \\ 6.46, & 3.5\le x \lt 6.5 \\ 9.13, & x\ge 6.5 \\ \end{cases} 用$f_2(x)$拟合训练数据的平方误差是：L(y,f_2(x)) = \sum_{i=1}^{10}(y_i-f_2(x_i))^2 = 0.79 结果继续求得 T_3(x) = \begin{cases} 0.15, & x\lt 6.5 \\ -0.22, & x \ge 6.5 \\ \end{cases} \quad L(y,f_3(x)) = 0.47 ,T_4(x) = \begin{cases} -0.16, & x\lt 4.5 \\ 0.11, & x \ge 4.5 \\ \end{cases} \quad L(y,f_3(x)) = 0.30 ,T_5(x) = \begin{cases} 0.07, & x\lt 6.5 \\ -0.11, & x \ge 6.5 \\ \end{cases} \quad L(y,f_3(x)) = 0.23 ,T_6(x) = \begin{cases} -0.15, & x\lt 2.5 \\ 0.04, & x \ge 2.5 \\ \end{cases}f_6(x) = f_5(x)+T_6(x) =T_1(x)+ \ldots + T_5(x) + T_6(x)= \begin{cases} 5.63, & x\lt 2.5 \\ 5.82, & 2.5 \le x\lt 3.5 \\ 6.56, & 3.5 \le x\lt 4.5 \\ 6.83, & 4.5 \le x\lt 6.5 \\ 8.95, & x\ge 6.5 \\ \end{cases} 用$f_6(x)$拟合训练数据的平方损失误差是L(y,f_6(x)) = \sum_{i=1}^{10}(y_i-f_6(x_i))^2 = 0.71假设此时已经满足误差要求，那么$f(x)=f_6(x)$即为所求的回归树。 综可知平方差函数越来越小 分类树的生成 输入：训练数据集$D$，停止计算的条件； 输出：CART决策树。根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树： 设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A＝a$的测试为“是”或“否”将$D$分割成$D1$和$D2$两部分，利用式$Gini(D)=1-\sum_{k=1}^{K}\left ( \frac{|c_k|}{D} \right )^2$计算$A＝a$时的基尼指数。 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 对两个子结点递归地调用以上两个步骤，直至满足停止条件。 生成CART决策树。 基尼指数 分类问题中，假设有 $K$ 个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为 \begin{align*} Gini(p)&=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2\\ Gini(D)&=1-\sum_{k=1}^{K}\left ( \frac{|c_k|}{D} \right )^2 \end{align*} 如果样本集合 $D$ 根据特征 $A$ 是否取某一可能值 $a$ 被分割成 $D_1$ 和 $D_2$ 两部分，则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为 Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2) 基尼指数 $Gini(D,A)$ 表示集合 $D$ 的不确定性，基尼指数 $Gini(D,A)$ 表示经 $A=a$ 分割后集合 $D$ 的不确定性。基尼指数值越大，样本集合的不确定性也就越强 计算实例分别以$A_1，A_2，A_3，A_4$表示年龄、有工作、有自己的房子和信贷情况4个特征，并以$1，2，3$表示年龄的值为青年、中年和老年，以$1，2$表示有工作和有自己的房子的值为是和否，以$1，2，3$表示信贷情况的值为非常好、好和一般。 Gini( D,A_1=1 )=\frac{5}{15}\left ( \frac{2}{5}(1-\frac{2}{5})+\frac{3}{5}(1-\frac{3}{5}) \right )+\frac{10}{15}\left ( \frac{7}{10}(1-\frac{7}{10})+\frac{3}{10}(1-\frac{3}{10}) \right )在$A_1，A_2，A_3，A_4$几个特征中，$Gini(D，A_3＝1)＝0.27$最小，所以选择特征 $A_3$ 为最优特征，$A_3＝1$为其最优切分点。于是根结点生成两个子结点，一个是叶结点。对另一个结点继续使用以上方法在$A_1，A_2，A_4$中选择最优特征及其最优切分点，结果是 $A_2＝1$。依此计算得知，所得结点都是叶结点。 CART剪枝CART决策树剪枝CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。CART剪枝算法由两步组成： 首先从生成算法产生的决策树 $T_0$ 底端开始不断剪枝，直到T0的根结点，形成一个子树序列$\{T_0，T_1,…,T_n\}$； 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。 那么问题来了，参数$α$给定的？谁来给？领域专家给？这是一种行之有效的办法，但却需要领域知识。理想化的模型都希望参数由data决定，也就是$α$也由数据决定。那么我们能想到的就是拿测试数据去测试在给定$α$下生成的子树。ID3与C4.5使用的$α$由人为决定，CART的$α$有算法决定下面主要介绍如何确定$α$ $α$的算法剪枝过程中计算子树的损失函数 C_{\alpha}(T) = C(T)+\alpha \left | T \right |其中，$T$为任意子树，$ C(T)$为对训练数据的预测误差（如基尼指数），$\left | T \right |$为子树的叶结点个数，$\alpha \geqslant 0$为参数，$C_\alpha (T)$为参数是$\alpha $时的子树$T$的整体损失，参数$\alpha $权衡训练数据的拟合程度与模型的复杂度 具体地，从整体树$T_0$开始剪枝，对$T_0$的任意内部结点$t$，以$t$为单结点树的损失函数是 C_{\alpha}(t) = C(t)+\alpha以$t$为为根结点的子树$T_t$为单结点树的损失函数是 C_{\alpha }(T_t) = C(T_t)+\alpha \left | T_t \right | 当$\alpha =0$及$\alpha $充分小时，有不等式 C_\alpha(T_t)C_\alpha(t)所以我们只要取$\alpha_1 = \frac{C(t)-C(T_t)}{\vert T_t\vert-1}$时，当且仅当$α≥α_1$时，剪枝必然发生。$α$必须满足上述条件，否则前提假设将失去意义。所以说，我们通过假设剪枝必然发生就能找到对应的$α$ 这个 $α$ 的值有什么意义，刚才我们高能预警的地方，$0=\alpha_0&lt;\alpha_1&lt;…&lt;\alpha_n&lt;+\infty$ ，在每个区间 $[α_i,α_{i+1})$ 中，子树 $T_i$ 是这个区间里最优的。为什么呢？原因就在刚才的推导，对于当前这个结点，只要 $α$ 大于这个值时，一定有$C_{\alpha}(t)&lt;C_{\alpha}(T_t)$ ，也就是剪掉这个结点后都比不剪要更优。所以每个最优子树对应的是一个区间，在这个区间内都是最优的。 $g(t)$为什么是最小的g(t) = \frac{C(t)-C(T_t)}{\vert T_t\vert-1} 当$α$较小时，结点不修剪的误差要小于修剪之后的误差，此时不剪为好，但当$α$增大时，修剪前后的误差先减小后增大，对应每个结点都有一个临界值 $g(t)$ 。为什么要选择最小的$ g(t)$ 呢？以图中两个点为例，结点1和结点2，$g(t)_2$ 大于$g(t)_1$, 假设在所有结点中$g(t)_1$最小，$g(t)_2$最大，两种选择方法：当选择最大值$g(t)_2$，即结点2进行剪枝，但此时结点1的不修剪的误差大于修剪之后的误差，即如果不修剪的话，误差变大，依次类推，对其它所有的结点的$g(t)$都是如此，从而造成整体的累计误差更大。反之，如果选择最小值$g(t)_1$，即结点1进行剪枝，则其余结点不剪的误差要小于剪后的误差，不修剪为好，且整体的误差最小。从而以最小$g(t)$剪枝获得的子树是该$α$值下的最优子树！ 剪枝流程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>模型</tag>
        <tag>树模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F06%2F21%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[简介分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。决策树案例图如下图所示。 算法 特征选择 目标 ID3 信息增益$g(D,A)=H(D)-H(D\mid A)$ 信息增益最大的 C4.5 信息增益比$g_{R}(D,A)=\frac{g(D,A)}{H_A(D)}$ 信息增益比大的 CART 分类：基尼指数$Gini( p )=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$回归：平方差 特征选择信息增益（用于ID3算法）信息熵表示的是不确定度。均匀分布时，不确定度最大，此时熵就最大。当选择某个特征对数据集进行分类时，分类后的数据集信息熵会比分类前的小，其差值表示为信息增益。信息增益可以衡量某个特征对分类结果的影响大小。信息增益=信息熵-条件熵；换句话说，信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度 定义特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的信息熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的条件熵 $H(D|A)$ 之差，即g(D,A)=H(D)-H(D|A) 算法设训练数据集为 $D$，$|D|$ 表示其样本容量，即样本个数。设有 $K$ 个类 $C_k，k＝1,2,…,K$ ，$|C_k|$ 为属于类 $C_k$ 的样本个数。设特征 $A$ 有 $n$ 个不同的取值 ${a_1，a_2,…,a_n}$ ，根据特征 $A$ 的取值将 $D$ 划分为$n$个子集 $D_1,D_2,…,D_n$，$|D_i|$ 为 $D_i$ 的样本个数，$\sum_{i=1}^{n}|D_i|=|D|$。记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$，即 $D_{ik}＝D_i⋂C_k$，$|D_{ik}|$ 为 $D_{ik}$ 的样本个数。于是信息增益的算法如下：输入：训练数据集$D$和特征$A$；输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$。 计算数据集$D$的信息熵$H(D)$H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|} 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$H(D|A)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|} 信息增益比当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较偏向取值较多的特征。例如面对连续增益(体重、身高、年龄等)，(极端情况下所有数据都独一无二)，在这种情况下，我们采用信息增益比。 \begin{align*} g_{R}(D,A)&=\frac{g(D,A)}{H_A(D)}\\ H_A(D)&=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_{i}|}{|D|} \end{align*}为什么使用信息增益比 Day Temperatrue Outlook Humidity Windy PlayGolf? 07-05 hot sunny high false no 07-06 hot sunny high true no 07-07 hot overcast high false yes 07-09 cool rain normal false yes 07-10 cool overcast normal true yes 07-12 mild sunny high false no 07-14 cool sunny normal false yes 07-15 mild rain normal false yes 07-20 mild sunny normal true yes 07-21 mild overcast high true yes 07-22 hot overcast normal false yes 07-23 mild sunny high true no 07-26 cool sunny normal true no 07-30 mild sunny high false yes 还是以此表为例，假如我们想用Day来做为特征(当然实际上一般人也不会傻到用Day用做特征)，显然，每一天都可以将样本分开，也就是形成了一颗叶子数量为14，深度只有两层的树。 信息增益\begin{align*} H(D) &= -\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}\\ &=-{5\over14}log {5\over14} - {9\over14}log {9\over14} =0.9403\\ H(D|A) &=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}\\ &=\sum_{i=1}^{14}1\times H(D_i)=\sum_{i=1}^{14}1\times\sum_{k=1}^{2}1\times log_2 \frac{1}{1}=0\\ g(D,A)&=H(D)-H(D|A)=0.9403 \end{align*}信息增益比较偏向取值较多的特征。 信息增益比\begin{align*} H_A(D) &=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}\\ &=-\sum_{i=1}^{14}\frac{1}{14}log_2\frac{1}{14}=3.807\\ g_{R}(D,A)&=\frac{g(D,A)}{H_A(D)} \\ &=\frac{0.9403}{3.807}=0.247 \end{align*}基尼指数分类问题中，假设有 $K$ 个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为 \begin{align*} Gini(p)&=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2\\ Gini(D)&=1-\sum_{k=1}^{K}\left ( \frac{|c_k|}{D} \right )^2 \end{align*}如果样本集合 $D$ 根据特征 $A$ 是否取某一可能值 $a$ 被分割成 $D_1$ 和 $D_2$ 两部分，则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为 Gini(D,A)=\frac{|D_1|}{D}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)基尼指数 $Gini(D,A)$ 表示集合 $D$ 的不确定性，基尼指数 $Gini(D,A)$ 表示经 $A=a$ 分割后集合 $D$ 的不确定性。基尼指数值越大，样本集合的不确定性也就越强 决策树生成迭代终止条件： 所有的特征都用了，没有特征可以继续来进行特征选择 当前特征集中的最大的信息增益小于我们设定的阈值 树的结点深度达到预定值 ID3ID3算法使用了信息增益。信息增益的缺点是：对取值数目比较多的属性有偏好。一个特征的信息增益越大，表明属性对样本熵减少的能力越强，不确定性变成确定性的能力越强。用信息增益训练出来的决策树深度很浅的树。 不足 ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3偏好选择取值多的特征做分支，ID3采用信息增益大的特征优先建立决策树的结点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。直白解释：取值比较多的特征，就可以分叉出更多的分支，分支更多，每个分支的纯度必然更高！ ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题，即没有剪枝处理 计算实例$A_1$：年龄 $A_2$：有工作 $A_3$：有自己的房子 $A_4$：信贷情况 根结点$H(D)=-\frac{9}{15}log_2\frac{9}{15}-\frac{6}{15}log_2\frac{6}{15}=0.971$由此可知$g(D,A_3)$最大，所以选择$A_3$（有自己的房子）作为根结点做为划分 树结点它将训练数据集 $D$ 划分为两个子集 $D_1$（$A_3$取值为“是”）和 $D_2$（$A_3$取值为“否”）。当 $A_3$ 取值为“是”的时候，$D_1$ 全部为是；当 $A_3$ 取值为“否”的时候，对 $D_2$ 进行划分$H(D_2)=-\frac{3}{9}log_2\frac{3}{9}-\frac{6}{9}log_2\frac{6}{9}=0.918$由此可知 $g(D,A_2)$ 最大，所以选择 $A_2$（有工作）作为树结点做为划分 结果 C4.5采用信息增益比作为特征选择标准，与算法与ID3类似 对与 ID3没有考虑连续特征(以两个相邻连续值的平均数进行划分)详情请参考如下连续值处理问题 对与 ID3偏好选择取值多的特征做分支改用信息增益比作为分支指标，因为特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 对与 ID3算法对于缺失值的情况没有做考虑 对与没有考虑过拟合的问题C4.5引入了正则化系数进行初步的剪枝。 CART决策树剪枝(ID3与C4.5)决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。 预减枝通过设定阈值，提前停止树的构建而对树剪枝，一旦停止，结点就是树叶。停止决策树生长最简单的方法有： 定义一个高度，当决策树达到该高度时就停止决策树的生长 达到某个结点的实例具有相同的特征向量，及时这些实例不属于同一类，也可以停止决策树的生长。这个方法对于处理数据的数据冲突问题比较有效。 定义一个阈值，当达到某个结点的实例个数小于阈值时就可以停止决策树的生长 定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值大小来决定是否停止决策树的生长。 后剪枝定义新的损失函数C_α(T)=C(T)+α|T|其中，$C(T)$即是该 Node 和不确定性相关的损失、$|T|$则是该 Node 下属叶结点的个数。不妨设第 $t$ 个叶结点含有 $N_t$个样本且这 $N_t$ 个样本的不确定性为 $H_t(T)$，那么新损失一般可以直接定义为加权不确定性： C\left( T \right) = \sum_{t = 1}^{\left| T \right|}{N_{t}H_{t}(T)}经验熵的损失函数当不确定性为经验熵$H_t(T)=-\sum_{k}\frac{N_{tk}}{N_t}log\: \frac{N_{tk}}{N_t}$时，其损失函数为 C(T)=\sum_{t=1}^{\left | T \right |}N_tH_t(T)=-\sum_{t=1}^{\left | T \right |} \sum_{k=1}^KN_{tk}log\: \frac{N_{tk}}{N_t}其中树的叶子结点个数为$\left | T \right |$,$t$是树$T$的叶结点，该叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,…,K$,$H(t)$为叶结点$t$上的经验熵，$\alpha \geqslant 0$为参数 算法 连续值的处理比如$m$个样本的连续特征$A$有$m$个，从小到大排列为$a_1,a_2,…,a_m$,则取相邻两样本值的中位数，一共取得$m−1$个划分点，其中第$i$个划分点$T_i$表示为： T_i = \frac{a_i+a_{i+1}}{2}对于这$m−1$个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点$a_t$,则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前结点为连续属性，则该属性后面还可以参与子结点的产生选择过程。 缺失值的处理可以简单的抛弃不完整样本，但是如果不完整样本太多，就不能简单的抛弃。面对不完整样本我们有两个问题需要解决： 如何在属性值缺失的情况下进行划分属性选择 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分 给定训练集$D$和属性$A$，令$\tilde{D}$表示$D$中属性$A$上没有缺失值的样本子集。对问题(1)，显然我们仅可根据$\tilde{D}$来判断属性$A$的优劣。假设属性$A$有$V$个可取的值${a^1,…,a^V}$，令$\tilde{D}_v$表示$\tilde{D}$中在属性$A$上取值为$a^V$的样本子集，$\tilde{D}_k$表示$\tilde{D}$中属于第$k$类$k=1,…,K$的样本子集，则显然有$\tilde{D}=\bigcup _{k=1}^K\tilde{D}_k$、$\tilde{D}=\bigcup _{v=1}^V\tilde{D}_v$。假定我们为每个样本$x$赋予一个权重$w_x$并定义$\rho =\frac{\sum _{x \in \tilde{D}}w_x}{\sum _{x \in D}w_x}$ $\rho $表示无缺失样本所占的比例$\tilde{p}_k =\frac{\sum _{x \in \tilde{D}_k}w_x}{\sum _{x \in \tilde{D}}w_x}\: \: (1\leqslant k\leqslant K)$ $\tilde{p}_k$表示无缺失值样本中第$k$类所占的比例$\tilde{r}_v =\frac{\sum _{x \in \tilde{D}_v}w_x}{\sum _{x \in \tilde{D}}w_x}\: \: (1\leqslant k\leqslant V)$ $\tilde{r}_v$表示无缺失值样本中在属性$A$上取值$a^v$所占的样本比例 \begin{align*} Gain(D|A)&=\rho \times Gain(\tilde{D} ,A) \\ &= \rho \times \left ( Ent(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_v Ent(\tilde{D_v})\right )\\ Ent(\tilde{D}) &= -\sum_{k=1}^{\left | \gamma \right |}\tilde{p_k}log_2\; \tilde{p_k} \end{align*}对问题(2)，若样本$x$在属性$A$上的取值已知，则将$x$划入取值相对应的子结点，且样本权重保持为$w_x$。若样本$x$在属性$A$上的取值未知，则将$x$同时划入所有的子结点，且样本权值在属性值$a^v$对应的子结点中调整为$\tilde{r}_v \cdot w_x$ 缺失值处理在某些情况下，可供使用的数据可能缺少某些属性的值。假如$〈x，c(x)〉$是样本集$S$中的一个训练实例，但是其属性$A$的值$A(x)$未知。处理缺少属性值的一种策略是赋给它结点$n$所对应的训练实例中该属性的最常见值；另外一种更复杂的策略是为$A$的每个可能值赋予一个概率。例如，给定一个布尔属性$A$，如果结点$n$包含6个已知$A=1$和4个$A=0$的实例，那么$A(x)=1$的概率是0.6，而$A(x)=0$的概率是0.4。于是，实例$x$的60%被分配到$A=1$的分支，40%被分配到另一个分支。这些片断样例的目的是计算信息增益，另外，如果有第二个缺少值的属性必须被测试，这些样例可以在后继的树分支中被进一步细分。C4.5就是使用这种方法处理缺少的属性值。 计算属性$A$的增益或者增益率时，如果有些样本没有属性$A$，可以有这么几种处理方式： (1). 忽略这些缺失属性$A$的样本。 (2). 给缺失属性$A$的样本赋予属性$A$一个均值或者最常用的的值。(3). 计算增益或者增益率时根据缺失属性样本个数所占的比率对增益/增益率进行相应的“打折”。 （请看周志华的机器学习）(4). 根据其他未知的属性想办法把这些样本缺失的属性补全。 当属性$A$已经被选择，该对样本进行分支的时候，如果有些样本缺失了属性$A$,那么： (1).忽略这些样本。 (2).把这些样本的属性$A$赋予一个均值或者最常出现的值，然后再对他们进行处理。(3).把属性缺失样本分配给所有的子集，也就是说每个子集都有这些属性缺失样本。 (4).单独为属性缺失的样本划分一个分支子集。(5).对于缺失属性$A$的样本，尝试着根据其他属性给他分配一个属性$A$的值，然后继续处理将其划分到相应的子集。 对于一个确实属性$A$的待分类样本，有这么几种解释(1).如果有单独的确实分支，依据此分支(2).把待分类的样本的属性$A$值分配一个最常见出现的$A$的属性值，然后进行分支预测(3).估计其他属性为该待分类样本填充一个属性$A$值，然后进行分支处理(4).在决策树中属性$A$结点的分支上，遍历属性$A$结点的所有分支，探索可能所有的分类结果，然后把这些分类结果结合起来一起考虑，按照概率决定一个分类。(5).待分类样本在到达属性$A$结点时就终止分类，然后根据此时$A$结点所覆盖的叶子节结点类别状况其分配一个发生概率最高的类]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>模型</tag>
        <tag>树模型</tag>
      </tags>
  </entry>
</search>
